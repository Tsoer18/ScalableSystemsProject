


<!DOCTYPE html><html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  	<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
	<link rel="icon" href="/favicon.ico" type="image/x-icon">
    
  
  <title>CData JDBC Driver for Apache Kafka - CData JDBC Driver for Apache Kafka</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="description" 	content="CData JDBC Driver for Apache Kafka - RSBApacheKafka - CData JDBC Driver for Apache Kafka: The connection string properties describe the various options that can be used to establish a connection." />
	<meta name="author" 		content="www.cdata.com"> 
	<meta name="generator" 		content="CData JDBC Driver for Apache Kafka (CData JDBC Driver for Apache Kafka  [SKRJV Build 8750]);">
	

    <meta http-equiv="X-UA-Compatible" content="IE=edge" >
    <!--if it works on the readme.htm file-->                  <link rel="stylesheet" type="text/css" href="./lib/bootstrap/bootstrap.min.css"/> 
        <link rel="stylesheet" type="text/css" href="./lib/syntaxhighlighter-3.0.83/styles/shCore.css" />
        <link rel="stylesheet" type="text/css" href="./lib/syntaxhighlighter-3.0.83/styles/shThemeDefault.css" />
        <link rel="stylesheet" type="text/css" href="./lib/help.css" />
        <link rel="stylesheet" type="text/css" href="./lib/tree2.css" />
      
    </head>
<body>
	
  <div class="header">
    <a href='http://www.cdata.com/'>
    <div id="whlogo">&nbsp;</div>
    </a>
    <div id='whheader'>
    	<h1>JDBC Driver for Apache Kafka</h1>

    	<span class="phones">Build 23.0.8750</span>
    	</div>
    <div id="hamburger-menu"></div>
  </div>
    <div border="0" cellpadding="0" cellspacing="0" id="whlayout">
        
        <div id="whcontent">
        <!-- TABLE OF CONTENTS -->            <div height="100%" id=whleftcol width='1%' > 
                <div style="width:340px;"></div> 
                <div id='whsizer' style='width:340px;'>
		


































<!--if it works on the readme.htm file-->

    
    

    

<div id="whtoc" style="display:none;">

<!--  BEGIN TOC CONTENT -->
<UL class="nav">


  <LI class="expanded">
  <a href="#default" style="font-weight: bold;">CData JDBC Driver for Apache Kafka</a>

  <UL>








  
    <LI>
      <a href="#pg_startedintroj">Getting Started</a>





  <UL>




  
    <LI>
      <a href="#pg_licensingjdbc"> Licensing</a>








  
    <LI>
      <a href="#pg_connectionj"> Establishing a Connection</a>








  
    <LI>
      <a href="#pg_jdbcconnstring"> Building the JDBC URL</a>








  
    <LI>
      <a href="#pg_azureadoauthcustomappcreate"> Creating a Custom Authentication Application</a>








  
    <LI>
      <a href="#pg_readingtopics"> Extracting Metadata From Topics</a>








  
    <LI>
      <a href="#pg_azureadoauthcustomappcreate"> Creating a Custom OAuth Application</a>








  
    <LI>
      <a href="#pg_changelog"> Changelog</a>








  </UL>

  
    <LI>
      <a href="#pg_usagej">Using JDBC</a>





  <UL>




  
    <LI>
      <a href="#pg_jars"> Installed Files</a>








  
    <LI>
      <a href="#pg_JDBCconnectcode"> Connecting from Code</a>








  
    <LI>
      <a href="#pg_JDBCqueries"> Executing Statements</a>








  
    <LI>
      <a href="#pg_JDBCupdates"> Using Prepared Statements</a>








  
    <LI>
      <a href="#pg_connectionpoolingjdbc"> Connection Pooling</a>





  <UL>




  
    <LI>
      <a href="#pg_jndipooling"> JNDI</a>








  </UL>

  
    <LI>
      <a href="#pg_storedprocedurej"> Calling Stored Procedures</a>








  </UL>

  
    <LI>
      <a href="#pg_jdbctoolsintro">Using from Tools</a>





  <UL>




  
    <LI>
      <a href="#pg_jdbcdbvisualizer"> DbVisualizer</a>








  
    <LI>
      <a href="#pg_jdbcdbeaver"> DBeaver</a>








  
    <LI>
      <a href="#pg_jdbcsquirrel"> SQuirreL SQL</a>








  
    <LI>
      <a href="#pg_jdbctableau"> Tableau</a>








  </UL>

  
    <LI>
      <a href="#pg_systemtablesintro">Schema Discovery</a>





  <UL>




  
    <LI>
      <a href="#pg_systemtablesj"> Tables</a>








  
    <LI>
      <a href="#pg_systemcolumnsj"> Columns</a>








  
    <LI>
      <a href="#pg_spmetadataj"> Procedures</a>








  
    <LI>
      <a href="#pg_spmetadataparamj"> Procedure Parameters</a>








  
    <LI>
      <a href="#pg_primarykeysjdbc"> Primary Keys</a>








  
    <LI>
      <a href="#pg_connmetadatajava"> Connection Properties</a>








  
    <LI>
      <a href="#pg_resultsetmetadatajdbc"> Result Sets</a>








  </UL>

  
    <LI>
      <a href="#pg_advancedfeatures">Advanced Features</a>





  <UL>




  
    <LI>
      <a href="#pg_userviews"> User Defined Views</a>








  
    <LI>
      <a href="#pg_advancedssl"> SSL Configuration</a>








  
    <LI>
      <a href="#pg_advancedproxy"> Firewall and Proxy</a>








  
    <LI>
      <a href="#pg_remotingintroj"> JDBC Remoting</a>





  <UL>




  
    <LI>
      <a href="#pg_cli"> CLI Options</a>








  
    <LI>
      <a href="#pg_conf"> Configuration File</a>








  </UL>

  
    <LI>
      <a href="#pg_caching"> Caching Data</a>





  <UL>




  
    <LI>
      <a href="#pg_workings">Configuring the Cache Connection</a>








  
    <LI>
      <a href="#pg_cachingMetadata">Caching Metadata</a>








  
    <LI>
      <a href="#pg_cacheAutomatically">Automatically Caching Data</a>








  
    <LI>
      <a href="#pg_cacheExplicitly">Explicitly Caching Data</a>








  
    <LI>
      <a href="#pg_cachedatatypemapping">Data Type Mapping</a>








  </UL>

  
    <LI>
      <a href="#pg_advancedqueryproc"> Query Processing</a>








  
    <LI>
      <a href="#pg_advancedlogging"> Logging</a>








  </UL>

  
    <LI>
      <a href="#pg_overview">SQL Compliance</a>





  <UL>




  
    <LI>
      <a href="#pg_sqlfunctions"> SQL Functions</a>





  <UL>




  
    <LI>
      <a href="#pg_sqlstringfunctions"> STRING Functions</a>








  
    <LI>
      <a href="#pg_sqlmathfunctions"> MATH Functions</a>








  
    <LI>
      <a href="#pg_sqldatefunctions"> DATE Functions</a>








  
    <LI>
      <a href="#pg_dateliteralfunctions">Date Literal Functions</a>








  </UL>

  
    <LI>
      <a href="#pg_select">SELECT Statements</a>





  <UL>




  
    <LI>
      <a href="#pg_sfagg">Aggregate Functions</a>








  
    <LI>
      <a href="#pg_sfjoin">JOIN Queries</a>








  
    <LI>
      <a href="#pg_window">Window Functions</a>








  </UL>

  
    <LI>
      <a href="#pg_insert">INSERT Statements</a>








  
    <LI>
      <a href="#pg_cache">CACHE Statements</a>








  
    <LI>
      <a href="#pg_exec">EXECUTE Statements</a>








  
    <LI>
      <a href="#pg_pivotunpivot">PIVOT and UNPIVOT</a>








  </UL>

  
    <LI>
      <a href="#pg_datamodel">Data Model</a>





  <UL>




  
    <LI>
      <a href="#pg_allsps"> Stored Procedures</a>





  <UL>




  
    <LI>
      <a href="#pg_sp-commitoffset">CommitOffset</a>








  
    <LI>
      <a href="#pg_sp-createschema">CreateSchema</a>








  
    <LI>
      <a href="#pg_sp-getadminconsenturl">GetAdminConsentURL</a>








  
    <LI>
      <a href="#pg_sp-getoauthaccesstoken">GetOAuthAccessToken</a>








  
    <LI>
      <a href="#pg_sp-getoauthauthorizationurl">GetOAuthAuthorizationURL</a>








  
    <LI>
      <a href="#pg_sp-producemessage">ProduceMessage</a>








  
    <LI>
      <a href="#pg_sp-refreshoauthaccesstoken">RefreshOAuthAccessToken</a>








  </UL>

  
    <LI>
      <a href="#pg_allsystables"> System Tables</a>





  <UL>




  
    <LI>
      <a href="#pg_table-syscatalogs">sys_catalogs</a>








  
    <LI>
      <a href="#pg_table-sysschemas">sys_schemas</a>








  
    <LI>
      <a href="#pg_table-systables">sys_tables</a>








  
    <LI>
      <a href="#pg_table-systablecolumns">sys_tablecolumns</a>








  
    <LI>
      <a href="#pg_table-sysprocedures">sys_procedures</a>








  
    <LI>
      <a href="#pg_table-sysprocedureparameters">sys_procedureparameters</a>








  
    <LI>
      <a href="#pg_table-syskeycolumns">sys_keycolumns</a>








  
    <LI>
      <a href="#pg_table-foreignkeys">sys_foreignkeys</a>








  
    <LI>
      <a href="#pg_table-primarykeys">sys_primarykeys</a>








  
    <LI>
      <a href="#pg_table-sysindexes">sys_indexes</a>








  
    <LI>
      <a href="#pg_table-sysconnectionprops">sys_connection_props</a>








  
    <LI>
      <a href="#pg_table-syssqlinfo">sys_sqlinfo</a>








  
    <LI>
      <a href="#pg_table-sysidentity">sys_identity</a>







  </UL>

  </UL>





  <LI>
    <a href="#Connection">Connection String Options</a>
  
  <UL>






<li>
<a href="#RSBApacheKafka_c_Authentication">Authentication</a>
<UL>
		
<LI>

<a href="#RSBApacheKafka_p_AuthScheme">AuthScheme</a>



		
<LI>

<a href="#RSBApacheKafka_p_User">User</a>



		
<LI>

<a href="#RSBApacheKafka_p_Password">Password</a>



		
<LI>

<a href="#RSBApacheKafka_p_BootstrapServers">BootstrapServers</a>



		
<LI>

<a href="#RSBApacheKafka_p_Topic">Topic</a>



		
<LI>

<a href="#RSBApacheKafka_p_UseSSL">UseSSL</a>


</UL>




<li>
<a href="#RSBApacheKafka_c_Connection">Connection</a>
<UL>
		
<LI>

<a href="#RSBApacheKafka_p_ConsumerGroupId">ConsumerGroupId</a>



		
<LI>

<a href="#RSBApacheKafka_p_AutoCommit">AutoCommit</a>


</UL>




<li>
<a href="#RSBApacheKafka_c_AzureAuthentication">Azure Authentication</a>
<UL>
		
<LI>

<a href="#RSBApacheKafka_p_AzureTenant">AzureTenant</a>



		
<LI>

<a href="#RSBApacheKafka_p_AzureResource">AzureResource</a>


</UL>




<li>
<a href="#RSBApacheKafka_c_OAuth">OAuth</a>
<UL>
		
<LI>

<a href="#RSBApacheKafka_p_InitiateOAuth">InitiateOAuth</a>



		
<LI>

<a href="#RSBApacheKafka_p_OAuthClientId">OAuthClientId</a>



		
<LI>

<a href="#RSBApacheKafka_p_OAuthClientSecret">OAuthClientSecret</a>



		
<LI>

<a href="#RSBApacheKafka_p_OAuthAccessToken">OAuthAccessToken</a>



		
<LI>

<a href="#RSBApacheKafka_p_OAuthSettingsLocation">OAuthSettingsLocation</a>



		
<LI>

<a href="#RSBApacheKafka_p_OAuthVerifier">OAuthVerifier</a>



		
<LI>

<a href="#RSBApacheKafka_p_OAuthRefreshToken">OAuthRefreshToken</a>



		
<LI>

<a href="#RSBApacheKafka_p_OAuthExpiresIn">OAuthExpiresIn</a>



		
<LI>

<a href="#RSBApacheKafka_p_OAuthTokenTimestamp">OAuthTokenTimestamp</a>


</UL>




<li>
<a href="#RSBApacheKafka_c_Kerberos">Kerberos</a>
<UL>
		
<LI>

<a href="#RSBApacheKafka_p_KerberosKeytabFile">KerberosKeytabFile</a>



		
<LI>

<a href="#RSBApacheKafka_p_KerberosSPN">KerberosSPN</a>



		
<LI>

<a href="#RSBApacheKafka_p_KerberosServiceName">KerberosServiceName</a>



		
<LI>

<a href="#RSBApacheKafka_p_UseKerberosTicketCache">UseKerberosTicketCache</a>


</UL>




<li>
<a href="#RSBApacheKafka_c_SSL">SSL</a>
<UL>
		
<LI>

<a href="#RSBApacheKafka_p_SSLServerCert">SSLServerCert</a>



		
<LI>

<a href="#RSBApacheKafka_p_SSLServerCertType">SSLServerCertType</a>



		
<LI>

<a href="#RSBApacheKafka_p_SSLServerCertPassword">SSLServerCertPassword</a>



		
<LI>

<a href="#RSBApacheKafka_p_SSLClientCert">SSLClientCert</a>



		
<LI>

<a href="#RSBApacheKafka_p_SSLClientCertType">SSLClientCertType</a>



		
<LI>

<a href="#RSBApacheKafka_p_SSLClientCertPassword">SSLClientCertPassword</a>



		
<LI>

<a href="#RSBApacheKafka_p_SSLIdentificationAlgorithm">SSLIdentificationAlgorithm</a>


</UL>




<li>
<a href="#RSBApacheKafka_c_SchemaRegistry">Schema Registry</a>
<UL>
		
<LI>

<a href="#RSBApacheKafka_p_RegistryUrl">RegistryUrl</a>



		
<LI>

<a href="#RSBApacheKafka_p_RegistryType">RegistryType</a>



		
<LI>

<a href="#RSBApacheKafka_p_RegistryService">RegistryService</a>



		
<LI>

<a href="#RSBApacheKafka_p_RegistryAuthScheme">RegistryAuthScheme</a>



		
<LI>

<a href="#RSBApacheKafka_p_RegistryUser">RegistryUser</a>



		
<LI>

<a href="#RSBApacheKafka_p_RegistryPassword">RegistryPassword</a>



		
<LI>

<a href="#RSBApacheKafka_p_RegistryClientCert">RegistryClientCert</a>



		
<LI>

<a href="#RSBApacheKafka_p_RegistryClientCertType">RegistryClientCertType</a>



		
<LI>

<a href="#RSBApacheKafka_p_RegistryClientCertPassword">RegistryClientCertPassword</a>



		
<LI>

<a href="#RSBApacheKafka_p_RegistryClientCertSubject">RegistryClientCertSubject</a>



		
<LI>

<a href="#RSBApacheKafka_p_RegistryVersion">RegistryVersion</a>



		
<LI>

<a href="#RSBApacheKafka_p_RegistryServerCert">RegistryServerCert</a>


</UL>




<li>
<a href="#RSBApacheKafka_c_Firewall">Firewall</a>
<UL>
		
<LI>

<a href="#RSBApacheKafka_p_FirewallType">FirewallType</a>



		
<LI>

<a href="#RSBApacheKafka_p_FirewallServer">FirewallServer</a>



		
<LI>

<a href="#RSBApacheKafka_p_FirewallPort">FirewallPort</a>



		
<LI>

<a href="#RSBApacheKafka_p_FirewallUser">FirewallUser</a>



		
<LI>

<a href="#RSBApacheKafka_p_FirewallPassword">FirewallPassword</a>


</UL>




<li>
<a href="#RSBApacheKafka_c_Proxy">Proxy</a>
<UL>
		
<LI>

<a href="#RSBApacheKafka_p_ProxyAutoDetect">ProxyAutoDetect</a>



		
<LI>

<a href="#RSBApacheKafka_p_ProxyServer">ProxyServer</a>



		
<LI>

<a href="#RSBApacheKafka_p_ProxyPort">ProxyPort</a>



		
<LI>

<a href="#RSBApacheKafka_p_ProxyAuthScheme">ProxyAuthScheme</a>



		
<LI>

<a href="#RSBApacheKafka_p_ProxyUser">ProxyUser</a>



		
<LI>

<a href="#RSBApacheKafka_p_ProxyPassword">ProxyPassword</a>



		
<LI>

<a href="#RSBApacheKafka_p_ProxySSLType">ProxySSLType</a>



		
<LI>

<a href="#RSBApacheKafka_p_ProxyExceptions">ProxyExceptions</a>


</UL>




<li>
<a href="#RSBApacheKafka_c_Logging">Logging</a>
<UL>
		
<LI>

<a href="#RSBApacheKafka_p_Logfile">Logfile</a>



		
<LI>

<a href="#RSBApacheKafka_p_Verbosity">Verbosity</a>



		
<LI>

<a href="#RSBApacheKafka_p_LogModules">LogModules</a>



		
<LI>

<a href="#RSBApacheKafka_p_MaxLogFileSize">MaxLogFileSize</a>



		
<LI>

<a href="#RSBApacheKafka_p_MaxLogFileCount">MaxLogFileCount</a>


</UL>




<li>
<a href="#RSBApacheKafka_c_Schema">Schema</a>
<UL>
		
<LI>

<a href="#RSBApacheKafka_p_Location">Location</a>



		
<LI>

<a href="#RSBApacheKafka_p_BrowsableSchemas">BrowsableSchemas</a>



		
<LI>

<a href="#RSBApacheKafka_p_Tables">Tables</a>



		
<LI>

<a href="#RSBApacheKafka_p_Views">Views</a>


</UL>




<li>
<a href="#RSBApacheKafka_c_Caching">Caching</a>
<UL>
		
<LI>

<a href="#RSBApacheKafka_p_AutoCache">AutoCache</a>



		
<LI>

<a href="#RSBApacheKafka_p_CacheDriver">CacheDriver</a>



		
<LI>

<a href="#RSBApacheKafka_p_CacheConnection">CacheConnection</a>



		
<LI>

<a href="#RSBApacheKafka_p_CacheLocation">CacheLocation</a>



		
<LI>

<a href="#RSBApacheKafka_p_CacheTolerance">CacheTolerance</a>



		
<LI>

<a href="#RSBApacheKafka_p_Offline">Offline</a>



		
<LI>

<a href="#RSBApacheKafka_p_CacheMetadata">CacheMetadata</a>


</UL>




<li>
<a href="#RSBApacheKafka_c_Miscellaneous">Miscellaneous</a>
<UL>
		
<LI>

<a href="#RSBApacheKafka_p_AggregateMessages">AggregateMessages</a>



		
<LI>

<a href="#RSBApacheKafka_p_BatchSize">BatchSize</a>



		
<LI>

<a href="#RSBApacheKafka_p_CompressionType">CompressionType</a>



		
<LI>

<a href="#RSBApacheKafka_p_ConnectionLifeTime">ConnectionLifeTime</a>



		
<LI>

<a href="#RSBApacheKafka_p_ConnectOnOpen">ConnectOnOpen</a>



		
<LI>

<a href="#RSBApacheKafka_p_ConsumerProperties">ConsumerProperties</a>



		
<LI>

<a href="#RSBApacheKafka_p_CreateTablePartitions">CreateTablePartitions</a>



		
<LI>

<a href="#RSBApacheKafka_p_CreateTableReplicationFactor">CreateTableReplicationFactor</a>



		
<LI>

<a href="#RSBApacheKafka_p_EnableIdempotence">EnableIdempotence</a>



		
<LI>

<a href="#RSBApacheKafka_p_FlattenArrays">FlattenArrays</a>



		
<LI>

<a href="#RSBApacheKafka_p_GenerateSchemaFiles">GenerateSchemaFiles</a>



		
<LI>

<a href="#RSBApacheKafka_p_MaximumBatchSize">MaximumBatchSize</a>



		
<LI>

<a href="#RSBApacheKafka_p_MaxRows">MaxRows</a>



		
<LI>

<a href="#RSBApacheKafka_p_MessageKeyColumn">MessageKeyColumn</a>



		
<LI>

<a href="#RSBApacheKafka_p_MessageKeyType">MessageKeyType</a>



		
<LI>

<a href="#RSBApacheKafka_p_OffsetResetStrategy">OffsetResetStrategy</a>



		
<LI>

<a href="#RSBApacheKafka_p_Other">Other</a>



		
<LI>

<a href="#RSBApacheKafka_p_Pagesize">Pagesize</a>



		
<LI>

<a href="#RSBApacheKafka_p_PoolIdleTimeout">PoolIdleTimeout</a>



		
<LI>

<a href="#RSBApacheKafka_p_PoolMaxSize">PoolMaxSize</a>



		
<LI>

<a href="#RSBApacheKafka_p_PoolMinSize">PoolMinSize</a>



		
<LI>

<a href="#RSBApacheKafka_p_PoolWaitTime">PoolWaitTime</a>



		
<LI>

<a href="#RSBApacheKafka_p_ProduceMeta">ProduceMeta</a>



		
<LI>

<a href="#RSBApacheKafka_p_ProducerProperties">ProducerProperties</a>



		
<LI>

<a href="#RSBApacheKafka_p_PseudoColumns">PseudoColumns</a>



		
<LI>

<a href="#RSBApacheKafka_p_ReadDuration">ReadDuration</a>



		
<LI>

<a href="#RSBApacheKafka_p_Readonly">Readonly</a>



		
<LI>

<a href="#RSBApacheKafka_p_RowScanDepth">RowScanDepth</a>



		
<LI>

<a href="#RSBApacheKafka_p_RTK">RTK</a>



		
<LI>

<a href="#RSBApacheKafka_p_SerializationFormat">SerializationFormat</a>



		
<LI>

<a href="#RSBApacheKafka_p_Timeout">Timeout</a>



		
<LI>

<a href="#RSBApacheKafka_p_TypeDetectionScheme">TypeDetectionScheme</a>



		
<LI>

<a href="#RSBApacheKafka_p_UseConfluentAvroFormat">UseConfluentAvroFormat</a>



		
<LI>

<a href="#RSBApacheKafka_p_UseConnectionPooling">UseConnectionPooling</a>



		
<LI>

<a href="#RSBApacheKafka_p_UserDefinedViews">UserDefinedViews</a>



		
<LI>

<a href="#RSBApacheKafka_p_ValidateRegistryTopics">ValidateRegistryTopics</a>


</UL>












	</UL>





<LI>

    <a href="#copyright">Third Party Copyrights</a>


</UL>

<!-- END TOC CONTENT -->

</div>              
        
                </div>
            </div>
        <!-- /TABLE OF CONTENTS -->    
        <div id="whrightcol">
				<div style='margin:0; padding:0;'>
						
						<div id=newver ></div>
				</div>
				
				<div id=whiframe>
               
					<! -- BEGIN CONTENT -->					<div id="wrapper">
						<div id="content">
							<h1 id="default">CData JDBC Driver for Apache Kafka</h1>
              <!-- <span id=whtitle>
              &prodname; - Build &prod.vermaj;.&prod.vermin;.&prod:verint;
              </span> -->



<p>
<h2>Overview</h2>

</p>

<p>The CData JDBC Driver for Apache Kafka offers the most natural way to connect to Apache Kafka data from Java-based applications and developer technologies. The driver wraps the complexity of accessing Apache Kafka data in an easy-to-integrate, 100%-Java JDBC driver. Applications can then access Apache Kafka as a traditional database.  The driver hides the complexity of accessing data and provides additional powerful security features, smart caching, batching, socket management, and more.


<h2>Key Features</h2>
</p>

<p><ul><li>Write SQL to retrieve and update Apache Kafka data.</li><li>Compliant with JDBC 3.0 and JDBC 4.0.</li><li>Codeless integration with popular BI, reporting, and ETL tools.</li></ul>



<h2>Getting Started</h2>

See <a href="#pg_startedintroj">Getting Started</a> for A-Z guides on authenticating and connecting to Apache Kafka data.
See the <a href="https://www.cdata.com/kb/tech/apachekafka-article-list.rst">Apache Kafka integration guides</a> for information on connecting from other applications.</p>

<p><b>NOTE:</b> The license file, <var>cdata.jdbc.apachekafka.lic</var>, must be in the same location as the JAR file, <var>cdata.jdbc.apachekafka.jar</var>. See the readme file for more information.</p>

<p>
<h2>Using the JDBC Driver/Using from Tools</h2>
</p>

<p>See <a href="#pg_usagej">Using JDBC</a> for examples of using standard JDBC classes like DataSource, Connection, Statement, ResultSet, and others, to work with Apache Kafka data. </p>

<p><a href="#pg_jdbctoolsintro">Using from Tools</a> walks through the steps of integration with JDBC tools, using several popular database tools as examples.


<h2>Schema Discovery</h2>
</p>

<p>See <a href="#pg_systemtablesintro">Schema Discovery</a> to access schema information through the standard JDBC interfaces. Query the <a href="#pg_allsystables">System Tables</a> to access additional metadata, such as data source capabilities. 


<h2>Advanced Features</h2>

<a href="#pg_advancedfeatures">Advanced Features</a> details additional features supported by the driver, such as defining user defined views, ssl configuration, remoting, caching, firewall/proxy settings, and advanced logging.

<h3>JDBC Remoting</h3>
</p>

<p>See <a href="#pg_remotingintroj">JDBC Remoting</a> to configure remote access to the JDBC data source. The JDBC remoting feature allows hosting the JDBC connection on a server to enable connections from virtually anywhere -- various clients on any platform (Java, .NET, C++, PHP, Python, and so on) and using any standards-based technology (ODBC, JDBC, and so on). JDBC remoting is enabled using the popular MySQL wire protocol server.


<h2>SQL Compliance</h2>
</p>

<p>See <a href="#pg_overview">SQL Compliance</a> for a syntax reference and code examples outlining the supported SQL.

    
<h2>Data Model</h2>

    </p>

<p>See <a href="#pg_datamodel">Data Model</a> for information on the available entities and how to query them.



<h2>Connection String Options</h2>
</p>

<p>The <a href="#Connection">Connection</a> properties describe the various options that can be used to establish a connection.
</p>





  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - Getting Started" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - Getting Started: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>Getting Started</h1>
       

    <div class="chapter_content" id="pg_startedintroj">

<p>
<h2>Connecting to Apache Kafka</h2>
</p>

<p><a href="#pg_connectionj">Establishing a Connection</a> shows how to authenticate to Apache Kafka and configure any necessary connection properties in a JDBC URL. You can also configure driver capabilities through the available <a href="#Connection">Connection</a> properties, from data modeling to firewall traversal. The Advanced Settings section shows how to set up more advanced configurations and troubleshoot connection errors. 


<h2>Connecting to JDBC Data Sources</h2>
</p>

<p>The CData JDBC Driver for Apache Kafka provides full support for integration into Java applications, including Eclipse, NetBeans, IntelliJ IDEA, and many other Integrated Development Environments, as well as J2EE applications running on a Java server such as Tomcat. You can find JSP, console, and swing demos in the installation folder.

<h2>Java Version Support</h2>

To deploy the driver JAR file, you must have Java Development Kit (JDK) 1.8 or higher installed on your system. 

<h2>Apache Kafka Version Support</h2>
The driver leverages the Apache Kafka client libraries to enable bidirectional access to Kafka topics.


<h2>See Also</h2>

<ul><li>
<h3>Create Connection Objects </h3>
See <a href="#pg_JDBCconnectcode">Connecting from Code</a> to create JDBC Connection objects.
  </li><li>
<h3>Query Data from Code</h3>
See <a href="#pg_JDBCqueries">Executing Statements</a> and <a href="#pg_JDBCupdates">Using Prepared Statements</a> to execute SQL statements to Apache Kafka tables.</li><li>
<h3>Connect from Java-Based Tools</h3>
See <a href="#pg_jdbctoolsintro">Using from Tools</a> shows how to connect to Apache Kafka and query data from several popular database tools.</li></ul>
</p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka -  Licensing" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka -  Licensing: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1> Licensing</h1>
       

    <div class="chapter_content" id="pg_licensingjdbc">

<p><b>Automatic Installation</b></p>

<p>
For Windows, the installer should automatically install the license. If the installation fails or you need to install the license on an offline computer, you can do a manual install instead.</p>

<p>
<b>Manual Installation</b></p>

<p>
For Mac/Unix, you need to install a license from the command line via cdata.jdbc.apachekafka.jar. To do a manual install, execute the following command: 

<br/><pre lang="">java -jar cdata.jdbc.apachekafka.jar --license</pre>

This creates a cdata.jdbc.apachekafka.lic that must be located next to the .jar file or in the cdata directory under the user's home directory.</p>

<p>In certain cases, you may need to do a manual installation on Windows. The procedure is the same. 


</p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka -  Establishing a Connection" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka -  Establishing a Connection: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1> Establishing a Connection</h1>
       

    <div class="chapter_content" id="pg_connectionj">

<p>
<h2>Creating a JDBC Data Source</h2>

</p>

<p>You can create a JDBC data source to connect from your Java application. Creating a JDBC data source based on the CData JDBC Driver for Apache Kafka consists of three basic steps:
<ul><li>Add the driver JAR file to the classpath. The JAR file is located in the lib subfolder of the installation directory. Note that the .lic file must be located in the same folder as the JAR file.</li><li>Provide the driver class. For example:
<br/><pre lang="">cdata.jdbc.apachekafka.ApacheKafkaDriver</pre></li><li>Provide the JDBC URL. For example:
<br/><pre lang="plain">jdbc:apachekafka:User=admin;Password=pass;BootStrapServers=https://localhost:9091;Topic=MyTopic;

or

jdbc:cdata:apachekafka:User=admin;Password=pass;BootStrapServers=https://localhost:9091;Topic=MyTopic;</pre>

<p>The second format above can be used whenever there is a conflict in your application between drivers using the same URL format to ensure you are using the CData driver. The URL must start with either "jdbc:apachekafka:" or "jdbc:cdata:apachekafka:" and can include any of the connection properties in name-value pairs separated with semicolons.</p>
</li></ul>
</p>

<p>The CData JDBC Driver for Apache Kafka relies on the Confluent.Kafka and librdkafka libraries to function. These assemblies are bundled with the installer and automatically installed alongside the driver.

<h2>Connecting to Apache Kafka</h2>
</p>

<p>Set <u>BootstrapServers</u> and the <u>Topic</u> properties to specify the address of your Apache Kafka server, as well as the topic you would like to interact with.
</p>

<p>By default, the driver communicates with the data source in PLAINTEXT, which means that all data is sent in the clear. To encrypt communication, you should configure the driver to use SSL encryption. 
To do this, set <u>UseSSL</u> to true and configure <u>SSLServerCert</u> and <u>SSLServerCertType</u> to load the server certificates.
JKS and PEM files are supported certificate stores.
</p>

<p>Note that proxy settings like <u>ProxyServer</u> and firewall settings like <u>FirewallServer</u> do not affect the connection to the Apache Kafka broker.
Internally, the driver connects to Apache Kafka using the official libraries which do not support proxies.
These options are only used when the driver connects to the schema registry as described in <a href="#pg_readingtopics">Extracting Metadata From Topics</a>.


<h2>Authenticating to Apache Kafka</h2>
</p>

<p>The Apache Kafka data source supports the following authentication methods: 
<ul><li>Anonymous</li><li>Plain</li><li>Scram</li><li>Kerberos</li></ul>


<h3>Anonymous</h3>
</p>

<p>Certain on-premise deployments of Apache Kafka are able to connect to Apache Kafka without setting any authentication connection properties. To do so, simply set the <u>AuthScheme</u> to "None", and you are ready to connect.


<h3>SASL Plain</h3>
</p>

<p>The <u>User</u> and <u>Password</u> properties should be specified. <u>AuthScheme</u> should be set to <b>Plain</b>.


<h3>SCRAM login module</h3>
</p>

<p>The <u>User</u> and <u>Password</u> properties should be specified. The <u>AuthScheme</u> should be set to 'SCRAM' (for SCRAM-SHA-256) or 'SCRAM-SHA-512'.


<h3>SSL client certificates</h3>
</p>

<p>The <u>SSLClientCert</u> and <u>SSLClientCertType</u> properties should be specified and <u>AuthScheme</u> should be set to <b>SSLCertificate</b>.
The JKS certificate format is recommended but both PEM and JKS are supported.
Using PEM often requires several conversion steps as Java only supports a subset of the encodings and encryption methods supported by tools like OpenSSL.


<h3>Kerberos</h3>

To authenticate to Apache Kafka using Kerberos, set the following properties:

<ul><li><b><u>AuthScheme</u></b>: Set this to <b>KERBEROS</b>.</li><li><b><u>KerberosServiceName</u></b>: This should match to the principal name of the Kafka brokers. For example, the principal is "kafka/kafka1.hostname.com@EXAMPLE.COM", so: <u>KerberosServiceName</u>=kafka.</li><li><b><u>KerberosKeytabFile</u></b>: Set this to The Keytab file absolute path containing your pairs of Kerberos principals and encrypted keys.</li><li><b><u>KerberosSPN</u></b>: Set this to the <b>service and host of the Apache Kafka Kerberos Principal</b>.  This will be the value prior to the '@' symbol (for instance, kafka/kafka1.hostname.com) of the <b>principal value</b> (for instance, kafka/kafka1.hostname.com@EXAMPLE.COM).</li></ul>


<h3>Use Ticket cache</h3>
</p>

<p>You can set the <u>UseKerberosTicketCache</u> to TRUE in order to use a ticket cache instead of specifying the keytab file. In that case, the <u>KerberosKeytabFile</u> will be ignored even if it's specified.



<h2>Connecting to Azure Event Hubs</h2>

</p>

<p>The driver supports connecting to Azure Event Hubs using OAuth and shared-access signatures.
Before you begin, check that your Event Hubs namespace supports connections using the Kafka protocol.
The driver requires this feature and it may not be available for certain pricing tiers.
</p>

<p>All connections to Azure must set these properties, in addition to the scheme-specific properties covered below.

<ul><li><b><u>BootstrapServers</u></b>: Set this to <b>https://mynamespace.servicebus.windows.net:9093</b>.</li><li><b><u>UseSSL</u></b>: Set this to true.</li></ul>
</p>

<p>
<h3>Azure AD</h3>

Azure AD is Microsoftâ€™s multi-tenant, cloud-based directory and identity management service. It is user-based authentication that requires that you set <u>AuthScheme</u> to <b>AzureAD</b>.</p>

<p>

<h4>Desktop Applications</h4>

CData provides an embedded OAuth application that simplifies authentication at the desktop; that is, in situations where the user is using a local server not connected to the internet.</p>

<p>Before you connect, set the following variables:
<ul><li><u>InitiateOAuth</u>: <b>GETANDREFRESH</b>. Used to automatically get and refresh the <u>OAuthAccessToken</u>.
CData provides an embedded OAuth application that simplifies authentication at the desktop; that is, in situations where the user is using a local server not connected to the internet.

<p>You can also authenticate from the desktop via a custom OAuth application, which you configure and register at the Apache Kafka console. For further information, see <a href="#pg_azureadoauthcustomappcreate">Creating a Custom OAuth Application</a>.					   </p>
</li><li><b>Custom Azure AD applications only:</b>
<ul><li><u>OAuthClientId</u>: The client Id assigned when you registered your custom OAuth application. </li><li><u>OAuthClientSecret</u>: The client secret assigned when you registered your custom OAuth application. </li><li><u>CallbackURL</u>: The redirect URI defined when you registered your custom OAuth application. </li></ul></li></ul></p>

<p>When you connect, the driver opens the Apache Kafka's OAuth endpoint in your default browser. Log in and grant permissions to the application.  The driver then completes the OAuth process:								   
<ol><li>Extracts the access token from the callback URL and authenticates requests.</li><li>Obtains a new access token when the old one expires.</li><li>Saves OAuth values in <u>OAuthSettingsLocation</u>. These values persist across connections. </li></ol>
 </p>

<p>When the access token expires, the driver refreshes it automatically.</p>

<p>   
 

<h4>Web Applications</h4>

Authenticating via the Web requires you to create and register a custom OAuth application with Apache Kafka, as described in <a href="#pg_azureadoauthcustomappcreate">Creating a Custom OAuth Application</a>. You can then use the driver to get and manage the OAuth token values.</p>

<p>This section describes how to get the OAuth access token, how to have the driver refresh the OAuth access token automatically, and how to refresh the OAuth access token manually.
</p>

<p><b>Get the OAuth access token:</b>				   
</p>

<p><ol><li>To obtain the <u>OAuthAccessToken</u>, set these connection properties:	
<ul><li>For authentication using a Client Secret:
<ul><li><u>OAuthClientId</u>: The client Id in your application settings.</li><li><u>OAuthClientSecret</u>: The client secret in your application settings.</li></ul></li><li>For authentication using a Certificate:
<ul><li><u>OAuthClientId</u>: The client Id in your application settings.</li><li><u>OAuthJWTCert</u>: The JWT Certificate store.</li><li><u>OAuthJWTCertType</u>: The type of the certificate store specified by OAuthJWTCert.</li></ul></li></ul></li><li>Call stored procedures to complete the OAuth exchange:
<ul><li>Call the <a href="#pg_sp-getoauthauthorizationurl">GetOAuthAuthorizationURL</a> stored procedure. Set the AuthMode input to <b>WEB</b> and the <u>CallbackURL</u> to the Redirect URI you specified in your application settings.
The stored procedure returns the URL to the OAuth endpoint.</li><li>Navigate to the URL that the stored procedure returned in Step 1. Log in and authorize the web application. You are redirected back to the callback URL. </li><li>Call the <a href="#pg_sp-getoauthaccesstoken">GetOAuthAccessToken</a> stored procedure. Set the AuthMode input to <b>WEB</b>. Set the Verifier input to the <var>code</var> parameter in the query string of the redirect URI.</li></ul>	 </li></ol>
</p>

<p>After you obtain the access and refresh tokens, you can connect to data and refresh the OAuth access token automatically.
</p>

<p><b>Automatic refresh of the OAuth access token:</b></p>

<p>	
To have the driver automatically refresh the OAuth access token, do the following:</p>

<p><ol><li>The first time you connect to data, set these connection parameters:
<ul><li><u>InitiateOAuth</u>: <b>REFRESH</b>.</li><li><u>OAuthClientId</u>: The client Id in your custom OAuth application settings.</li><li><u>OAuthClientSecret</u>: The client secret in your custom OAuth application settings.</li><li><u>OAuthAccessToken</u>: The access token returned by <a href="#pg_sp-getoauthaccesstoken">GetOAuthAccessToken</a>.</li><li><u>OAuthSettingsLocation</u>: The path where you want the driver to save the OAuth values, which persist across connections.</li></ul></li><li>On subsequent data connections, set:
<ul><li><u>InitiateOAuth</u></li><li><u>OAuthSettingsLocation</u></li></ul></li></ol></p>

<p><b>Manual refresh of the OAuth access token:</b>				</p>

<p>The only value needed to manually refresh the OAUth access token is the OAuth refresh token.</p>

<p><ol><li>To manually refresh the OAuthAccessToken after the ExpiresIn period (returned by <a href="#pg_sp-getoauthaccesstoken">GetOAuthAccessToken</a>) has elapsed, call the <a href="#pg_sp-refreshoauthaccesstoken">RefreshOAuthAccessToken</a> stored procedure.</li><li>Set these connection properties:
<ul><li><u>OAuthClientId</u>: The Client Id in your custom OAuth application settings.</li><li><u>OAuthClientSecret</u>: The Client Secret in your custom OAuth application settings.</li></ul>

<p></p>
</li><li>Call <a href="#pg_sp-refreshoauthaccesstoken">RefreshOAuthAccessToken</a> with OAuthRefreshToken set to the OAuth refresh token returned by GetOAuthAccessToken. </li><li>After the new tokens have been retrieved, set the <u>OAuthAccessToken</u> property to the value returned by <a href="#pg_sp-refreshoauthaccesstoken">RefreshOAuthAccessToken</a>. This opens a new connection. </li></ol></p>

<p>Store the OAuth refresh token so that you can use it to manually refresh the OAuth access token after it has expired.

</p>

<p>
<h4>Headless Machines</h4>
 
If you need to log in to a resource that resides on a headless machine, you must authenticate on another device that 
has an internet browser. You can do this in either of the following ways:</p>

<p><ul><li>Obtain the <u>OAuthVerifier</u> value as described in <i>Obtain and Exchange a Verifier Code</i>, below.</li><li>Install the driver on another machine and transfer the OAuth authentication values after you authenticate through the usual browser-based flow.</li></ul></p>

<p>After you execute either of these options, configure the driver to automatically refresh the access token on the headless machine. 
</p>

<p><b>Obtaining and Exchanging a Verifier Code</b></p>

<p>To obtain a verifier code, you must authenticate at the OAuth authorization URL from a machine with an internet browser, and obtain the <u>OAuthVerifier</u> connection property.</p>

<p><ol><li>Choose one of these options: 
<p><ul><li>If you are using the Embedded OAuth Application, click <a href="">Apache Kafka OAuth endpoint</a> to open the endpoint in your browser.</li><li>If you are using a custom OAuth application, set the following properties to create the Authorization URL:
<ul><li><u>InitiateOAuth</u>: <b>OFF</b>.
    </li><li><u>OAuthClientId</u>: The client Id assigned when you registered your application.
    </li><li><u>OAuthClientSecret</u>: The client secret assigned when you registered your application. </li></ul>
    After the Authorization URL is established, call the <a href="#pg_sp-getoauthauthorizationurl">GetOAuthAuthorizationURL</a> stored procedure with the appropriate CallbackURL.  Open the URL returned by the stored procedure in a browser.</li></ul></p>
</li><li>Log in and grant permissions to the driver. You are redirected to the callback URL, which contains the verifier code.</li><li>Save the value of the verifier code. Later you will set this in the <u>OAuthVerifier</u> connection property.</li></ol>
    
Next, exchange the OAuth verifier code for OAuth refresh and access tokens. 
</p>

<p>To obtain the OAuth authentication values, set these properties:
<ul><li><u>InitiateOAuth</u>: <b>REFRESH</b>.</li><li><u>OAuthVerifier</u>: The verifier code.</li><li><u>OAuthSettingsLocation</u>: The location of the file where the driver saves the OAuth token values that persist across connections.</li><li><b>Custom applications only:</b>
<ul><li><u>OAuthClientId</u>: (custom applications only) Set this to the client Id in your custom OAuth application settings.</li><li><u>OAuthClientSecret</u>: (custom applications only) Set this to the client secret in the custom OAuth application settings.</li></ul></li></ul>
</p>

<p>After the OAuth settings file is generated, re-set the following properties to connect: 
<ul><li><u>InitiateOAuth</u>: <b>REFRESH</b>.</li><li><u>OAuthSettingsLocation</u>: The location containing the encrypted OAuth authentication values. Make sure this location grants read and write permissions to the driver to enable the automatic refreshing of the access token. </li><li><b>Custom applications only:</b>
<ul><li><u>OAuthClientId</u>: The client Id assigned when you registered your application.</li><li><u>OAuthClientSecret</u>: The client secret assigned when you registered your application.</li></ul></li></ul>
    </p>

<p><b>Transferring OAuth Settings</b></p>

<p>     
Prior to connecting on a headless machine, you must create and install a connection with the driver on a device that supports an internet browser. Set the connection properties as described in "Desktop Applications" above.
</p>

<p>After completing the instructions in "Desktop Applications", the resulting authentication values are encrypted and written to the location specified by <u>OAuthSettingsLocation</u>. The default filename is <var>OAuthSettings.txt</var>. </p>

<p> 
Once you have successfully tested the connection, copy the OAuth settings file to your headless machine. 
      </p>

<p>On the headless machine, set the following connection properties to connect to data:
<ul><li><u>InitiateOAuth</u>: <b>REFRESH</b>.</li><li><u>OAuthSettingsLocation</u>: The location of your OAuth settings file. Make sure this location gives read and write permissions to the driver to enable the automatic refreshing of the access token.</li><li><b>Custom applications only:</b>
<ul><li><u>OAuthClientId</u>: The client Id assigned when you registered your application.</li><li><u>OAuthClientSecret</u>:  The client secret assigned when you registered your application.</li></ul></li></ul></p>

<p>


<h3>Azure Service Principal</h3>
</p>

<p>Azure Service Principal is role-based application-based authentication. This means that authentication is done per application, rather than per user.
All tasks taken on by the application are executed without a default user context, but based on the assigned roles. 
The application access to the resources is controlled through the assigned roles' permissions. </p>

<p>For information about how to set up Azure Service Principal authentication, see <a href="#pg_azureadoauthcustomappcreate">Creating a Custom OAuth Application</a>.</p>

<p>
<h3>Managed Service Identity (MSI)</h3>
</p>

<p>If you are running Apache Kafka on an Azure VM and want to leverage MSI to connect, set <u>AuthScheme</u> to <b>AzureMSI</b>.


<h4>User-Managed Identities</h4>

To obtain a token for a managed identity, use the <u>OAuthClientId</u> property to specify the managed identity's "client_id".</p>

<p>It is also necessary to specify <u>OAuthClientId</u> when your VM has multiple user-assigned managed identities.


<h3>Shared-Access Signature</h3>
</p>

<p>The driver also supports password-based authentication using shared-access signatures.
Once you have created a shared secret, configure the driver with these options:

<ul><li><b><u>AuthScheme</u></b>: Set this to <b>Plain</b>.</li><li><b><u>User</u></b>: Set this to <b>$ConnectionString</b>.</li><li><b><u>Password</u></b>: Set this to the Event Hubs connection string from the Shared Access Policies screen.</li></ul>

</p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka -  Building the JDBC URL" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka -  Building the JDBC URL: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1> Building the JDBC URL</h1>
       

    <div class="chapter_content" id="pg_jdbcconnstring">

<p>
<h2>Building the JDBC URL</h2>
</p>

<p>Connection strings provide information about a data source and how to connect to that data source. The driver comes with a connection string builder that makes it easier to create and manage the contents of connection strings.
</p>

<p>After downloading and installing the driver, double-click the .jar file in the lib folder. You can also manually run the .jar file, as shown in the following examples.
</p>

<p>From Windows:

<br/><pre lang="">java -jar 'C:\Program Files\CData\CData JDBC Driver for Apache Kafka 2023\lib\cdata.jdbc.apachekafka.jar'</pre>
</p>

<p>From macOS:

<br/><pre lang="">java -jar cdata.jdbc.apachekafka.jar</pre>
</p>

<p>Running the .jar file opens the <b>Connection Properties</b> dialog box. You can use this dialog box to build and test a connection string. Click <b>Test Connection</b> to test and validate the entered connection properties. Click <b>Copy to Clipboard</b> to copy the connection string for use within the application where the JDBC driver is being used.
</p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka -  Creating a Custom Authentication Application" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka -  Creating a Custom Authentication Application: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1> Creating a Custom Authentication Application</h1>
       

    <div class="chapter_content" id="pg_azureadoauthcustomappcreate">

<p>
<h2>Creating a Custom OAuth Application</h2>

Apache Kafka supports authentication using Azure AD and Azure Service Principal, both of which are OAuth-based.</p>

<p>This topic describes how to:
<ul><li>create and register custom OAuth application for Azure AD or Azure Service Principal</li><li>provide Admin Consent to a custom OAuth application </li><li>create a custom OAuth application for use with client credentials </li></ul>
</p>

<p>
<h3>Azure AD</h3>

In <var>portal.azure.com</var>:
<ol><li>Log in to <a href="https://portal.azure.com">https://portal.azure.com</a>.</li><li>In the left-hand navigation pane, select <b>Azure Active Directory</b>, then <b>applicationRegistrations</b>.</li><li>Click <b>New registration</b>.</li><li>Enter a name for the application.</li><li>Select the desired tenant setup: single- or multi-tenant, and public or private use.

<p><ul><li>If you select the default option, "Accounts in this organizational directory only", you must set the <u>AzureTenant</u> connection property to the Id of the Azure AD Tenant when establishing a connection with the CData JDBC Driver for Apache Kafka. Otherwise, the authentication attempt fails with an error. </li><li>If your application is for private use only, specify <b>Accounts in this organization directory only</b>. </li><li>If you want to distribute your application, choose one of the multi-tenant options.</li></ul></p>
</li><li>Set the redirect url to <var>http://localhost:33333</var> (the driver's default) OR specify a different port and set <u>CallbackURL</u> to the exact reply URL you defined.</li><li>Click <b>Register</b> to register the new application. An application management screen displays. 
<br>
Note the value in <b>Application (client) ID</b> as the <u>OAuthClientId</u> and the <b>Directory (tenant) ID</b> as the <u>AzureTenant</u>.</li><li>Navigate to <b>Certificates &amp; Secrets</b> and define the application authentication type. There are two types of authentication available: certificate (recommended) or client secret.

<p><ul><li>For certificate authentication: In <b>Certificates &amp; Secrets</b>, select <b>Upload certificate</b>, then upload the certificate from your local machine.</li><li>For creating a new client secret: In <b>Certificates &amp; Secrets</b>, select <b>New Client Secret</b> for the application and specify its duration. After the client secret is saved, Apache Kafka displays the key value. <i>Copy this value, as it is displayed only once.</i> This value becomes the <u>OAuthClientSecret</u>.</li></ul></p>
</li><li>Select <b>API Permissions &gt; Add &gt; Delegated permissions</b>.</li><li>Save your changes.</li><li>If you have specified the use of permissions that require admin consent (such as the Application Permissions), you can grant them from the current tenant on the API Permissions page.
</li></ol>
</p>

<p>
<h3>Azure Service Principal</h3>

To use Azure Service Principal authentication, you must set up the ability to assign a role to the authentication application, then register an application with the Azure AD tenant to create a new 
Service Principal. That new Service Principal can then leverage the assigned role-based access 
control to access resources in your subscription.</p>

<p>In <var>portal.azure.com</var>:
<ol><li>Create a custom OAuth AD application, as described above.</li><li>Use the search bar to search for the Subscriptions service.</li><li>Open the <b>Subscriptions</b> page.</li><li>Select the subscription to which to assign the application.</li><li>Open the <b>Access control (IAM)</b>.</li><li>Select <b>Add &gt; Add role assignment</b>. Apache Kafka opens the <b>Add role assignment</b> page.</li><li>Assign your custom Azure AD application the role of <b>Owner</b>.</li></ol>
</p>

<p>
<h2>Admin Consent</h2>
</p>

<p>Some custom applications require administrative permissions to operate within an Azure Active Directory tenant. Admin consent can be granted when creating a new custom OAuth application, by adding relevant permissions that are already marked with "Admin Consent Required". Admin consent is also required to use Client Credentials in the OAuth flow. </p>

<p>To grant admin consent:
<ol><li>Have an admin log in to <var>portal.azure.com</var>.</li><li>Navigate to <b>App Registrations</b> and find the custom OAuth application you created. </li><li>Under <b>API Permissions</b>, click <b>Grant Consent</b>. </li></ol>
This gives your application permissions on the tenant under which it was created.
</p>

<p>
<h4>Consent for Client Credentials</h4>

OAuth supports the use of client credentials to authenticate. In a client credentials OAuth flow, credentials are created for the authenticating application itself. The auth flow acts just like 
the usual auth flow except that there is no prompt for an associated user to provide credentials. 
All tasks accepted by the application are executed outside of the context of a default user.</p>

<p><b>Note:</b> Since the embedded OAuth credentials authenticate on a per-user basis, you cannot 
use them in a client OAuth flow. You must always create a custom OAuth application to use client 
credentials.</p>

<p>In <var>portal.azure.com</var>:
<ol><li>Create a custom OAuth application, as described above.</li><li>Navigate to <b>App Registrations</b>.</li><li>Find the application you just created, and open <b>API Permissions</b>.</li><li>Select the Microsoft Graph permissions. 
There are two distinct sets of permissions: Delegated and Application.</li><li>Under <b>Application Permissions</b>, select the permissions you require for your integration. </li></ol>
</p>

<p></p>

<p>
<h2>Creating a Custom OAuth Application</h2>

Apache Kafka supports authentication using Azure AD and Azure Service Principal, both of which are OAuth-based.</p>

<p>This topic describes how to:
<ul><li>create and register custom OAuth application for Azure AD or Azure Service Principal</li><li>provide Admin Consent to a custom OAuth application </li><li>create a custom OAuth application for use with client credentials </li></ul>
</p>

<p>
<h3>Azure AD</h3>

In <var>portal.azure.com</var>:
<ol><li>Log in to <a href="https://portal.azure.com">https://portal.azure.com</a>.</li><li>In the left-hand navigation pane, select <b>Azure Active Directory</b>, then <b>applicationRegistrations</b>.</li><li>Click <b>New registration</b>.</li><li>Enter a name for the application.</li><li>Select the desired tenant setup: single- or multi-tenant, and public or private use.

<p><ul><li>If you select the default option, "Accounts in this organizational directory only", you must set the <u>AzureTenant</u> connection property to the Id of the Azure AD Tenant when establishing a connection with the CData JDBC Driver for Apache Kafka. Otherwise, the authentication attempt fails with an error. </li><li>If your application is for private use only, specify <b>Accounts in this organization directory only</b>. </li><li>If you want to distribute your application, choose one of the multi-tenant options.</li></ul></p>
</li><li>Set the redirect url to <var>http://localhost:33333</var> (the driver's default) OR specify a different port and set <u>CallbackURL</u> to the exact reply URL you defined.</li><li>Click <b>Register</b> to register the new application. An application management screen displays. 
<br>
Note the value in <b>Application (client) ID</b> as the <u>OAuthClientId</u> and the <b>Directory (tenant) ID</b> as the <u>AzureTenant</u>.</li><li>Navigate to <b>Certificates &amp; Secrets</b> and define the application authentication type. There are two types of authentication available: certificate (recommended) or client secret.

<p><ul><li>For certificate authentication: In <b>Certificates &amp; Secrets</b>, select <b>Upload certificate</b>, then upload the certificate from your local machine.</li><li>For creating a new client secret: In <b>Certificates &amp; Secrets</b>, select <b>New Client Secret</b> for the application and specify its duration. After the client secret is saved, Apache Kafka displays the key value. <i>Copy this value, as it is displayed only once.</i> This value becomes the <u>OAuthClientSecret</u>.</li></ul></p>
</li><li>Select <b>API Permissions &gt; Add &gt; Delegated permissions</b>.</li><li>Save your changes.</li><li>If you have specified the use of permissions that require admin consent (such as the Application Permissions), you can grant them from the current tenant on the API Permissions page.
</li></ol>
</p>

<p>
<h3>Azure Service Principal</h3>

To use Azure Service Principal authentication, you must set up the ability to assign a role to the authentication application, then register an application with the Azure AD tenant to create a new 
Service Principal. That new Service Principal can then leverage the assigned role-based access 
control to access resources in your subscription.</p>

<p>In <var>portal.azure.com</var>:
<ol><li>Create a custom OAuth AD application, as described above.</li><li>Use the search bar to search for the Subscriptions service.</li><li>Open the <b>Subscriptions</b> page.</li><li>Select the subscription to which to assign the application.</li><li>Open the <b>Access control (IAM)</b>.</li><li>Select <b>Add &gt; Add role assignment</b>. Apache Kafka opens the <b>Add role assignment</b> page.</li><li>Assign your custom Azure AD application the role of <b>Owner</b>.</li></ol>
</p>

<p>
<h2>Admin Consent</h2>
</p>

<p>Some custom applications require administrative permissions to operate within an Azure Active Directory tenant. Admin consent can be granted when creating a new custom OAuth application, by adding relevant permissions that are already marked with "Admin Consent Required". Admin consent is also required to use Client Credentials in the OAuth flow. </p>

<p>To grant admin consent:
<ol><li>Have an admin log in to <var>portal.azure.com</var>.</li><li>Navigate to <b>App Registrations</b> and find the custom OAuth application you created. </li><li>Under <b>API Permissions</b>, click <b>Grant Consent</b>. </li></ol>
This gives your application permissions on the tenant under which it was created.
</p>

<p>
<h4>Consent for Client Credentials</h4>

OAuth supports the use of client credentials to authenticate. In a client credentials OAuth flow, credentials are created for the authenticating application itself. The auth flow acts just like 
the usual auth flow except that there is no prompt for an associated user to provide credentials. 
All tasks accepted by the application are executed outside of the context of a default user.</p>

<p><b>Note:</b> Since the embedded OAuth credentials authenticate on a per-user basis, you cannot 
use them in a client OAuth flow. You must always create a custom OAuth application to use client 
credentials.</p>

<p>In <var>portal.azure.com</var>:
<ol><li>Create a custom OAuth application, as described above.</li><li>Navigate to <b>App Registrations</b>.</li><li>Find the application you just created, and open <b>API Permissions</b>.</li><li>Select the Microsoft Graph permissions. 
There are two distinct sets of permissions: Delegated and Application.</li><li>Under <b>Application Permissions</b>, select the permissions you require for your integration. </li></ol>
</p>

<p></p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka -  Extracting Metadata From Topics" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka -  Extracting Metadata From Topics: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1> Extracting Metadata From Topics</h1>
       

    <div class="chapter_content" id="pg_readingtopics">

<p>
<h2>Reading Apache Kafka Data</h2>
</p>

<p>Reads in Apache Kafka don't have a natural stopping point. To avoid perpetual read operations, items are read until the <u>ReadDuration</u> or <u>Timeout</u> expires. <u>ReadDuration</u> is set to 30 seconds by default.</p>

<p>The driver models topics as tables and messages as rows.</p>

<p>It facilitates this in two ways:
<ol><li>For services that contain a schema registry, such as Confluent and AWS hosted instances, the schema is read directly from the schema registry.</li><li>For services that do not contain a schema registry, the schema is inferred by the driver.</li></ol>


<h2>Schema Registry</h2>
</p>

<p>Set the following to connect to a service with a schema registry:
<ul><li><u>BootstrapServers</u>: The server (hostname or IP address) and port (in the format server:port) of the Apache Kafka BoostrapServers.</li><li><u>TypeDetectionScheme</u>: Set to <b>SchemaRegistry</b>. </li><li><u>RegistryAuthScheme</u>: Set to the appropriate authentication method, see the next sections for details.</li><li><u>RegistryService</u>: The schema registry service used to read topic schemas. The options are <b>Confluent</b> and <b>AWSGlue</b>.</li><li><u>RegistryUrl</u>: Set to the server for the schema registry.</li></ul>
</p>

<p>The schema registry contains a list of topics which have registered schemas. The list of tables and columns are simply read directly from the schema registry.


<h3>Confluent Schema Registry</h3>
</p>

<p>When you connect to Confluent Cloud, the <u>RegistryUrl</u> corresponds to the Schema Registry endpoint value in <b>Schemas -&gt; Schema Registry -&gt; Instructions</b>.
</p>

<p>The Confluent schema registry supports several authentication options.
Confluent Cloud deployments will typically require <u>RegistryAuthScheme</u> to be set to <b>Basic</b>, along with a <u>RegistryUser</u> and <u>RegistryPassword</u>.
These can be found by navigating to <b>Schemas &gt; Schema Registry &gt; API Access</b> and finding the access key and secret key values.
</p>

<p>On-premise deployments may not require authentication, in these configurations <u>RegistryAuthScheme</u> should be set to <b>None</b>.
They may also require SSL client certificates, which can be set using the <b>SSLCertificate</b> <u>RegistryAuthScheme</u> along with the <u>RegistryClientCert</u> and <u>RegistryClientCertType</u> options. 


<h3>AWS Glue Schema Registry</h3>
</p>

<p>When connecting to AWS Glue, the <u>RegistryUrl</u> corresponds to the ARN value of the registry.
</p>

<p>The AWS Glue schema registry only supports the <b>Basic</b> <u>RegistryAuthScheme</u>. 
<u>RegistryUser</u> and <u>RegistryPassword</u>, and should be set to the access key and secret key of a user with access to the registry.


<h3>No Schema Registry</h3>
</p>

<p>Set the following to connect to a service without a schema registry:
<ul><li><u>BootstrapServers</u>: The server (hostname or IP address) and port (in the format server:port) of the Apache Kafka BoostrapServers.</li><li><u>TypeDetectionScheme</u>: Set to <b>RowScan</b>. </li></ul></p>

<p>Schema discovery is performed as follows:
<ol><li>An attempt is made to autodetect the format (AVRO/JSON/XML/CSV). This can also be set explicitly with the <u>SerializationFormat</u> property.</li><li>With the format read, rows are analyzed from the topic. Set a higher <u>RowScanDepth</u> for increased accuracy, though higher depth may decrease performance. The driver begins reading at the current offset (configurable via the <u>OffsetResetStrategy</u> property). From this point, future SELECTs will start from the beginning.</li><li>Deserialization is performed based on the determined serialization format, completing schema discovery.</li></ol>
</p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka -  Creating a Custom OAuth Application" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka -  Creating a Custom OAuth Application: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1> Creating a Custom OAuth Application</h1>
       

    <div class="chapter_content" id="pg_azureadoauthcustomappcreate">

<p>
<h2>Creating a Custom OAuth Application</h2>

Apache Kafka supports authentication using Azure AD and Azure Service Principal, both of which are OAuth-based.</p>

<p>This topic describes how to:
<ul><li>create and register custom OAuth application for Azure AD or Azure Service Principal</li><li>provide Admin Consent to a custom OAuth application </li><li>create a custom OAuth application for use with client credentials </li></ul>
</p>

<p>
<h3>Azure AD</h3>

In <var>portal.azure.com</var>:
<ol><li>Log in to <a href="https://portal.azure.com">https://portal.azure.com</a>.</li><li>In the left-hand navigation pane, select <b>Azure Active Directory</b>, then <b>applicationRegistrations</b>.</li><li>Click <b>New registration</b>.</li><li>Enter a name for the application.</li><li>Select the desired tenant setup: single- or multi-tenant, and public or private use.

<p><ul><li>If you select the default option, "Accounts in this organizational directory only", you must set the <u>AzureTenant</u> connection property to the Id of the Azure AD Tenant when establishing a connection with the CData JDBC Driver for Apache Kafka. Otherwise, the authentication attempt fails with an error. </li><li>If your application is for private use only, specify <b>Accounts in this organization directory only</b>. </li><li>If you want to distribute your application, choose one of the multi-tenant options.</li></ul></p>
</li><li>Set the redirect url to <var>http://localhost:33333</var> (the driver's default) OR specify a different port and set <u>CallbackURL</u> to the exact reply URL you defined.</li><li>Click <b>Register</b> to register the new application. An application management screen displays. 
<br>
Note the value in <b>Application (client) ID</b> as the <u>OAuthClientId</u> and the <b>Directory (tenant) ID</b> as the <u>AzureTenant</u>.</li><li>Navigate to <b>Certificates &amp; Secrets</b> and define the application authentication type. There are two types of authentication available: certificate (recommended) or client secret.

<p><ul><li>For certificate authentication: In <b>Certificates &amp; Secrets</b>, select <b>Upload certificate</b>, then upload the certificate from your local machine.</li><li>For creating a new client secret: In <b>Certificates &amp; Secrets</b>, select <b>New Client Secret</b> for the application and specify its duration. After the client secret is saved, Apache Kafka displays the key value. <i>Copy this value, as it is displayed only once.</i> This value becomes the <u>OAuthClientSecret</u>.</li></ul></p>
</li><li>Select <b>API Permissions &gt; Add &gt; Delegated permissions</b>.</li><li>Save your changes.</li><li>If you have specified the use of permissions that require admin consent (such as the Application Permissions), you can grant them from the current tenant on the API Permissions page.
</li></ol>
</p>

<p>
<h3>Azure Service Principal</h3>

To use Azure Service Principal authentication, you must set up the ability to assign a role to the authentication application, then register an application with the Azure AD tenant to create a new 
Service Principal. That new Service Principal can then leverage the assigned role-based access 
control to access resources in your subscription.</p>

<p>In <var>portal.azure.com</var>:
<ol><li>Create a custom OAuth AD application, as described above.</li><li>Use the search bar to search for the Subscriptions service.</li><li>Open the <b>Subscriptions</b> page.</li><li>Select the subscription to which to assign the application.</li><li>Open the <b>Access control (IAM)</b>.</li><li>Select <b>Add &gt; Add role assignment</b>. Apache Kafka opens the <b>Add role assignment</b> page.</li><li>Assign your custom Azure AD application the role of <b>Owner</b>.</li></ol>
</p>

<p>
<h2>Admin Consent</h2>
</p>

<p>Some custom applications require administrative permissions to operate within an Azure Active Directory tenant. Admin consent can be granted when creating a new custom OAuth application, by adding relevant permissions that are already marked with "Admin Consent Required". Admin consent is also required to use Client Credentials in the OAuth flow. </p>

<p>To grant admin consent:
<ol><li>Have an admin log in to <var>portal.azure.com</var>.</li><li>Navigate to <b>App Registrations</b> and find the custom OAuth application you created. </li><li>Under <b>API Permissions</b>, click <b>Grant Consent</b>. </li></ol>
This gives your application permissions on the tenant under which it was created.
</p>

<p>
<h4>Consent for Client Credentials</h4>

OAuth supports the use of client credentials to authenticate. In a client credentials OAuth flow, credentials are created for the authenticating application itself. The auth flow acts just like 
the usual auth flow except that there is no prompt for an associated user to provide credentials. 
All tasks accepted by the application are executed outside of the context of a default user.</p>

<p><b>Note:</b> Since the embedded OAuth credentials authenticate on a per-user basis, you cannot 
use them in a client OAuth flow. You must always create a custom OAuth application to use client 
credentials.</p>

<p>In <var>portal.azure.com</var>:
<ol><li>Create a custom OAuth application, as described above.</li><li>Navigate to <b>App Registrations</b>.</li><li>Find the application you just created, and open <b>API Permissions</b>.</li><li>Select the Microsoft Graph permissions. 
There are two distinct sets of permissions: Delegated and Application.</li><li>Under <b>Application Permissions</b>, select the permissions you require for your integration. </li></ol>
</p>

<p></p>

<p>
<h2>Creating a Custom OAuth Application</h2>

Apache Kafka supports authentication using Azure AD and Azure Service Principal, both of which are OAuth-based.</p>

<p>This topic describes how to:
<ul><li>create and register custom OAuth application for Azure AD or Azure Service Principal</li><li>provide Admin Consent to a custom OAuth application </li><li>create a custom OAuth application for use with client credentials </li></ul>
</p>

<p>
<h3>Azure AD</h3>

In <var>portal.azure.com</var>:
<ol><li>Log in to <a href="https://portal.azure.com">https://portal.azure.com</a>.</li><li>In the left-hand navigation pane, select <b>Azure Active Directory</b>, then <b>applicationRegistrations</b>.</li><li>Click <b>New registration</b>.</li><li>Enter a name for the application.</li><li>Select the desired tenant setup: single- or multi-tenant, and public or private use.

<p><ul><li>If you select the default option, "Accounts in this organizational directory only", you must set the <u>AzureTenant</u> connection property to the Id of the Azure AD Tenant when establishing a connection with the CData JDBC Driver for Apache Kafka. Otherwise, the authentication attempt fails with an error. </li><li>If your application is for private use only, specify <b>Accounts in this organization directory only</b>. </li><li>If you want to distribute your application, choose one of the multi-tenant options.</li></ul></p>
</li><li>Set the redirect url to <var>http://localhost:33333</var> (the driver's default) OR specify a different port and set <u>CallbackURL</u> to the exact reply URL you defined.</li><li>Click <b>Register</b> to register the new application. An application management screen displays. 
<br>
Note the value in <b>Application (client) ID</b> as the <u>OAuthClientId</u> and the <b>Directory (tenant) ID</b> as the <u>AzureTenant</u>.</li><li>Navigate to <b>Certificates &amp; Secrets</b> and define the application authentication type. There are two types of authentication available: certificate (recommended) or client secret.

<p><ul><li>For certificate authentication: In <b>Certificates &amp; Secrets</b>, select <b>Upload certificate</b>, then upload the certificate from your local machine.</li><li>For creating a new client secret: In <b>Certificates &amp; Secrets</b>, select <b>New Client Secret</b> for the application and specify its duration. After the client secret is saved, Apache Kafka displays the key value. <i>Copy this value, as it is displayed only once.</i> This value becomes the <u>OAuthClientSecret</u>.</li></ul></p>
</li><li>Select <b>API Permissions &gt; Add &gt; Delegated permissions</b>.</li><li>Save your changes.</li><li>If you have specified the use of permissions that require admin consent (such as the Application Permissions), you can grant them from the current tenant on the API Permissions page.
</li></ol>
</p>

<p>
<h3>Azure Service Principal</h3>

To use Azure Service Principal authentication, you must set up the ability to assign a role to the authentication application, then register an application with the Azure AD tenant to create a new 
Service Principal. That new Service Principal can then leverage the assigned role-based access 
control to access resources in your subscription.</p>

<p>In <var>portal.azure.com</var>:
<ol><li>Create a custom OAuth AD application, as described above.</li><li>Use the search bar to search for the Subscriptions service.</li><li>Open the <b>Subscriptions</b> page.</li><li>Select the subscription to which to assign the application.</li><li>Open the <b>Access control (IAM)</b>.</li><li>Select <b>Add &gt; Add role assignment</b>. Apache Kafka opens the <b>Add role assignment</b> page.</li><li>Assign your custom Azure AD application the role of <b>Owner</b>.</li></ol>
</p>

<p>
<h2>Admin Consent</h2>
</p>

<p>Some custom applications require administrative permissions to operate within an Azure Active Directory tenant. Admin consent can be granted when creating a new custom OAuth application, by adding relevant permissions that are already marked with "Admin Consent Required". Admin consent is also required to use Client Credentials in the OAuth flow. </p>

<p>To grant admin consent:
<ol><li>Have an admin log in to <var>portal.azure.com</var>.</li><li>Navigate to <b>App Registrations</b> and find the custom OAuth application you created. </li><li>Under <b>API Permissions</b>, click <b>Grant Consent</b>. </li></ol>
This gives your application permissions on the tenant under which it was created.
</p>

<p>
<h4>Consent for Client Credentials</h4>

OAuth supports the use of client credentials to authenticate. In a client credentials OAuth flow, credentials are created for the authenticating application itself. The auth flow acts just like 
the usual auth flow except that there is no prompt for an associated user to provide credentials. 
All tasks accepted by the application are executed outside of the context of a default user.</p>

<p><b>Note:</b> Since the embedded OAuth credentials authenticate on a per-user basis, you cannot 
use them in a client OAuth flow. You must always create a custom OAuth application to use client 
credentials.</p>

<p>In <var>portal.azure.com</var>:
<ol><li>Create a custom OAuth application, as described above.</li><li>Navigate to <b>App Registrations</b>.</li><li>Find the application you just created, and open <b>API Permissions</b>.</li><li>Select the Microsoft Graph permissions. 
There are two distinct sets of permissions: Delegated and Application.</li><li>Under <b>Application Permissions</b>, select the permissions you require for your integration. </li></ol>
</p>

<p></p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka -  Changelog" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka -  Changelog: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1> Changelog</h1>
       

    <div class="chapter_content" id="pg_changelog">

<p>
<h2>General Changes</h2>
</p>

<p></p>

<p><p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap"><b>Date</b></td><td><b>Build<br>Number</b></td><td><b>Change<br>Type</b></td><td><b>Description</b></td></tr><tr><td style="white-space:nowrap">11/29/2023</td><td>8733</td><td>General</td><td><b>Changed</b><br><ul><li>The ROUND function doesn't accept the negative precision values anymore.</li></ul><br><b>Changed</b><br><ul><li>The returning types of the FDMonth, FDQuarter, FDWeek, LDMonth, LDQuarter, LDWeek functions are changed from Timestamp to Date.</li><li>The return type of the ABS function will be consistent with the parameter value type.</li></ul>

</td></tr><tr><td style="white-space:nowrap">11/13/2023</td><td>8717</td><td>Apache Kafka</td><td><b>Changed</b><br><ul><li>Java editions now bundle: kafka-clients 3.6.0, lz4-java 1.8.0, slf4j-api 1.7.36, snappy-java 1.1.10.4, zstd-jni 1.5.5-1, Jackson 2.13.5 (jackson-annotations, jackson-core, jackson-databind, jackson-datatype-jdk8), jose4j 0.9.3, org.osgi.core 6.0.0</li><li>.NET editions now reference Confluent.Kafka 2.3.0 and its dependencies (check its NuGet page for a complete list)</li></ul>


</td></tr><tr><td style="white-space:nowrap">10/27/2023</td><td>8700</td><td>jdbc</td><td><b>Changed</b><br><ul><li>Added support for Connection.abort() method.</li></ul>

</td></tr><tr><td style="white-space:nowrap">08/22/2023</td><td>8634</td><td>Apache Kafka</td><td><b>Added</b><br><ul><li>Added support for authenticating to Azure Event Hubs with OAuth. AzureAD, AzureServicePrincipal, AzureServicePrincipalCert, and AzureMSI are all supported.</li></ul>
</td></tr><tr><td style="white-space:nowrap">08/03/2023</td><td>8615</td><td>Apache Kafka</td><td><b>Changed</b><br><ul><li>ProduceMessage now uses MessageText/MessageBytes to provide the message body, with corresponding inputs for the key.</li></ul><br><b>Removed</b><br><ul><li>ProduceMessage no longer accepts input streams for providing raw keys and messages.</li></ul>

</td></tr><tr><td style="white-space:nowrap">06/20/2023</td><td>8571</td><td>General</td><td><b>Added</b><br><ul><li>Added the new sys_lastresultinfo system table.</li></ul>




</td></tr><tr><td style="white-space:nowrap">04/25/2023</td><td>8515</td><td>General</td><td><b>Removed</b><br><ul><li>Removed support for the SELECT INTO CSV statement. The core code doesn't support it anymore.</li></ul>

</td></tr><tr><td style="white-space:nowrap">04/19/2023</td><td>8509</td><td>Apache Kafka</td><td><b>Added</b><br><ul><li>Added ProduceMessage stored procedure for producing messages. It can be used to publish raw messages to topics regardless of the connection TypeDetectionScheme.</li><li>Added support for binary message keys by setting MessageKeyType=binary.</li></ul>

</td></tr><tr><td style="white-space:nowrap">03/14/2023</td><td>8473</td><td>Apache Kafka</td><td><b>Added</b><br><ul><li>Added support for reading and writing specific partitions. A SELECT that filters on a specific value (or set of values) from the Partition column will subscribe to just those partitions. This increases read performance when many partitions are present. An INSERT can provide a value for the Partition column to write a message to a partition instead of having it auto-assigned.</li></ul>
</td></tr><tr><td style="white-space:nowrap">12/14/2022</td><td>8383</td><td>General</td><td><b>Changed</b><br><ul><li>Added the Default column to the sys_procedureparameters table.</li></ul>

</td></tr><tr><td style="white-space:nowrap">11/22/2022</td><td>8361</td><td>Apache Kafka</td><td><b>Added</b><br><ul><li>Added support for detecting the correct serialization format when performing an INSERT. The driver will determine this from either the schema registry or by row scan, depending upon the TypeDetectionScheme propery. This allows connections with SerializationFormat=AUTO to insert data when the serialization format is unknown or different across topics. Other values of SerializationFormat keep their old behavior and skip this format detection.</li></ul>


</td></tr><tr><td style="white-space:nowrap">10/26/2022</td><td>8334</td><td>Apache Kafka</td><td><b>Changed</b><br><ul><li>Updated the versions of the Kafka libraries used in both Java and .NET. JDBC now bundles kafka-clients 3.3.1 while ADO.NET is built against Confluent.Kafka 1.9.3. The new version of Confluent.Kafka only supports .NET Framework versions 4.6.2 and up. The supported versions for other .NET variants (.NET Standard and .NET 5+) are unaffected.</li></ul>

</td></tr><tr><td style="white-space:nowrap">09/30/2022</td><td>8308</td><td>General</td><td><b>Changed</b><br><ul><li>Added the IsPath column to the sys_procedureparameters table.</li></ul>




</td></tr><tr><td style="white-space:nowrap">08/05/2022</td><td>8252</td><td>Apache Kafka</td><td><b>Added</b><br><ul><li>Added support for CreateTablePartitions and CreateTableReplicationFactor. This allows CREATE TABLE to be used on smaller clusters, and to be tuned for better read throughput or higher failover redundancy.</li></ul>
</td></tr><tr><td style="white-space:nowrap">07/18/2022</td><td>8234</td><td>Apache Kafka</td><td><b>Added</b><br><ul><li>Added support for ConsumerProperties and ProducerProperties options. These allow for direct configuration of the underlying Kafka client libraries, in cases where the driver doesn't expose its own version of a property.</li></ul>
</td></tr><tr><td style="white-space:nowrap">06/11/2022</td><td>8197</td><td>jdbc</td><td><b>Changed</b><br><ul><li>The method GetJDBCMinorVersion() and GetJDBCMajorVersion() return 4.2 as the supported JDBC major / minor version.</li></ul>




</td></tr><tr><td style="white-space:nowrap">05/03/2022</td><td>8158</td><td>Apache Kafka</td><td><b>Added</b><br><ul><li>RegistryClientCert and related properties, as well as RegistryServerCert. These properties are used for the schema registry specifically and support the same SSL options as other providers. They are added to avoid confusion between what SSL properties apply to the schema registry vs. the Kafka broker.</li></ul><br><b>Deprecated</b><br><ul><li>The SSLKey, SSLCert and SSLCertPassword properties as well as TrustStorePassword and TrustStorePath. These hidden properties were required to connect to brokers via SSL, but they were specific to each edition and did not expose all of the certificate formats that the Kafka client libraries could accept.</li></ul><br><b>Replacements</b><br><ul><li>SSLClientCert and related properties, as well as SSLServerCert and related properties. Internally these configure the same Kafka client properties, but the names and values of these options more consistent with the registry SSL options. Only a subset of certificate types are supported due to limitations of the Kafka client libraries.</li></ul>
</td></tr><tr><td style="white-space:nowrap">04/28/2022</td><td>8153</td><td>jdbc</td><td><b>Changed</b><br><ul><li>Removed support for JRE 1.6. JRE 1.8 and above is now required.</li></ul>


</td></tr><tr><td style="white-space:nowrap">04/20/2022</td><td>8145</td><td>Apache Kafka</td><td><b>Added</b><br><ul><li>Added support for Confluent-compatible Avro encoding with the UseConfluentAvroFormat option. The default format is still the Avro file block format, but setting this option allows the provider to produce messages which can be read and validated by Confluent tools and libraries.</li></ul>
</td></tr><tr><td style="white-space:nowrap">04/07/2022</td><td>8132</td><td>Apache Kafka</td><td><b>Changed</b><br><ul><li>Updated the versions of the Kafka libraries used in both Java and .NET. .NET now requires the following assemblies to be present at runtime: Confluent.Kafka (and librdkafka.redist) : 1.8.2.0 System.Memory: 4.5.2 System.Buffers: 4.5.1 System.Runtime.CompilerServices.Unsafe: 4.5.3</li></ul>
</td></tr><tr><td style="white-space:nowrap">03/30/2022</td><td>8124</td><td>Apache Kafka</td><td><b>Added</b><br><ul><li>Added support for authenticating using SCRAM-SHA-512.</li></ul>


</td></tr><tr><td style="white-space:nowrap">02/25/2022</td><td>8091</td><td>jdbc</td><td><b>Changed</b><br><ul><li>The method DatabaseMetaData.getTypeInfo() now returns all of the data types that the driver supports. Previously, it was returning only a portion of the data types. The types include: tinyint, smallint, int, bigint, float, double, numeric, decimal, bit, date, datetime, time, varchar, binary, uuid.</li></ul>
</td></tr><tr><td style="white-space:nowrap">02/24/2022</td><td>8090</td><td>jdbc</td><td><b>Changed</b><br><ul><li>Corrected the return value of method DatabaseMetaData.supportsOuterJoins(), which used to return an incorrect value.</li></ul>





</td></tr><tr><td style="white-space:nowrap">12/13/2021</td><td>8017</td><td>Apache Kafka</td><td><b>Removed</b><br><ul><li>Removed unnecesary dependencies relating to embedded Kafka servers. This also removes the log4j component which was affected by the recent CVE reports.</li></ul>



</td></tr><tr><td style="white-space:nowrap">10/27/2021</td><td>7970</td><td>jdbc</td><td><b>Changed</b><br><ul><li>Added support for JDK 17</li></ul>

</td></tr><tr><td style="white-space:nowrap">09/23/2021</td><td>7936</td><td>Apache Kafka</td><td><b>Added</b><br><ul><li>Added support for Authenticating with Kerberos on both JDBC and ADO editions.</li></ul>
</td></tr><tr><td style="white-space:nowrap">09/12/2021</td><td>7925</td><td>jdbc</td><td><b>Removed</b><br><ul><li>Removed the IS_READONLY, IS_KEY, NUMERIC_PRECISION, and DECIMAL_DIGITS columns from DatabaseMetadata.GetColumns(), as they are not part of the JDBC Specification. This information is still available via other metadata calls.</li></ul>
</td></tr><tr><td style="white-space:nowrap">09/02/2021</td><td>7915</td><td>General</td><td><b>Added</b><br><ul><li>Added support for the STRING_SPLIT table-valued function in the CROSS APPLY clause.</li></ul>
</td></tr><tr><td style="white-space:nowrap">08/26/2021</td><td>7908</td><td>Apache Kafka</td><td><b>Added</b><br><ul><li>Added support for AWS Glue Schema Registry service. Now we support Confluent and AWS Glue Schema Registry types.</li></ul>
</td></tr><tr><td style="white-space:nowrap">08/07/2021</td><td>7889</td><td>General</td><td><b>Changed</b><br><ul><li>Added the KeySeq column to the sys_foreignkeys table.</li></ul>
</td></tr><tr><td style="white-space:nowrap">08/06/2021</td><td>7888</td><td>General</td><td><b>Changed</b><br><ul><li>Added the new sys_primarykeys system table.</li></ul>


</td></tr><tr><td style="white-space:nowrap">07/23/2021</td><td>7874</td><td>General</td><td><b>Changed</b><br><ul><li>Updated the Literal Function Names for relative date/datetime functions. Previously, relative date/datetime functions resolved to a different value when used in the projection as opposed to the predicate. For example: SELECT LAST_MONTH() AS lm, Col FROM Table WHERE Col &gt; LAST_MONTH(). Formerly, the two LAST_MONTH() methods would resolve to different datetimes. Now, they will match.</li><li>As a replacement for the previous behavior, the relative date/datetime functions in the criteria may have an 'L' appended to them. For example: WHERE col &gt; L_LAST_MONTH(). This will continue to resolve to the same values that were previously calculated in the criteria. Note that the "L_" prefix will only work in the predicate - it not available for the projection.</li></ul>


</td></tr><tr><td style="white-space:nowrap">07/12/2021</td><td>7863</td><td>Apache Kafka</td><td><b>Removed</b><br><ul><li>Removed the Topic static table. Previously it was used only to retrieve some static columns. Now it's no longer needed.</li></ul>




</td></tr><tr><td style="white-space:nowrap">06/23/2021</td><td>7844</td><td>Apache Kafka</td><td><b>Added</b><br><ul><li>Added support for JSON Scheme type on SchemaRegistry mode. Now we support AVRO and JSON schema types.</li></ul>









</td></tr><tr><td style="white-space:nowrap">04/23/2021</td><td>7783</td><td>General</td><td><b>Changed</b><br><ul><li>Updated how display sizes are determined for varchar primary key and foreign key columns so they will match the reported length of the column.</li></ul>



</td></tr><tr><td style="white-space:nowrap">04/16/2021</td><td>7776</td><td>General</td><td><b>Changed</b><br><ul><li>Reduced the length to 255 for varchar primary key and foreign key columns.</li></ul><br><b>Changed</b><br><ul><li>Updated index naming convention to avoid duplicates.</li></ul>
</td></tr><tr><td style="white-space:nowrap">04/16/2021</td><td>7776</td><td>General</td><td><b>Changed</b><br><ul><li>Updated implicit and metadata caching to improve performance and support for multiple connections. Old metadata caches are not compatible - you need to generate new metadata caches if you are currently using CacheMetadata.</li></ul>







</td></tr></table></center><p /></p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - Using JDBC" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - Using JDBC: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>Using JDBC</h1>
       

    <div class="chapter_content" id="pg_usagej">

<p>This section provides a walk-through of writing data access code to Apache Kafka in JDBC. </p>

<p>See <a href="#pg_datamodel">Data Model</a> for more information on the available API objects and how to query them with SQL. 
See <a href="#pg_overview">SQL Compliance</a> for the SQL syntax.


<h2>Connecting from Code</h2>
</p>

<p>See <a href="#pg_connectionj">Establishing a Connection</a> for the prerequisite information you need to deploy the driver and configure the connection to Apache Kafka. <a href="#pg_JDBCconnectcode">Connecting from Code</a> shows how to connect with the DriverManager or ApacheKafkaDataSource classes.


<h3>Executing SQL</h3>
</p>

<p>Use the Statement and PreparedStatement classes to execute SQL to Apache Kafka:
<ul><li>See <a href="#pg_JDBCqueries">Executing Statements</a> to execute Statements and iterate over the returned ResultSets. </li><li>See <a href="#pg_JDBCupdates">Using Prepared Statements</a> to execute parameterized statements. The PreparedStatement class provides a means to efficiently execute queries more than once and to mitigate SQL injection attacks.</li></ul></p>

<p> 

<h3>Executing Stored Procedures</h3>
</p>

<p>You can execute stored procedures as parameterized statements (with the CallableStatement class) or SQL statements (with the EXECUTE syntax): see <a href="#pg_storedprocedurej">Calling Stored Procedures</a>.


<h3>Connection Pooling</h3>
</p>

<p>Instantiate pooled connections with ApacheKafkaDataSource objects: see <a href="#pg_connectionpoolingjdbc">Connection Pooling</a> to create and configure the pool.
</p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka -  Installed Files" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka -  Installed Files: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1> Installed Files</h1>
       

    <div class="chapter_content" id="pg_jars">

<p>The CData JDBC Driver for Apache Kafka ships the following files, located in the lib subfolder of the installation directory:
<ul><li>cdata.jdbc.apachekafka.jar: Pure Java Type 4/5 JDBC Driver, compiled with JDK 1.8.</li><li>cdata.jdbc.apachekafka.lic: This is the license file and must be placed in same folder as the JAR.</li><li>cdata.jdbc.apachekafka.remoting.ini: This is the configuration file for <a href="#pg_remotingintroj">JDBC Remoting</a>.</li></ul>
</p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka -  Connecting from Code" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka -  Connecting from Code: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1> Connecting from Code</h1>
       

    <div class="chapter_content" id="pg_JDBCconnectcode">

<p>This section describes how to connect with the JDBC DriverManager or ApacheKafkaDataSource interfaces.


<h2>Connecting with the DriverManager</h2>
</p>

<p>When connecting with the DriverManager class, the CData JDBC Driver for Apache Kafka follows the JDBC convention: First, load the ApacheKafka driver class. Then, make a connection.


<h3>Load the Driver</h3>

The following step is optional per the JDBC 4.0 specification.
<br/><pre lang="">Class.forName("cdata.jdbc.apachekafka.ApacheKafkaDriver");</pre>


<h3>Establish a Connection</h3>
</p>

<p>Provide the connection string with the getConnection method of the static DriverManager class. Start the connection string with "jdbc:apachekafka:". A typical connection string is the following:
<br/><pre lang="">Connection conn = DriverManager.getConnection("jdbc:apachekafka:User=admin;Password=pass;BootStrapServers=https://localhost:9091;Topic=MyTopic;");</pre>

Alternatively, you can prepare the connection options using a Properties object. Pass the Properties object to the DriverManager.
<br/><pre lang="">prop.setProperty("User","admin");
prop.setProperty("Password","pass");
prop.setProperty("BootStrapServers","https://localhost:9091");
prop.setProperty("Topic","MyTopic");

Connection conn = DriverManager.getConnection("jdbc:apachekafka:",prop);
</pre>


<h2>Connecting with the ApacheKafkaDataSource Class</h2>
</p>

<p>You can use the ApacheKafkaDataSource class to create pooled connections, as shown in the following example. See <a href="#pg_connectionpoolingjdbc">Connection Pooling</a> for more information.</p>

<p>The following example instantiates a pooled Connection object:
<br/><pre lang="java">ApacheKafkaDataSource ds = new ApacheKafkaDataSource("jdbc:apachekafka:UseConnectionPooling=true;User=admin;Password=pass;BootStrapServers=https://localhost:9091;Topic=MyTopic;"); 
Connection conn = ds.getConnection();</pre> 
</p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka -  Executing Statements" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka -  Executing Statements: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1> Executing Statements</h1>
       

    <div class="chapter_content" id="pg_JDBCqueries">

<p>After <a href="#pg_JDBCconnectcode">Connecting from Code</a>, you can execute SQL statements with the Statement class. See <a href="#pg_JDBCupdates">Using Prepared Statements</a> to execute parameterized statements.</p>

<p>
<h2>Select</h2>
</p>

<p>To execute SQL statements that return data, use the Statement class' generic execute method or the executeQuery method. To return the results of a query, call the getResultSet method of the Statement. </p>

<p>The following example calls the execute method and iterates over the results returned:
<br/><pre lang="">Statement stat = conn.createStatement();
boolean ret = stat.execute("SELECT Id, Column1 FROM SampleTable_1");
if (ret) {
  ResultSet rs=stat.getResultSet();
  while(rs.next()) {
    for(int i=1;i&lt;=rs.getMetaData().getColumnCount();i++) {
      System.out.println(rs.getMetaData().getColumnLabel(i) +"="+rs.getString(i));
    }
  }
}</pre>


<h2>Insert</h2>
</p>

<p>To execute an INSERT, use the generic execute method or the executeUpdate method of the Statement class. For example:
<br/><pre lang="">Statement stat = conn.createStatement();
int count = stat.executeUpdate("INSERT INTO SampleTable_1 (Id, Column1) VALUES ('Id','Column1')");</pre>
 

 
 

</p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka -  Using Prepared Statements" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka -  Using Prepared Statements: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1> Using Prepared Statements</h1>
       

    <div class="chapter_content" id="pg_JDBCupdates">

<p>The PreparedStatement object represents a precompiled SQL statement. A PreparedStatement can be used multiple times and mitigates SQL injection attacks. A PreparedStatement can be a SELECT, INSERT, UPDATE, or DELETE statement. 
</p>

<p>To execute a prepared statement, you can use the generic execute method of the Statement class. This section describes how to execute a prepared statement.
<ol><li>
<p>Instantiate a PreparedStatement object with the prepareStatement method of the Connection class.</p>


<p>See <a href="#pg_JDBCconnectcode">Connecting from Code</a> to create the connection.</p>
</li><li>Declare parameters by calling the PreparedStatement's corresponding setter method. Note that the parameter indices start from one.</li><li>Call the PreparedStatement's execute method to execute the statement.</li><li>Call the PreparedStatement's getResultSet method to pull the results into a ResultSet object.</li><li>Call ResultSet.next to iterate over the result set. Use the ResultSetMetaData class to obtain column information about the result set. To instantiate a ResultSetMetaData object, call the ResultSet's getMetaData method.</li></ol>


<h2>Select</h2>
</p>

<p>The following example shows how to execute a SELECT prepared statement: 
<br/><pre lang="">String query = "SELECT * FROM SampleTable_1 WHERE Id=? AND Column1=?";
PreparedStatement pstmt = conn.prepareStatement(query);
pstmt.setString(1, "XXX");
pstmt.setString(2, "YYY");
boolean ret = pstmt.execute();
if (ret) {
  ResultSet rs=pstmt.getResultSet();
  while(rs.next()) {
    for(int i=1;i&lt;=rs.getMetaData().getColumnCount();i++) {
      System.out.println(rs.getMetaData().getColumnLabel(i) +"="+rs.getString(i));
    }
  }
} </pre> 

  
<h2>INSERT</h2>

  </p>

<p>To execute an INSERT, you can use the generic execute method or the executeUpdate method, as shown in the following example.
 
<br/><pre lang="">String query = "INSERT INTO SampleTable_1 (Id, Column1) VALUES (?,?)";
PreparedStatement pstmt = conn.prepareStatement(query);
pstmt.setString(1, "XXX");
pstmt.setString(2, "YYY");
int count = pstmt.executeUpdate();</pre> 
</p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka -  Connection Pooling" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka -  Connection Pooling: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1> Connection Pooling</h1>
       

    <div class="chapter_content" id="pg_connectionpoolingjdbc">

<p>The driver implements a standard JDBC connection pool. Set <u>UseConnectionPooling</u> to enable the pool. The following sections show how to configure and use them.


<h2>Working with Pooled Connections</h2>
</p>

<p>Just as you would interact with a non-pooled connection, you use standard JDBC objects to get and close connections. But, in this case, the Connection object retrieved is a handle for the physical connection owned by the connection pool. When the connection is closed, instead of the connection being destroyed, the handle is returned to the pool, where it is available for the next connection request.</p>

<p>You must explicitly close the connection for it to be returned to the pool.  


</p>

<p>
<h2>Configuring the Connection Pool</h2>
</p>

<p>In addition to <u>UseConnectionPooling</u>, set the following connection properties to control the connection pool:
<ul><li><u>PoolMaxSize</u>: Define the maximum number of connections that can be open at any given time.</li><li><u>PoolIdleTimeout</u>: Set a limit to how long connections can remain idle in the connection pool. If this limit is exceeded, the connection is removed from the pool.</li><li><u>PoolWaitTime</u>: Set a limit to how long new connection requests should wait for a connection to become available. If this limit is exceeded, the request returns an error. By default, connection requests wait forever for a connection to become available. </li></ul>


<h3>Connection Pooling with  apachekafkaDataSource</h3>
</p>

<p>To use the default method for pooling connections, instantiate the ApacheKafkaDataSource with <u>UseConnectionPooling</u>:
<br/><pre lang="java">ApacheKafkaDataSource apachekafkaDataSource = new ApacheKafkaDataSource();
apachekafkaDataSource.setURL("jdbc:apachekafka:UseConnectionPooling=true;User=admin;Password=pass;BootStrapServers=https://localhost:9091;Topic=MyTopic;");</pre>


<h3>JDBC Connection Pooling</h3>
</p>

<p>If you would like to establish a pooled connection using the the JDBC ConnectionPoolDataSource interface, instantiate the ApacheKafkaConnectionPoolDataSource with <u>UseConnectionPooling</u>:
<br/><pre lang="java">ApacheKafkaConnectionPoolDataSource apachekafkaPoolDataSource = new ApacheKafkaConnectionPoolDataSource();
apachekafkaPoolDataSource.setURL("jdbc:apachekafka:UseConnectionPooling=true;User=admin;Password=pass;BootStrapServers=https://localhost:9091;Topic=MyTopic;");</pre>

<h2>Closing the Connection Pool</h2>
</p>

<p>On JRE 1.3 or higher, the connection pool itself automatically closes when the application stops running. You can manually close the connection pool by invoking the close method of the DataSource object.</p>

<p></p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka -  JNDI" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka -  JNDI: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1> JNDI</h1>
       

    <div class="chapter_content" id="pg_jndipooling">

<p>
<h2>Connection Pooling with JNDI</h2>
</p>

<p>The Java Naming and Directory Service (JNDI) is an API which allows distributed application to look up services. JNDI can be used to easily set up connection pools.</p>

<p>To set up a connection pool using JNDI, you will need to initialize the JNDI File System Service Provider, as shown in the example code below. To run the example, you need to add the fscontext.jar and providerutil.jar files to your classpath. You can download these files from the Oracle Java Archive: Under the Java SE section, select <b>Java Platform Technologies &gt; Java Naming and Directory Interface</b>.
<br/><pre lang="java"> 
Hashtable env = new Hashtable();
env.put(Context.INITIAL_CONTEXT_FACTORY,
    "com.sun.jndi.fscontext.RefFSContextFactory");
env.put(Context.PROVIDER_URL, "file:///tmp");

Context ctx = new InitialContext(env); 
DataSource ds = null;
Connection conn = null;</pre>
The following code registers the ApacheKafkaDataSource with the JNDI naming service, gets an instance of the DataSource from the service, and creates pooled connections from that instance.
<br/><pre lang="java">try {
  ApacheKafkaConnectionPoolDataSource apachekafkaDataSource = new ApacheKafkaConnectionPoolDataSource();
  apachekafkaDataSource.setURL("jdbc:apachekafka:UseConnectionPooling=true;User=admin;Password=pass;BootStrapServers=https://localhost:9091;Topic=MyTopic;");
  ctx.bind("jdbc/apachekafka", apachekafkaDataSource);
  ds = (DataSource) ctx.lookup("jdbc/apachekafka");

  conn = ds.getConnection();
  Statement stat = conn.createStatement();
  boolean ret = stat.execute("SELECT 1");
  ResultSet rs=stat.getResultSet(); 
} catch(Exception ex) { } finally {
  if(conn != null) conn.close();
}</pre>
</p>

<p></p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka -  Calling Stored Procedures" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka -  Calling Stored Procedures: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1> Calling Stored Procedures</h1>
       

    <div class="chapter_content" id="pg_storedprocedurej">

<p>Use CallableStatement objects to execute parameterized stored procedure calls. Use Statement objects to execute stored procedures as SQL statements with the EXEC syntax.



<h2>Using Callable Statement Objects</h2>
</p>

<p>You can use the generic execute method of the CallableStatement class to execute any stored procedure as a parameterized query. </p>

<p>To return the stored procedure's results, call getResultSet. To return a count of updated rows, call getUpdateCount.</p>

<p>The following example shows how to execute the SampleProcedure stored procedure:

<br/><pre lang="">CallableStatement cstmt = conn.prepareCall("SampleProcedure");
cstmt.setString("Id", "7");
boolean ret = cstmt.execute();   
if (!ret) {
  int count=cstmt.getUpdateCount();
  if (count!=-1) {
    System.out.println("Affected rows: "+count);
  }
}
else {
  ResultSet rs=cstmt.getResultSet();
  while(rs.next()){
    for(int i=1;i&lt;=rs.getMetaData().getColumnCount();i++) {
      System.out.println(rs.getMetaData().getColumnLabel(i) +"="+rs.getString(i));
    }
  }
}</pre>


<h2>Using Statement Objects</h2>
</p>

<p>You can use the execute method of the Statement class to execute any stored procedure as an SQL statement. </p>

<p>To return the stored procedure's results, call getResultSet. To return a count of updated rows, call getUpdateCount.</p>

<p>The following example shows how to execute the SampleProcedure stored procedure:  (See <a href="#pg_exec">EXECUTE Statements</a> for more on the syntax.) 
<br/><pre lang="">Statement stmt = conn.createStatement();
boolean ret = stmt.execute("EXEC SampleProcedure Id = '7'");

if (!ret) {
  int count=stmt.getUpdateCount();
  if (count!=-1) {
    System.out.println("Affected rows: "+count);
  }
}
else {
  ResultSet rs=stmt.getResultSet();
  while(rs.next()) {
    for(int i=1;i&lt;=rs.getMetaData().getColumnCount();i++) {
      System.out.println(rs.getMetaData().getColumnLabel(i) +"="+rs.getString(i));
    }
  }
}</pre></p>

<p>
</p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - Using from Tools" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - Using from Tools: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>Using from Tools</h1>
       

    <div class="chapter_content" id="pg_jdbctoolsintro">

<p>The CData JDBC Driver for Apache Kafka provides the standard JDBC connection process in analytics tools and other applications.


<h2>JDBC Integration Quickstarts</h2>
</p>

<p>The CData JDBC Driver for Apache Kafka provides the standard JDBC connection process in applications ranging from business intelligence tools to IDEs. The following sections show how to create and start querying Apache Kafka JDBC data sources, walking through data access in JDBC from several popular database tools.

<ul><li><a href="#pg_jdbcdbvisualizer">DbVisualizer</a>
  </li><li><a href="#pg_jdbcsquirrel">SQuirreL SQL</a>
  </li><li><a href="#pg_jdbcdbeaver">DBeaver</a>
  </li><li><a href="#pg_jdbctableau">Tableau</a></li></ul>


<h2>Complete List of Apache Kafka Integration Quickstarts</h2>
</p>

<p>See <a href="https://www.cdata.com/kb/tech/apachekafka-article-list.rst">Apache Kafka integration guides</a> for information on connecting from other applications.
</p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka -  DbVisualizer" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka -  DbVisualizer: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1> DbVisualizer</h1>
       

    <div class="chapter_content" id="pg_jdbcdbvisualizer">

<p>This section shows how to establish a connection to data in DbVisualizer, use the table editor to edit and save data, and execute SQL.

<h2>Add the JDBC Driver for Apache Kafka</h2>
</p>

<p>Complete the following steps to add the driver .jar file in a driver definition:
<ol><li>In the DbVisualizer toolbar, select <b>Tools &gt; Driver Manager</b>.
  </li><li>Click the "<b>+</b>" button in the top-left corner of the Driver Manager window and select <b>Custom</b>.
  </li><li>Enter a name for the driver.
  </li><li>Enter the following in the URL Format box: <br/><pre lang="">jdbc:apachekafka:</pre>
  </li><li>In the <b>Driver artifacts and jar files</b> section, click the "<b>+</b>" button on the right and select <b>Add Files</b>... to browse for the driver .jar file, <b>cdata.jdbc.apachekafka.jar</b>. By default, this is located in the <b>lib</b> subfolder of the installation directory. Note that the .lic file in this folder must be located in the same folder as the .jar file.
  </li><li>In the Driver Class menu, select the ApacheKafkaDriver class, <b>cdata.jdbc.apachekafka.ApacheKafkaDriver</b>.</li></ol>

<h2>Create a Database Connection for Apache Kafka</h2>
</p>

<p>Complete the following steps to select the Apache Kafka driver and build the JDBC URL to create the JDBC data source:
<ol><li>    
<p>In the DbVisualizer toolbar, select <b>Database &gt; Create Database Connection</b>. The <b>Driver Name</b> selector will then appear. </p>

<p><b>Note:</b> If this option is greyed out, you may need to first select the <b>Connections</b> root element of the tree view, found in the <b>Databases</b> tab of the left-hand pane. 
  </p>
</li><li>Double-click the name of the JDBC driver you defined in the section above.
  </li><li>Set <b>Settings Format</b> to <b>Database URL</b> if it isn't already set to that value.
  </li><li>Set <b>Driver Type</b> to the name you defined for your JDBC driver, if it doesn't default to that name.
  </li><li>Set <b>Database URL</b> to the full JDBC URL. The syntax of the JDBC URL is <b>jdbc:apachekafka:</b> followed by the connection properties in a semicolon-separated list of name-value pairs.
    
<p>A typical connection string is below:<br/><pre lang="plain">jdbc:apachekafka:User=admin;Password=pass;BootStrapServers=https://localhost:9091;Topic=MyTopic;</pre>
  </p>
</li><li>Click <b>Connect</b>.
  </li><li>If the connection succeeds, set the <b>Database Type</b> to <b>Generic</b>.</li></ol>


<h2>Discover Schemas and Query Apache Kafka Data</h2>
</p>

<p>You can now browse data and execute SQL in the following ways:
<ul><li>To access the available tables, from the <b>Databases</b> tab, expand the nodes for the connection, database, schema, and table or view.
  </li><li>To browse through table data and metadata, right-click a table and click <b>Open in Tab</b> or <b>Open in New Tab</b>.
  </li><li>To execute SQL queries, select <b>SQL Commander &gt; New SQL Commander</b>. Select the Database Connection, Database, and Schema from the available menus.</li></ul></p>

<p>See <a href="#pg_datamodel">Data Model</a> for information on querying specific tables. See <a href="#pg_overview">SQL Compliance</a> for more information on the SQL syntax.</p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka -  DBeaver" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka -  DBeaver: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1> DBeaver</h1>
       

    <div class="chapter_content" id="pg_jdbcdbeaver">

<p>This section describes how to connect to Apache Kafka following the standard JDBC connection process in DBeaver: Add the driver JAR, provide the driver class name, and provide any Apache Kafka-specific parameters in the JDBC URL.

<h2>Add the JDBC Driver for Apache Kafka</h2>
</p>

<p>Complete the following steps to add the driver .jar file:
<ol><li>Open the DBeaver application and, in the Database menu, select the <b>Driver Manager</b> option. Click <b>New</b> to open the Create New Driver form.
  </li><li>In the Driver Name box, enter a user-friendly name for the driver.
  </li><li>
<p>To add the .jar file, click <b>Add File</b> on the Libraries tab. Select the cdata.jdbc.apachekafka.jar file, located in the lib subfolder of the installation directory.</p>
  
<p>  Note that the.lic file must be located in the same folder as the .jar file.
  </p>
</li><li>Click <b>Find Class</b>, and in the list select, "cdata.jdbc.apachekafka.ApacheKafkaDriver".
  </li><li>In the URL Template field, enter <i>jdbc:apachekafka:</i>.</li></ol>

<h2>Create the JDBC Data Source</h2>
</p>

<p>Complete the following steps to select the Apache Kafka driver and build the JDBC URL to create the JDBC data source:
<ol><li>In the main DBeaver window, click <b>Database &gt; New Connection</b>.
  </li><li>Select the driver definition you created in the dialog that is displayed.
  </li><li>On the next page of the wizard, click the <b>Driver Properties</b> tab.
  </li><li>    
<p>Enter any connection properties required to connect to Apache Kafka. 
    </p>

<p>Here is a typical connection string:<br/><pre lang="">jdbc:apachekafka:User=admin;Password=pass;BootStrapServers=https://localhost:9091;Topic=MyTopic;</pre> 
    See <a href="#pg_connectionj">Establishing a Connection</a> for a connection and authentication guide.
  </p>
</li><li>Finish creating the connection with the default settings or configure advanced network settings -- proxy, firewall, and SSH.</li></ol>

<h2>Discover Schemas and Query Apache Kafka Data</h2>
</p>

<p>Complete the following steps to query information from the tables exposed by the connection:
<ol><li>Expand the node for the connection to access the database metadata. </li><li>Browse the table metadata and edit the table data by right-clicking a Table and then clicking <b>Edit Table</b>.</li></ol>
</p>

<p>To execute an SQL query, select <b>SQL Editor &gt; New SQL Editor</b> and select the Apache Kafka connection you created. You can then enter queries using code completion.

</p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka -  SQuirreL SQL" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka -  SQuirreL SQL: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1> SQuirreL SQL</h1>
       

    <div class="chapter_content" id="pg_jdbcsquirrel">

<p>This section describes how to create a JDBC data source for Apache Kafka data and execute queries.

<h2>Add the JDBC Driver for Apache Kafka</h2>
</p>

<p>Complete the following steps to add the driver .jar file: 
<ol><li>In the Drivers pane, click the plus icon to open the Add Driver wizard. 
  </li><li>In the Name box, enter a user-friendly name for the driver; for example, CData JDBC Driver for Apache Kafka.
  </li><li>In the Example URL box, enter <i>jdbc:apachekafka:</i>
  </li><li>In the Extra Class Path tab, click <b>Add</b>.
  </li><li>In the file explorer that opens, select the .jar file for the driver, located in the lib subfolder of the installation directory.
  </li><li>Click <b>List Drivers</b> to populate the Class Name menu with the class name for the driver, cdata.jdbc.apachekafka.ApacheKafkaDriver.</li></ol>

<h2>Create the JDBC Data Source</h2>
</p>

<p>Complete the following steps to select the Apache Kafka driver you created and build the JDBC URL to create the JDBC data source:
<ol><li>In the Aliases pane, click the plus icon.
  </li><li>    
<p>In the Add Alias wizard that opens, provide values for the following fields:
    <ul><li><b>Name</b>: Enter a name for the alias; for example, CData Apache Kafka Source.
      </li><li><b>Driver</b>: Select the driver definition you created. 
      </li><li><b>URL</b>: Enter <i>jdbc:apachekafka:</i>
      </li><li><b>User Name</b>: If needed, enter a user name for authentication, which is added to the JDBC URL.
      </li><li><b>Password</b>: If needed, enter a password for authentication, which is added to the JDBC URL.
    </li></ul>
  </p>
</li><li>If you want to define any additional properties, add them to the JDBC URL in a semicolon-separated list.
    
<p>The following is a typical connection string:<br/><pre lang="">jdbc:apachekafka:User=admin;Password=pass;BootStrapServers=https://localhost:9091;Topic=MyTopic;</pre>
    See <a href="#pg_connectionj">Establishing a Connection</a> for a connection and authentication guide.
    </p>
</li></ol>
    Or, follow these steps to add the properties in the Driver Properties dialog:
    <ol><li>Select the <b>Use Driver Properties</b> check box.
  </li><li>In the Specify column, select the check boxes for the required connection properties and specify the corresponding values.
  </li><li>In the dialog that appears after you click OK, click <b>Connect</b> to test the connection.</li></ol>

<h2>Discover Schemas and Query Apache Kafka Data</h2>
</p>

<p>To connect to the data source, right-click the alias on the Aliases pane and then click <b>Connect</b>. After the metadata has loaded, a new tab for the Apache Kafka data source is displayed. On the Objects subtab, you can discover schema information, such as the available tables and views.</p>

<p>To view table data and metadata, select the table on the Objects tab. Access the table data on the Content tab.</p>

<p>To execute an SQL query, enter the query on the SQL tab and then click <b>Run SQL</b> (the runner icon). 
</p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka -  Tableau" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka -  Tableau: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1> Tableau</h1>
       

    <div class="chapter_content" id="pg_jdbctableau">

<p>This section describes how to connect and start querying data from Tableau.


<h2>Add the CData JDBC Driver for Apache Kafka</h2>

</p>

<p><b>Note:</b> Before starting Tableau on <b>Windows</b>, make sure that you have placed the .jar file in the C:\Program Files\Tableau\Drivers folder. Before starting Tableau on <b>macOS</b>, make sure that you have placed the .jar file in the ~/Library/Tableau/Drivers folder.

 </p>

<p>To add the driver .jar file:
   <ol><li>Start Tableau.</li><li>Under <b>To a Server</b>, select <b>More</b>.</li><li>Select <b>Other Databases (JDBC)</b>.</li><li>Enter the JDBC connection string in the URL field. See <b>Building the JDBC URL for Apache Kafka</b> below for more information.</li><li>Select <b>Sign in</b>.</li></ol>


<h2 id="jdbcurl">Building the JDBC URL for Apache Kafka</h2>
</p>

<p>Connection strings provide information about a data source and how to connect to that data source. The driver comes with a connection string builder that makes it easier to create and manage the contents of connection strings.
</p>

<p>After downloading and installing the driver, double-click the .jar file in the lib folder. You can also manually run the .jar file, as shown in the following examples.
</p>

<p>From Windows:

<br/><pre lang="">java -jar 'C:\Program Files\CData\CData JDBC Driver for Apache Kafka 2023\lib\cdata.jdbc.apachekafka.jar'</pre>
</p>

<p>From macOS:

<br/><pre lang="">java -jar cdata.jdbc.apachekafka.jar</pre>
</p>

<p>Running the .jar file opens the <b>Connection Properties</b> dialog box. You can use this dialog box to build and test a connection string. Click <b>Test Connection</b> to test and validate the entered connection properties. Click <b>Copy to Clipboard</b> to copy the connection string for use within the application where the JDBC driver is being used. See the help documentation for more information about the connection string options.


<h2>Discover Schemas and Query Data</h2>
</p>

<p>To query data:

<ol><li>Select <b>CData</b> from the <b>Database</b> pull-down menu.</li><li>Select <b>Odata</b> from the <b>Schema</b> pull-down menu.</li><li>Drag the table onto the join area. You can include multiple tables.</li><li>Select <b>Update Now</b> or <b>Automatically Update</b>. Update Now lets you preview the first 10,000 rows of the data source (or enter the number of rows you want to see in the <b>Rows</b> text box). Automatically Update automatically reflects the changes in the preview area.</li><li>In the Connection menu, select the <b>Live</b> option, so that you skip loading a copy of the data into Tableau and instead work on real-time data.</li><li>Click the tab for your worksheet. Columns are listed as Dimensions and Measures, depending on the data type.</li></ol>
</p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - Schema Discovery" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - Schema Discovery: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>Schema Discovery</h1>
       

    <div class="chapter_content" id="pg_systemtablesintro">

<p>The driver supports schema discovery using JDBC classes or using SQL queries to the available system tables. The JDBC classes enable access to schema information, connection property information, and information on the columns returned.</p>

<p>Through SQL queries to the available <a href="#pg_allsystables">System Tables</a>, you can access schema and connection property information as well as information on data source functionality and statistics on update operations.
</p>

<p>

<h2>Using JDBC Interfaces</h2>
</p>

<p>You can use JDBC interfaces to access schema information, connection property metadata, and result set metadata. The driver implements the standard interfaces as defined in the JDBC 4.0 specification. 


<h3>Retrieving Schema Information</h3>
</p>

<p>The DatabaseMetaData class provides information on the following: 
<ul><li><a href="#pg_systemtablesj">Tables</a></li><li><a href="#pg_systemcolumnsj">Columns</a></li><li><a href="#pg_spmetadataj">Procedures</a></li><li><a href="#pg_spmetadataparamj">Procedure Parameters</a></li><li><a href="#pg_primarykeysjdbc">Primary Keys</a></li></ul>


<h3>Retrieving Connection Property Information</h3>
</p>

<p>The Driver class, returned by DriverManager, provides information about <a href="#pg_connmetadatajava">Connection Properties</a>.


<h3>Retrieving Result Set Column Information</h3>
</p>

<p>The ResultSetMetaData class provides information about the columns returned in <a href="#pg_resultsetmetadatajdbc">Result Sets</a>.


<h2>Using SQL</h2>
</p>

<p>You can query the <a href="#pg_allsystables">System Tables</a> to access any metadata surfaced through the driver.


</p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka -  Tables" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka -  Tables: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1> Tables</h1>
       

    <div class="chapter_content" id="pg_systemtablesj">

<p>You can use the getTables method of the DatabaseMetaData interface to retrieve a list of tables:
<br/><pre lang="">String connectionString = "jdbc:apachekafka:User=admin;Password=pass;BootStrapServers=https://localhost:9091;Topic=MyTopic;";

Connection conn = DriverManager.getConnection(connectionString);
DatabaseMetaData table_meta = conn.getMetaData();
ResultSet rs=table_meta.getTables(null, null, "%", null);  
while(rs.next()){
  System.out.println(rs.getString("TABLE_NAME"));
}</pre> 
The getTables method returns the following columns:
<p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap"><b>Column Name</b></td><td><b>Data Type</b></td><td><b>Description</b></td></tr><tr><td style="white-space:nowrap">TABLE_CAT</td><td>String</td><td>The table catalog.</td></tr><tr><td style="white-space:nowrap">TABLE_SCHEM</td><td>String</td><td>The table schema.</td></tr><tr><td style="white-space:nowrap">TABLE_NAME</td><td>String</td><td>The table name.</td></tr><tr><td style="white-space:nowrap">TABLE_TYPE</td><td>String</td><td>The table type.</td></tr><tr><td style="white-space:nowrap">REMARKS</td><td>String</td><td>The table description.</td></tr></table></center><p />
</p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka -  Columns" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka -  Columns: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1> Columns</h1>
       

    <div class="chapter_content" id="pg_systemcolumnsj">

<p>You can use the getColumns method of the DatabaseMetaData interface to retrieve column information.
You can restrict the results by the table name. 
The code example below retrieves the column names for the SampleTable_1 table:

<br/><pre lang="">String connectionString = "jdbc:apachekafka:User=admin;Password=pass;BootStrapServers=https://localhost:9091;Topic=MyTopic;";

Connection conn = DriverManager.getConnection(connectionString);
DatabaseMetaData table_meta = conn.getMetaData();
ResultSet rs = table_meta.getColumns(null,null,"SampleTable_1", null);
while(rs.next()){
  System.out.println(rs.getString("COLUMN_NAME")); 
}</pre>

The getColumns method returns the following columns:
<p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap"><b>Column Name</b></td><td><b>Data Type</b></td><td><b>Description</b></td></tr><tr><td style="white-space:nowrap">TABLE_CAT</td><td>String</td><td>The database name.</td></tr><tr><td style="white-space:nowrap">TABLE_SCHEM</td><td>String</td><td>The table schema.</td></tr><tr><td style="white-space:nowrap">TABLE_NAME</td><td>String</td><td>The table name.</td></tr><tr><td style="white-space:nowrap">COLUMN_NAME</td><td>String</td><td>The column name.</td></tr><tr><td style="white-space:nowrap">DATA_TYPE</td><td>int</td><td>The data type identified by the value of a constant defined in java.sql.Types.</td></tr><tr><td style="white-space:nowrap">TYPE_NAME</td><td>String</td><td>The data type name used by the driver.</td></tr><tr><td style="white-space:nowrap">COLUMN_SIZE</td><td>int</td><td>The length in characters of the column or the numeric precision.</td></tr><tr><td style="white-space:nowrap">BUFFER_LENGTH</td><td>int</td><td>The buffer length.</td></tr><tr><td style="white-space:nowrap">DECIMAL_DIGITS</td><td>int</td><td>The column scale or number of digits to the right of the decimal point.</td></tr><tr><td style="white-space:nowrap">NUM_PREC_RADIX</td><td>int</td><td>The radix, or base.</td></tr><tr><td style="white-space:nowrap">NULLABLE</td><td>int</td><td>Whether the column can contain null as defined by the following JDBC DatabaseMetaData constants: columnNoNulls (0) or columnNullable (1).</td></tr><tr><td style="white-space:nowrap">REMARKS</td><td>String</td><td>The column description.</td></tr><tr><td style="white-space:nowrap">COLUMN_DEF</td><td>String</td><td>The default value for the column.</td></tr><tr><td style="white-space:nowrap">SQL_DATA_TYPE</td><td>int</td><td>Reserved by the specification.</td></tr><tr><td style="white-space:nowrap">SQL_DATETIME_SUB</td><td>int</td><td>Reserved by the specification.</td></tr><tr><td style="white-space:nowrap">CHAR_OCTET_LENGTH</td><td>int</td><td>The maximum length of binary and character-based columns.</td></tr><tr><td style="white-space:nowrap">ORDINAL_POSITION</td><td>int</td><td>The column index, starting at 1.</td></tr><tr><td style="white-space:nowrap">IS_NULLABLE</td><td>String</td><td>Whether a null value is allowed: YES or NO.</td></tr><tr><td style="white-space:nowrap">SCOPE_CATALOG</td><td>String</td><td>The table catalog that is the scope of a reference attribute.</td></tr><tr><td style="white-space:nowrap">SCOPE_SCHEMA</td><td>String</td><td>The table schema that is the scope of a reference attribute.</td></tr><tr><td style="white-space:nowrap">SCOPE_TABLE</td><td>String</td><td>The table name that is the scope of a reference attribute.</td></tr><tr><td style="white-space:nowrap">SOURCE_DATA_TYPE</td><td>int</td><td>The source type of a distinct type. Or, a user-generated Ref type. If DATA_TYPE is not DISTINCT, this value is null. If a user-generated Ref, this value is null.</td></tr><tr><td style="white-space:nowrap">IS_AUTOINCREMENT</td><td>String</td><td>Whether the column value is assigned by Apache Kafka in fixed increments.</td></tr><tr><td style="white-space:nowrap">IS_GENERATEDCOLUMN</td><td>String</td><td>Whether the column is generated: YES or NO.</td></tr></table></center><p />
</p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka -  Procedures" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka -  Procedures: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1> Procedures</h1>
       

    <div class="chapter_content" id="pg_spmetadataj">

<p>You can use the DatabaseMetaData interface to retrieve stored procedure information. The getProcedures method returns descriptions of the available stored procedures. 
</p>

<p>The following code retrieves the names of the available stored procedures:
<br/><pre lang="">String connectionString = "jdbc:apachekafka:User=admin;Password=pass;BootStrapServers=https://localhost:9091;Topic=MyTopic;";

Connection conn = DriverManager.getConnection(connectionString);
DatabaseMetaData meta = conn.getMetaData();
ResultSet rs = meta.getProcedures(null, null, "%");
while(rs.next()){
  System.out.println(rs.getString("PROCEDURE_NAME"));
}</pre>
The getProcedures method returns the following columns:
<p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap"><b>Column Name</b></td><td><b>Data Type</b></td><td><b>Description</b></td></tr><tr><td style="white-space:nowrap">PROCEDURE_CAT</td><td>String</td><td>The catalog the procedure belongs to.</td></tr><tr><td style="white-space:nowrap">PROCEDURE_SCHEM</td><td>String</td><td>The schema the procedure belongs to.</td></tr><tr><td style="white-space:nowrap">PROCEDURE_NAME</td><td>String</td><td>The stored procedure name.</td></tr><tr><td style="white-space:nowrap">REMARKS</td><td>String</td><td>The description of the stored procedure.</td></tr><tr><td style="white-space:nowrap">PROCEDURE_TYPE</td><td>short</td><td>Returns 2 if the procedure returns a result. Returns 1 if the procedure does not return a result. Returns 0 if unknown.</td></tr><tr><td style="white-space:nowrap">SPECIFIC_NAME</td><td>String</td><td>The name that uniquely identifies the stored procedure within its schema.</td></tr></table></center><p />

  </p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka -  Procedure Parameters" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka -  Procedure Parameters: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1> Procedure Parameters</h1>
       

    <div class="chapter_content" id="pg_spmetadataparamj">

<p>You can use the DatabaseMetaData interface to retrieve stored procedure information. The getProcedureColumns method returns descriptions of stored procedure parameters.
You can restrict the results by the stored procedure name.</p>

<p>The following code example outputs information about the parameters of the SampleProcedure stored procedure:
<br/><pre lang="">String connectionString = "jdbc:apachekafka:User=admin;Password=pass;BootStrapServers=https://localhost:9091;Topic=MyTopic;";

Connection conn = DriverManager.getConnection(connectionString);
DatabaseMetaData meta = conn.getMetaData();
ResultSet rs=meta.getProcedureColumns(null, null, "SampleProcedure", null);  
while(rs.next()) {   
  for(int i=1;i&lt;=rs.getMetaData().getColumnCount();i++)  {
    System.out.println(rs.getMetaData().getColumnName(i) +"="+rs.getString(i));
  }
}</pre>

The getProcedureColumns method returns the following columns:
<p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap"><b>Column Name</b></td><td><b>Data Type</b></td><td><b>Description</b></td></tr><tr><td style="white-space:nowrap">PROCEDURE_CAT</td><td>String</td><td>The catalog that the procedure belongs to.</td></tr><tr><td style="white-space:nowrap">PROCEDURE_SCHEM</td><td>String</td><td>The schema that the procedure belongs to.</td></tr><tr><td style="white-space:nowrap">PROCEDURE_NAME</td><td>String</td><td>The name of the stored procedure.</td></tr><tr><td style="white-space:nowrap">COLUMN_NAME</td><td>String</td><td>The name of the procedure column.</td></tr><tr><td style="white-space:nowrap">COLUMN_TYPE</td><td>String</td><td>The type of procedure column as defined by the following DatabaseMetaData constants: procedureColumnIn (1), procedureColumnInOut (2), procedureColumnResult (3), procedureColumnOut (4), and procedureColumnReturn (5).</td></tr><tr><td style="white-space:nowrap">DATA_TYPE</td><td>int</td><td>The data type name as defined in java.sql.Types.</td></tr><tr><td style="white-space:nowrap">TYPE_NAME</td><td>String</td><td>The driver-defined data type name.</td></tr><tr><td style="white-space:nowrap">PRECISION</td><td>int</td><td>The number of digits allowed for numeric data.</td></tr><tr><td style="white-space:nowrap">LENGTH</td><td>int</td><td>The number of characters allowed for character data. The number of digits allowed for numeric data.</td></tr><tr><td style="white-space:nowrap">SCALE</td><td>short</td><td>The number of digits to the right of the decimal point in numeric data.</td></tr><tr><td style="white-space:nowrap">RADIX</td><td>short</td><td>The radix, or base.</td></tr><tr><td style="white-space:nowrap">NULLABLE</td><td>short</td><td>Whether the parameter can contain null as defined by the following DatabaseMetaData constants: parameterNoNulls (0), parameterNullable (1), and parameterNullableUnknown (2).</td></tr><tr><td style="white-space:nowrap">REMARKS</td><td>String</td><td>The description of the parameter.</td></tr><tr><td style="white-space:nowrap">COLUMN_DEF</td><td>String</td><td>The default value for the parameter.</td></tr><tr><td style="white-space:nowrap">SQL_DATA_TYPE</td><td>int</td><td>Reserved in the specification.</td></tr><tr><td style="white-space:nowrap">SQL_DATETIME_SUB</td><td>int</td><td>Reserved in the specification.</td></tr><tr><td style="white-space:nowrap">CHAR_OCTET_LENGTH</td><td>int</td><td>The maximum length of binary-based and character-based columns. Null for other data types.</td></tr><tr><td style="white-space:nowrap">ORDINAL_POSITION</td><td>int</td><td>The index of the output parameter.</td></tr><tr><td style="white-space:nowrap">IS_NULLABLE</td><td>String</td><td>Whether the column can include null: YES or NO.</td></tr><tr><td style="white-space:nowrap">SPECIFIC_NAME</td><td>String</td><td>The name that uniquely identifies the stored procedure within its schema.</td></tr></table></center><p />



  </p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka -  Primary Keys" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka -  Primary Keys: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1> Primary Keys</h1>
       

    <div class="chapter_content" id="pg_primarykeysjdbc">

<p>You can use the getPrimaryKeys method to return information about the primary keys for Apache Kafka tables. You can restrict the results by the table name.</p>

<p>The following code example outputs the column or columns composing the primary key for the SampleTable_1 table:
<br/><pre lang="">String connectionString = "jdbc:apachekafka:User=admin;Password=pass;BootStrapServers=https://localhost:9091;Topic=MyTopic;";

Connection conn = DriverManager.getConnection(connectionString);
ResultSet rs = conn.getMetaData().getPrimaryKeys(null,null,"SampleTable_1");
while(rs.next()){
  System.out.println(rs.getString("COLUMN_NAME"));
}</pre></p>

<p>The getPrimaryKeys method returns the following columns:
<p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap"><b>Column Name</b></td><td><b>Data Type</b></td><td><b>Description</b></td></tr><tr><td style="white-space:nowrap">TABLE_CAT</td><td>String</td><td>The table catalog</td></tr><tr><td style="white-space:nowrap">TABLE_SCHEM</td><td>String</td><td>The table schema.</td></tr><tr><td style="white-space:nowrap">TABLE_NAME</td><td>String</td><td>The table name.</td></tr><tr><td style="white-space:nowrap">COLUMN_NAME</td><td>String</td><td>The column name.</td></tr><tr><td style="white-space:nowrap">KEY_SEQ</td><td>short</td><td>The sequence number, or column index starting from 1, within the foreign key.</td></tr><tr><td style="white-space:nowrap">PK_NAME</td><td>String</td><td>The primary key name.</td></tr></table></center><p />
</p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka -  Connection Properties" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka -  Connection Properties: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1> Connection Properties</h1>
       

    <div class="chapter_content" id="pg_connmetadatajava">

<p>The available connection properties can be retrieved with the getPropertyInfo method of the Driver class. This method returns an array with elements of type DriverPropertyInfo. 
<br/><pre lang="">String connectionString = "jdbc:User=admin;Password=pass;BootStrapServers=https://localhost:9091;Topic=MyTopic;:";

Driver driver = DriverManager.getDriver(connectionString);
Properties info = new Properties();
DriverPropertyInfo[] attr = driver.getPropertyInfo(connectionString,info);
for(int i=0;i&lt;attr.length;i++){
  System.out.println(attr[i].name);
  System.out.println(attr[i].description);
  System.out.println(attr[i].required);
  System.out.println(attr[i].value);
  String[] c = attr[i].choices;
  if(c != null) {
    for(String s: c)
      System.out.println(s);
  }
}</pre></p>

<p>The DriverPropertyInfo class has the following properties:
<p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap"><b>Property Name</b></td><td><b>Data Type</b></td><td><b>Description</b></td></tr><tr><td style="white-space:nowrap">Name</td><td>String</td><td>The name of the connection property.</td></tr><tr><td style="white-space:nowrap">Description</td><td>String</td><td>The description for the connection property.</td></tr><tr><td style="white-space:nowrap">Required</td><td>boolean</td><td>Whether the connection property must be set to connect to Apache Kafka.</td></tr><tr><td style="white-space:nowrap">Choices</td><td>String[]</td><td>An array of the allowed values for the connection property.</td></tr><tr><td style="white-space:nowrap">Value</td><td>String</td><td>The current value of the connection property or the default value if one is not set by the user.</td></tr></table></center><p />

</p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka -  Result Sets" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka -  Result Sets: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1> Result Sets</h1>
       

    <div class="chapter_content" id="pg_resultsetmetadatajdbc">

<p>You can use ResultSetMetaData to retrieve metadata about the results of a query. 
</p>

<p>The query can contain any of the following:
<ul><li>Aliases</li><li>Fully qualified names</li><li>Generated columns</li></ul></p>

<p>You can instantiate a ResultSetMetaData object by invoking the getMetaData method of the Statement class. A ResultSetMetaData instance is populated with data after the statement has been executed. The following query prints out the columns in the result of the query:
<br/><pre lang="">String connectionString = "jdbc:apachekafka:User=admin;Password=pass;BootStrapServers=https://localhost:9091;Topic=MyTopic;";

Connection conn = DriverManager.getConnection(connectionString);
PreparedStatement pstmt = conn.prepareStatement("SELECT Id, Column1 AS My_Column1, GETDATE() FROM SampleTable_1 WHERE Column2 = 'Bob'");
pstmt.executeQuery();
ResultSetMetaData rs = pstmt.getMetaData();
for(int i=1;i&lt;=rs.getColumnCount();i++) {
  System.out.println(rs.getColumnName(i));
}</pre>


</p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - Advanced Features" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - Advanced Features: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>Advanced Features</h1>
       

    <div class="chapter_content" id="pg_advancedfeatures">

<p>This section details a selection of advanced features of the Apache Kafka driver.

</p>

<p>
<h2>User Defined Views</h2>
 </p>

<p>The driver allows you to define virtual tables, called <i>user defined views</i>, whose contents are decided by a pre-configured query.  
These views are useful when you cannot directly control
queries being issued to the drivers. See <a href="#pg_userviews">User Defined Views</a> for an overview of creating and configuring custom views.


<h2>SSL Configuration</h2>
</p>

<p>Use <a href="#pg_advancedssl">SSL Configuration</a> to adjust how driver handles TLS/SSL certificate negotiations. You can choose from various certificate formats; 
see the <u>SSLServerCert</u> property under "Connection String Options" for more information.</p>

<p>
<h2>Firewall and Proxy</h2>
</p>

<p>Configure the driver for compliance with <a href="#pg_advancedproxy">Firewall and Proxy</a>, including Windows proxies and HTTP proxies. You can also set up tunnel connections.</p>

<p>

<h2>JDBC Remoting</h2>
</p>

<p>Configure <a href="#pg_remotingintroj">JDBC Remoting</a> to connect to the driver from remote machines. You can choose between remoting via CLI and using a configuration file. 





</p>

<p>
<h2>Caching Data</h2>
</p>

<p><a href="#pg_caching">Caching Data</a> enables faster access to data and reduces the number of API calls, improving performance. 
The connector supports a simple caching model where multiple connections can also share the cache over time.
When configuring the cache connection, you can specify automatic or explicit data caching. 


<h2>Query Processing</h2>
</p>

<p>The driver offloads as much of the SELECT statement processing as possible to Apache Kafka and then processes the rest of the query in memory (client-side).</p>

<p>See <a href="#pg_advancedqueryproc">Query Processing</a> for more information.


<h2>Logging</h2>
</p>

<p>See <a href="#pg_advancedlogging">Logging</a> for an overview of configuration settings that can be used to refine CData logging. For basic logging, you only need to set two 
connection properties, but there are numerous features that support more refined logging, where you can select subsets of information to be logged using the <u>LogModules</u> connection
 property. </p>

<p></p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka -  User Defined Views" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka -  User Defined Views: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1> User Defined Views</h1>
       

    <div class="chapter_content" id="pg_userviews">

<p>The CData JDBC Driver for Apache Kafka allows you to define a virtual table whose contents are decided by a pre-configured query. These are called <i>User Defined Views</i>, which  are useful in situations where you cannot directly control the query being issued to the driver, e.g. when using the driver from a tool. 

The User Defined Views can be used to define predicates that are always applied. If you specify additional predicates in the query to the view, they are combined with the query already defined as part of the view.</p>

<p>There are two ways to create user defined views:
<ul><li>Create a JSON-formatted configuration file defining the views you want. </li><li>DDL statements.</li></ul></p>

<p>

<h2>Defining Views Using a Configuration File</h2>

User Defined Views are defined in a JSON-formatted configuration file called <var>UserDefinedViews.json</var>. The driver automatically detects the views specified in this file. </p>

<p>You can also have multiple view definitions and control them using the <u>UserDefinedViews</u> connection property. When you use this property, only the specified views are seen by the driver.</p>

<p>This User Defined View configuration file is formatted as follows:
<ul><li>Each root element defines the name of a view.</li><li>Each root element contains a child element, called <b>query</b>, which contains the custom SQL query for the view.</li></ul></p>

<p>For example:
<br/><pre lang="">{
	"MyView": {
		"query": "SELECT * FROM SampleTable_1 WHERE MyColumn = 'value'"
	},
	"MyView2": {
		"query": "SELECT * FROM MyTable WHERE Id IN (1,2,3)"
	}
}</pre>

Use the <u>UserDefinedViews</u> connection property to specify the location of your JSON configuration file. For example:
<br/><pre lang="">"UserDefinedViews", "C:\\Users\\yourusername\\Desktop\\tmp\\UserDefinedViews.json"</pre>
</p>

<p>
<h2>Defining Views Using DDL Statements</h2>
</p>

<p>The driver is also capable of creating and altering the schema via DDL Statements such as CREATE LOCAL VIEW, ALTER LOCAL VIEW, and DROP LOCAL VIEW.</p>

<p>
<h3>Create a View</h3>
</p>

<p>To create a new view using DDL statements, provide the view name and query as follows:
<br/><pre lang="">CREATE LOCAL VIEW [MyViewName] AS SELECT * FROM Customers LIMIT 20;</pre></p>

<p>If no JSON file exists, the above code creates one. The view is then created in the JSON configuration file and is now discoverable. The JSON file location is specified by the <u>UserDefinedViews</u> connection property. 


<h3>Alter a View</h3>
</p>

<p>To alter an existing view, provide the name of an existing view alongside the new query you would like to use instead:
<br/><pre lang="">ALTER LOCAL VIEW [MyViewName] AS SELECT * FROM Customers WHERE TimeModified &gt; '3/1/2020';</pre></p>

<p>The view is then updated in the JSON configuration file.

<h3>Drop a View</h3>
</p>

<p>To drop an existing view, provide the name of an existing schema alongside the new query you would like to use instead.
<br/><pre lang="">DROP LOCAL VIEW [MyViewName]</pre></p>

<p>This removes the view from the JSON configuration file. It can no longer be queried.
</p>

<p>
<h2>Schema for User Defined Views</h2>

User Defined Views are exposed in the <b>UserViews</b> schema by default. This is done to avoid the view's name clashing with an actual entity in the data model. You can change the name of the schema used for UserViews by setting the <u>UserViewsSchemaName</u> property.



<h2>Working with User Defined Views</h2>

For example, a SQL statement with a User Defined View called <var>UserViews.RCustomers</var> only lists customers in Raleigh:

<br/><pre lang="">SELECT * FROM Customers WHERE City = 'Raleigh';</pre>

An example of a query to the driver:

<br/><pre lang="">SELECT * FROM UserViews.RCustomers WHERE Status = 'Active';</pre> 

Resulting in the effective query to the source:

<br/><pre lang="">SELECT * FROM Customers WHERE City = 'Raleigh' AND Status = 'Active';</pre>

That is a very simple example of a query to a User Defined View that is effectively a combination of the view query and the view definition. It is possible to compose these queries in much more complex patterns. All SQL operations are allowed in both queries and are combined when appropriate.
</p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka -  SSL Configuration" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka -  SSL Configuration: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1> SSL Configuration</h1>
       

    <div class="chapter_content" id="pg_advancedssl">

<p>
<h2>Customizing the SSL Configuration</h2>

</p>

<p>By default, the driver attempts to negotiate SSL/TLS by checking the server's certificate against the system's trusted certificate store.</p>

<p>To specify another certificate, see the <u>SSLServerCert</u> property for the available formats to do so. </p>

<p>
<h2>Client SSL Certificates</h2>
</p>

<p>The Apache Kafka driver also supports setting client certificates. Set the following to connect using a client certificate.
<ul><li><u>SSLClientCert</u>: The name of the certificate store for the client certificate.</li><li><u>SSLClientCertType</u>: The type of key store containing the TLS/SSL client certificate.</li><li><u>SSLClientCertPassword</u>: The password for the TLS/SSL client certificate.</li><li><u>SSLClientCertSubject</u>: The subject of the TLS/SSL client certificate.</li></ul></p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka -  Firewall and Proxy" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka -  Firewall and Proxy: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1> Firewall and Proxy</h1>
       

    <div class="chapter_content" id="pg_advancedproxy">

<p>
<h2>Connecting Through a Firewall or Proxy</h2>


<h3>HTTP Proxies</h3>

</p>

<p>To connect through the Windows system proxy, you do not need to set any additional connection properties. To connect to other proxies, set <u>ProxyAutoDetect</u> to false.
</p>

<p>In addition, to authenticate to an HTTP proxy, set <u>ProxyAuthScheme</u>, <u>ProxyUser</u>, and <u>ProxyPassword</u>, in addition to <u>ProxyServer</u> and <u>ProxyPort</u>. 


<h3>Other Proxies</h3>

</p>

<p>Set the following properties: 

<ul><li>To use a proxy-based firewall, set <u>FirewallType</u>, <u>FirewallServer</u>, and <u>FirewallPort</u>.</li><li>To tunnel the connection, set <u>FirewallType</u> to TUNNEL.</li><li>To authenticate, specify <u>FirewallUser</u> and <u>FirewallPassword</u>. </li><li>To authenticate to a SOCKS proxy, additionally set <u>FirewallType</u> to SOCKS5.</li></ul></p>

<p></p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka -  JDBC Remoting" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka -  JDBC Remoting: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1> JDBC Remoting</h1>
       

    <div class="chapter_content" id="pg_remotingintroj">

<p>The JDBC remoting feature allows you to connect to the JDBC driver from remote machines. The remoting feature is a daemon process that listens for database queries and responds to them. The driver supports the MySQL protocol for remoting.</p>

<p>JDBC remoting will allow any MySQL client (ODBC drivers, programming languages like PHP, Perl, Python, tools like MySQL workbench, and many other applications) to easily connect to Apache Kafka.

<h2>Configuring the Driver for Remoting</h2>
</p>

<p>Remoting can be configured through either a Command Line Interface or an INI-formatted configuration file.

<h3>Remoting via CLI</h3>
</p>

<p>The MySQL daemon can be started directly from the command line. This can be accomplish by invoking the JAR and specifying the necessary <a href="#pg_cli">CLI Options</a>.

<h3>Remoting with a Configuration File</h3>
</p>

<p>If you prefer not to provide every configuration property directly in the command line, a <a href="#pg_conf">Configuration File</a> can be defined, which will explicitly list all properties. Once defined, the daemon can be started with a single reference to this file in the CLI.
</p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka -  CLI Options" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka -  CLI Options: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1> CLI Options</h1>
       

    <div class="chapter_content" id="pg_cli">

<p>You can start the MySQL daemon from the command line, as shown below:
<br/><pre lang="">java -jar cdata.jdbc.apachekafka.jar [ options ]</pre>
The following command-line options are available:
<p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap"></td><td><b>Option</b></td><td><b>Description</b></td></tr><tr><td style="white-space:nowrap"></td><td>-h, --help</td><td>Display help for available options and exit.</td></tr><tr><td style="white-space:nowrap"></td><td>-f, --config-file</td><td>The configuration file for the daemon.</td></tr><tr><td style="white-space:nowrap"></td><td>-u, --user</td><td>The user allowed to connect. Use a configuration file to configure multiple users. If a user is specified on the command line, then only that user is given access.</td></tr><tr><td style="white-space:nowrap"></td><td>-p, --password</td><td>The password for the user specified with the user option. If both user and password are set on the command line, the users section in the config file is ignored.</td></tr><tr><td style="white-space:nowrap"></td><td>-d, --database</td><td>The database that clients will use to connect. If multiple databases are specified in the config file, connections are allowed to only the database specified on the command line.</td></tr><tr><td style="white-space:nowrap"></td><td>-c, --connection</td><td>The connection string used to connect to the data source being surfaced.  If no connection string is specified on the command line, the connection string is read from the config file.</td></tr><tr><td style="white-space:nowrap"></td><td>-P, --port</td><td>The port number to use to listen for TCP/IP connections. The default port is 3306.</td></tr><tr><td style="white-space:nowrap"></td><td>-m, --max-connections</td><td>The maximum number of allowed TCP/IP connections. The default value is 25 connections.</td></tr><tr><td style="white-space:nowrap"></td><td>    --session-timeout</td><td>The session timeout time in seconds. The default timeout is 20 seconds.</td></tr><tr><td style="white-space:nowrap"></td><td>-t, --protocol</td><td>The protocol used for remoting. The default value is MySQL.</td></tr><tr><td style="white-space:nowrap"></td><td>-g, --logfile</td><td>The full path of the log file.</td></tr><tr><td style="white-space:nowrap"></td><td>-F, --logrotationscheme</td><td>The interval at which to truncate the logs. The options are 1 (daily in the format [MyFileName]_2016_3_21.txt), 2 (weekly in the format [MyFileName]_Week_5.txt, where 5 is the fifth week in the  year), and 3 (monthly in the format [MyFileName]_2016_3_21.txt).</td></tr><tr><td style="white-space:nowrap"></td><td>-v, --verbosity</td><td>The verbosity of the log. 1 is informational. Levels up to 5 add the following subsequent details: (2) HTTP headers,  (3) the HTTP body, (4) transport-level communication including SSL, and (5) interface commands and other data source communication.  </td></tr><tr><td style="white-space:nowrap"></td><td>    --test</td><td>The database to test the connection with. If this property is not specified, the default database is used.</td></tr><tr><td style="white-space:nowrap"></td><td>    --ssl-cert</td><td>The path to the SSL certificate.</td></tr><tr><td style="white-space:nowrap"></td><td>    --ssl-subject</td><td>The subject of the SSL certificate.</td></tr><tr><td style="white-space:nowrap"></td><td>    --ssl-password</td><td>The password of the SSL certificate.</td></tr><tr><td style="white-space:nowrap"></td><td>-n, --nodeid</td><td>Displays the NodeId of this machine.</td></tr><tr><td style="white-space:nowrap"></td><td>-l, --license</td><td>Installs the license on this machine. This option will prompt you for the type of license and other details.</td></tr></table></center><p />
Options specified on the command line take precedence over options specified in the config file. You can pass in command-line options to specify a restricted subset of the options allowed in the <a href="#pg_conf">Configuration File</a>.
</p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka -  Configuration File" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka -  Configuration File: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1> Configuration File</h1>
       

    <div class="chapter_content" id="pg_conf">

<p>You can save configuration settings for MySQL remoting in a config file. The file must be structured in the INI file format. Specify this file with the -f command-line option. The config file can have the following sections:


<h2>[mysqld]</h2>
</p>

<p>In the mysqld section, use the following properties to configure the MySQL daemon:
<p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap"></td><td><b>Name</b></td><td><b>Description</b></td></tr><tr><td style="white-space:nowrap"></td><td>port</td><td>The port number to use to listen for TCP/IP connections. The default port is 3306.</td></tr><tr><td style="white-space:nowrap"></td><td>max-connections</td><td>The maximum number of allowed TCP/IP connections. 25 is the default.</td></tr><tr><td style="white-space:nowrap"></td><td>session-timeout</td><td>The session timeout time in seconds. The default timeout is 20 seconds.</td></tr><tr><td style="white-space:nowrap"></td><td>logfile</td><td>The full path of the log file.</td></tr><tr><td style="white-space:nowrap"></td><td>verbosity</td><td>The verbosity of the log. 1 is informational. Levels up to 5 add the following subsequent details: (2) HTTP headers,  (3) the HTTP body, (4) transport-level communication including SSL, and (5) interface commands and other data source communication.</td></tr><tr><td style="white-space:nowrap"></td><td>logrotationscheme</td><td>The interval at which to truncate the logs. The options are 1 (daily in the format [MyFileName]_2016_3_21.txt), 2 (weekly in the format [MyFileName]_Week_5.txt, where 5 is the fifth week in the  year), and 3 (monthly in the format [MyFileName]_2016_3_21.txt).

<p>The default is 2.</p>
</td></tr><tr><td style="white-space:nowrap"></td><td>ssl-cert</td><td>The path to the SSL certificate.</td></tr><tr><td style="white-space:nowrap"></td><td>ssl-subject</td><td>The subject of the SSL certificate.</td></tr><tr><td style="white-space:nowrap"></td><td>ssl-password</td><td>The password of the SSL certificate.</td></tr></table></center><p />


<h2>[databases]</h2>
</p>

<p>In the databases section, define keys that map the MySQL database to Apache Kafka connection strings. Clients connect to the MySQL database defined here.  To connect to Apache Kafka, the driver uses the connection string that corresponds to this key. 
<br/><pre lang="">[databases]
ApacheKafka = "User=admin;Password=pass;BootStrapServers=https://localhost:9091;Topic=MyTopic;"</pre>


<h2>[users]</h2>
</p>

<p>In the users section, define the usernames and passwords of the users of the server. If the acl section is not defined, all users have access to all databases.


<h2>[mysql_vars]</h2>
</p>

<p>In the mysql_vars section, define system variables for the MySQL server. The standard variables are supported. Below are several examples:
<p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap"></td><td><b>Name</b></td><td><b>Description</b></td></tr><tr><td style="white-space:nowrap"></td><td>version_comment</td><td>This value is hard-coded as CData JDBC Driver for Apache Kafka (MySQL Remoting).</td></tr><tr><td style="white-space:nowrap"></td><td>character_set_client</td><td>The character_set used in statements sent by the client.</td></tr></table></center><p />


<h2>[acl]</h2>
</p>

<p>In the acl section, allow users to access Apache Kafka databases. Databases must be defined in the databases section. Users must be defined in the users section. Use commas to separate users authorized to access the specified database.


<h2>Example Config File</h2>
</p>

<p>Below is an example config file. The example includes all properties required to configure the server. It also shows how to configure access control for several users and Apache Kafka instances.

<br/><pre lang="">[mysqld]
port = 3306
max-connections = 25
session-timeout = 20
logfile = ApacheKafkaRemotingLog.txt
verbosity = 2
ssl-cert = "CData.JDBC.ApacheKafka.Remoting.pfx"
ssl-subject = &lt;subject&gt;
ssl-password = &lt;password&gt;

[databases]
ApacheKafka = "User=admin;Password=pass;BootStrapServers=https://localhost:9091;Topic=MyTopic;"
ApacheKafka_ReadOnly = "User=admin;Password=pass;BootStrapServers=https://localhost:9091;Topic=MyTopic;ReadOnly=True"

[users]
root = &lt;password&gt;
test = &lt;password&gt;

[mysql_vars]
version_comment =  "CData JDBC Driver for Apache Kafka (MySQL Remoting)"

[acl]
ApacheKafka = root
ApacheKafka_ReadOnly = root, test</pre></p>

<p>You can further restrict the allowed options by passing in options on the command line. See <a href="#pg_cli">CLI Options</a> for more information.
</p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka -  Caching Data" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka -  Caching Data: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1> Caching Data</h1>
       

    <div class="chapter_content" id="pg_caching">

<p>
<h2>Caching Data</h2>

</p>

<p>Caching data provides several benefits, including faster access to data and reducing the number of API calls, which improve performance. The connector supports a simple caching model where multiple connections can also share the cache over time. You can enable and configure caching features by setting the necessary connection properties. 


<h2>Contents</h2>

</p>

<p>The sections in this chapter detail the driver's caching functionality and link to the corresponding connection properties, as well as SQL statements.  


<h3>Configuring the Cache Connection</h3>
</p>

<p><a href="#pg_workings">Configuring the Cache Connection</a> describes the properties that you can set when configuring the cache database. 


<h3>Caching Metadata</h3>
</p>

<p><a href="#pg_cachingMetadata">Caching Metadata</a> describes the <u>CacheMetadata</u> property. This property determines whether or not to cache the table metadata to a file store.


<h3>Automatically Caching Data</h3>
</p>

<p><a href="#pg_cacheAutomatically">Automatically Caching Data</a> describes how the driver automatically refreshes the cache when the <u>AutoCache</u> property is set.


<h3>Explicitly Caching Data</h3>
</p>

<p><a href="#pg_cacheExplicitly">Explicitly Caching Data</a> describes how you can decide what data is stored in the cache and when it is updated.


<h3>Data Type Mapping</h3>
</p>

<p><a href="#pg_cachedatatypemapping">Data Type Mapping</a> shows the mappings between the data types configured in the schema and the data types in the database.


  </p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - Configuring the Cache Connection" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - Configuring the Cache Connection: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>Configuring the Cache Connection</h1>
       

    <div class="chapter_content" id="pg_workings">

<p>
<h2>Configuring the Caching Database</h2>

</p>

<p>This section describes the properties for caching data to the persistent store of your choice. 


<h3>CacheLocation</h3>

</p>

<p>The <u>CacheLocation</u> property species the path to a file-system-based database. When caching is enabled, a file-system-based database is used by default. If <u>CacheLocation</u> is not specified, this database is stored at the path in <u>Location</u>. If neither of these connection properties are specified, the driver uses a platform-dependent default location.


<h3>CacheConnection</h3>

</p>

<p>The <u>CacheConnection</u> property specifies a database driver and the connection string to the caching database.



<h3>CacheDriver</h3>
</p>

<p>The <u>CacheDriver</u> property specifies a database driver and the connection string to the caching database.


  </p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - Caching Metadata" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - Caching Metadata: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>Caching Metadata</h1>
       

    <div class="chapter_content" id="pg_cachingMetadata">

<p>This section describes how to enable caching metadata and how to update the metadata cache.
</p>

<p>Before being able to query data, the connector requires relevant metadata to be retrieved. By default, metadata is cached in memory and shared across connections. But if you want to persist across processes, or if metadata requests are expensive, the solution is to cache the metadata to disk.


<h2>Enable Caching Metadata</h2>

</p>

<p>To enable caching of metadata, set <u>CacheMetadata</u> = true and see <a href="#pg_workings">Configuring the Cache Connection</a> for instructions on how to configure your connection string. The driver caches the metadata the first time it is needed and uses the metadata cache for subsequent requests.


<h2>Update the Metadata Cache</h2>

</p>

<p>Because metadata is cached, changes to metadata on the live source, for example, adding or removing a column or attribute, are not automatically reflected in the metadata cache. To get updates to the live metadata, you need to delete or drop the cached data.


<h2>Cache Metadata from Code</h2>

</p>

<p>The following code can be used to build the metadata cache. This is especially useful when using the Object-Relational Mapping Framework (for example, Hibernate).

<br/><pre lang="">Connection conn = DriverManager.getConnection("jdbc:apachekafka:Cache Location=C:\\cdata.apachekafka.db;AutoCache=True;User=admin;Password=pass;BootStrapServers=https://localhost:9091;Topic=MyTopic;");
DatabaseMetaData table_meta = conn.getMetaData();

ResultSet rs=table_meta.getTables(null, null, "%", null); 

while (rs.next()) {
  String tableName = rs.getString("TABLE_NAME");
  System.out.println("Cached metadata for table: "+tableName);
  ResultSet cols = table_meta.getColumns(null, null, tableName, null);
  while(rs.next()){
  }
}
System.out.println();
System.out.println("All tables cached.");
conn.close();</pre>
  </p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - Automatically Caching Data" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - Automatically Caching Data: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>Automatically Caching Data</h1>
       

    <div class="chapter_content" id="pg_cacheAutomatically">

<p>Automatically caching data is useful when you do not want to rebuild the cache for each query. When you query data for the first time, the driver automatically initializes and builds a cache in the background. When <u>AutoCache</u> = true, the driver uses the cache for subsequent query executions, resulting in faster response times.
</p>

<p> 


<h2>Configuring Automatic Caching</h2>
</p>

<p>

<h2>Caching the SampleTable_1 Table</h2>
</p>

<p>The following example caches the SampleTable_1 table in the file specified by the <u>CacheLocation</u> property of the connection string. 


<br/><pre lang="">String connectionString = "jdbc:apachekafka:Cache Location=C:\\cache.db;" +
                          "AutoCache=true;" +
                          "User=admin;Password=pass;BootStrapServers=https://localhost:9091;Topic=MyTopic;";
Connection connection = DriverManager.getConnection(connectionString);
Statement stat = connection.createStatement();
boolean ret = stat.execute("SELECT Id, Column1 FROM SampleTable_1 WHERE Column2 = 'Bob'");
ResultSet rs=stat.getResultSet();
while(rs.next()){
  System.out.println("Read and cached the row with Id "+rs.getString("Id"));
}
connection.close();</pre>




<h2>Common Use Case</h2>
</p>

<p>A common use for automatically caching data is to improve driver performance when making repeated requests to a live data source, such as building a report or creating a visualization. With auto caching enabled, repeated requests to the same data may be executed in a short period of time, but within an allowable tolerance (CacheTolerance) of what is considered "live" data.
  </p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - Explicitly Caching Data" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - Explicitly Caching Data: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>Explicitly Caching Data</h1>
       

    <div class="chapter_content" id="pg_cacheExplicitly">

<p>With explicit caching (<u>AutoCache</u> = false), you decide exactly what data is cached and when to query the cache instead of the live data. Explicit caching gives you full control over the cache contents by using <a href="#pg_cache">CACHE Statements</a>. This section describes some strategies to use the caching features offered by the driver.


<h2>Creating the Cache</h2>

</p>

<p>To load data in the cache, issue the following statement.

<br/><pre lang="">CACHE SELECT * FROM tableName WHERE ...</pre>
</p>

<p>Once the statement is issued, any matching data in tableName is loaded into the corresponding table.


<h2>Updating the Cache</h2>
</p>

<p>This section describes two ways to update the cache.


<h3>Updating with the SELECT Statement</h3>

</p>

<p>The following example shows a statement that can update modified rows and add missing rows in the cached table. However, this statement does not delete extra rows that are already in the cache. This statement only merges the new rows or updates the existing rows. 


<br/><pre lang="">String cmd = "CACHE SELECT * FROM SampleTable_1 WHERE Column2 = 'Bob'", connection";
stat.execute(cmd);
connection.close();</pre>


<h3>Updating with the TRUNCATE Statement</h3>

</p>

<p>The following example shows a statement that can update modified rows and add missing rows in the cached table. This statement can also delete rows in the cache table that are not present in the live data source.


<br/><pre lang="">String cmd = "CACHE WITH TRUNCATE SELECT * FROM SampleTable_1 WHERE Column2 = 'Bob'";
stat.execute(cmd);
connection.close();</pre>



<h2>Query the Data in Online or Offline Mode</h2>
</p>

<p>This section describes how to query the data in online or offline mode.


<h3>Online: Select Cached Tables</h3>
</p>

<p>You can use the tableName#CACHE syntax to explicitly execute queries to the cache while still online, as shown in the following example.
<br/><pre lang="plain">SELECT * FROM SampleTable_1#CACHE</pre>


<h3>Offline: Select Cached Tables</h3>
</p>

<p>With <u>Offline</u> = true, SELECT statements always execute against the local cache database, regardless of whether you explicitly specify the cached table or not. Modification of the cache is disabled in <u>Offline</u> mode to prevent accidentally updating only the cached data. Executing a DELETE/UPDATE/INSERT statement while in <u>Offline</u> mode results in an exception. 
</p>

<p>The following example selects from the local cache but not the live data source because <u>Offline</u> = true.


<br/><pre lang="">Connection connection = DriverManager.getConnection("jdbc:apachekafka:User=admin;Password=pass;BootStrapServers=https://localhost:9091;Topic=MyTopic;Offline=true;Cache Location=C:\\cache.db;");
Statement stat = connection.createStatement();
String query = "SELECT * FROM SampleTable_1 WHERE Column2='Bob' ORDER BY Column1 ASC";
stat.execute(query);
connection.close();</pre>


<h2>Delete Data from the Cache</h2>

</p>

<p>You can delete data from the cache by building a direct connection to the database. Note that the driver does not support manually deleting data from the cache.



<h2>Common Use Case</h2>

</p>

<p>A common use for caching is to have an application always query the cached data and only update the cache at set intervals, such as once every day or every two hours. There are two ways in which this can be implemented:

<ul><li><u>AutoCache</u> = false and <u>Offline</u> = false. All queries issued by the application explicitly reference the tableName#CACHE table. When the cache needs to be updated, the application executes a tableName#CACHE ... statement to bring the cached data up to date.</li><li><u>Offline</u> = true. Caching is transparent to the application. All queries are executed against the table as normal, so most application code does not need to be aware that caching is done. To update the cached data, simply create a separate connection with <u>Offline</u> = false and execute a tableName#CACHE ... statement.</li></ul>
  </p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - Data Type Mapping" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - Data Type Mapping: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>Data Type Mapping</h1>
       

    <div class="chapter_content" id="pg_cachedatatypemapping">

<p>The driver maps types from the data source to the corresponding data type available in the chosen cache database. The following table shows the mappings between the data types configured in the schema and the data types in the database. Some schema types have synonyms which are all listed in the <b>Schema</b> column.


<h2>Data Type Mapping</h2>

</p>

<p><b>Note:</b> String columns can map to different data types depending on their length.
</p>

<p><p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap"><b>Schema</b>                        </td><td><b>.NET</b>         </td><td><b>JDBC</b>           </td><td><b>SQL Server</b> </td><td><b>Derby</b>  </td><td><b>MySQL</b>  </td><td><b>Oracle</b> </td><td><b>SQLite</b>   </td><td><b>Access</b></td></tr><tr><td style="white-space:nowrap">int, integer, int32                  </td><td>Int32               </td><td>int                   </td><td>int               </td><td>INTEGER       </td><td>INT           </td><td>NUMBER        </td><td>integer         </td><td>LONG</td></tr><tr><td style="white-space:nowrap">smallint, short, int16               </td><td>Int16               </td><td>short                 </td><td>smallint          </td><td>SMALLINT      </td><td>SMALLINT      </td><td>NUMBER        </td><td>integer         </td><td>SHORT</td></tr><tr><td style="white-space:nowrap">double, float, real                  </td><td>Double              </td><td>double                </td><td>float             </td><td>DOUBLE        </td><td>DOUBLE        </td><td>NUMBER        </td><td>double          </td><td>DOUBLE</td></tr><tr><td style="white-space:nowrap">date                                 </td><td>DateTime            </td><td>java.sql.Date         </td><td>date              </td><td>DATE          </td><td>DATE          </td><td>DATE          </td><td>date            </td><td>DATETIME</td></tr><tr><td style="white-space:nowrap">datetime, timestamp                  </td><td>DateTime            </td><td>java.sql.Date         </td><td>datetime          </td><td>TIMESTAMP     </td><td>DATETIME      </td><td>TIMESTAMP     </td><td>datetime        </td><td>DATETIME</td></tr><tr><td style="white-space:nowrap">time, timespan                       </td><td>TimeSpan            </td><td>java.sql.Time         </td><td>time              </td><td>TIME          </td><td>TIME          </td><td>TIMESTAMP     </td><td>datetime        </td><td>DATETIME</td></tr><tr><td style="white-space:nowrap">string, varchar                      </td><td>String              </td><td>java.lang.String      </td><td>If length &gt; 4000: nvarchar(max), Otherwise: nvarchar(length)</td><td>If length &gt; 32672: LONG VARCHAR, Otherwise VARCHAR(length)</td><td>If length &gt; 255: LONGTEXT, Otherwise: VARCHAR(length)</td><td>If length &gt; 4000: CLOB, Otherwise: VARCHAR2(length)</td><td>nvarchar(length)</td><td>If length &gt; 255: LONGTEXT, Otherwise: VARCHAR(length)</td></tr><tr><td style="white-space:nowrap">long, int64, bigint                  </td><td>Int64               </td><td>long                  </td><td>bigint            </td><td>BIGINT        </td><td>BIGINT        </td><td>NUMBER        </td><td>bigint          </td><td>LONG</td></tr><tr><td style="white-space:nowrap">boolean, bool                        </td><td>Boolean             </td><td>boolean               </td><td>tinyint           </td><td>SMALLINT      </td><td>BIT           </td><td>NUMBER        </td><td>tinyint         </td><td>BIT</td></tr><tr><td style="white-space:nowrap">decimal, numeric                     </td><td>Decimal             </td><td>java.math.BigDecimal  </td><td>decimal           </td><td>DECIMAL       </td><td>DECIMAL       </td><td>DECIMAL       </td><td>decimal         </td><td>CURRENCY</td></tr><tr><td style="white-space:nowrap">uuid                                 </td><td>Guid                </td><td>java.util.UUID        </td><td>nvarchar(length) </td><td>VARCHAR(length)</td><td>VARCHAR(length) </td><td>VARCHAR2(length)</td><td>nvarchar(length) </td><td>VARCHAR(length)</td></tr><tr><td style="white-space:nowrap">binary, varbinary, longvarbinary     </td><td>byte[]              </td><td>byte[]                </td><td>binary(1000) or varbinary(max) after SQL Server 2000, image otherwise </td><td>BLOB </td><td>LONGBLOB </td><td>BLOB </td><td>BLOB </td><td>LONGBINARY</td></tr></table></center><p />

  </p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka -  Query Processing" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka -  Query Processing: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1> Query Processing</h1>
       

    <div class="chapter_content" id="pg_advancedqueryproc">

<p>
<h2>Query Processing</h2>

CData has a client-side SQL engine built into the driver library. This enables support for the full capabilities that SQL-92 offers, including filters, aggregations, functions, etc. </p>

<p>
For sources that do not support SQL-92, the driver offloads as much of SQL statement processing as possible to Apache Kafka and then processes the rest of the query in memory (client-side). This results in optimal performance.</p>

<p> 

For data sources with limited query capabilities, the driver handles transformations of the SQL query to make it simpler for the driver.
The goal is to make smart decisions based on the query capabilities of the data source to push down as much of the computation as possible. The Apache Kafka Query Evaluation component examines SQL queries and returns information indicating what parts of the query the driver is not capable of executing natively.</p>

<p>
The Apache Kafka Query Slicer component is used in more specific cases to separate a single query into multiple independent queries. The client-side Query Engine makes decisions about simplifying queries, breaking queries into multiple queries, and pushing down or computing aggregations on the client-side while minimizing the size of the result set.</p>

<p>
There's a significant trade-off in evaluating queries, even partially, client-side. There are always queries that are impossible to execute efficiently in this model, and some can be particularly expensive to compute in this manner. CData always pushes down as much of the query as is feasible for the data source to generate the most efficient query possible and provide the most flexible query capabilities.</p>

<p>

<h2>More Information</h2>


For a full discussion of how CData handles query processing, see <a href="https://www.cdata.com/blog/data_connectivity/20181213-cdata-architecture-query-execution">CData Architecture: Query Execution</a>.
</p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka -  Logging" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka -  Logging: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1> Logging</h1>
       

    <div class="chapter_content" id="pg_advancedlogging">

<p>Capturing driver logging can be very helpful when diagnosing error messages or other unexpected behavior.

<h2>Basic Logging</h2>
</p>

<p>You will simply need to set two connection properties to begin capturing driver logging.
<ul><li><u>Logfile</u>: A filepath which designates the name and location of the log file. </li><li><u>Verbosity</u>: This is a numerical value (1-5) that determines the amount of detail in the log. See the page in the Connection Properties section for an explanation of the five levels.</li><li><u>MaxLogFileSize</u>: When the limit is hit, a new log is created in the same folder with the date and time appended to the end. The default limit is 100 MB. Values lower than 100 kB will use 100 kB as the value instead.</li><li><u>MaxLogFileCount</u>: A string specifying the maximum file count of log files. When the limit is hit, a new log is created in the same folder with the date and time appended to the end and the oldest log file will be deleted. Minimum supported value is 2. A value of 0 or a negative value indicates no limit on the count.</li></ul></p>

<p>Once this property is set, the driver will populate the log file as it carries out various tasks, such as when authentication is performed or queries are executed. If the specified file doesn't already exist, it will be created.</p>

<p>
<h2>Log Verbosity</h2>
</p>

<p>The verbosity level determines the amount of detail that the driver reports to the <u>Logfile</u>. <u>Verbosity</u> levels from 1 to 5 are supported. These are described in the following list:
<p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap">1</td><td>Setting <u>Verbosity</u> to 1 will log the query, the number of rows returned by it, the start of execution and the time taken, and any errors.</td></tr><tr><td style="white-space:nowrap">2</td><td>Setting <u>Verbosity</u> to 2 will log everything included in <u>Verbosity</u> 1, cache queries, and additional information about the request.</td></tr><tr><td style="white-space:nowrap">3</td><td>Setting <u>Verbosity</u> to 3 will additionally log HTTP headers, as well as the body of the request and the response.</td></tr><tr><td style="white-space:nowrap">4</td><td>Setting <u>Verbosity</u> to 4 will additionally log transport-level communication with the data source. This includes SSL negotiation.</td></tr><tr><td style="white-space:nowrap">5</td><td>Setting <u>Verbosity</u> to 5 will additionally log communication with the data source and additional details that may be helpful in troubleshooting problems. This includes interface commands.</td></tr></table></center><p />
</p>

<p>The <u>Verbosity</u> should not be set to greater than 1 for normal operation. Substantial amounts of data can be logged at higher verbosities, which can delay execution times.</p>

<p>To refine the logged content further by showing/hiding specific categories of information, see <u>LogModules</u>.


<h2>Sensitive Data</h2>


Verbosity levels 3 and higher may capture information that you do not want shared outside of your organization. The following lists information of concern for each level:</p>

<p><ul><li>Verbosity 3: The full body of the request and the response, which includes all the data returned by the driver</li><li>Verbosity 4: SSL certificates</li><li>Verbosity 5: Any extra transfer data not included at Verbosity 3, such as non human-readable binary transfer data</li></ul>
</p>

<p><b>Best Practices for Data Security</b></p>

<p>Although we mask sensitive values, such as passwords, in the connection string and any request in the log, it is always best practice to review the logs for any sensitive information before sharing outside your organization.</p>

<p>
<h2>Java Logging</h2>
</p>

<p>When Java logging is enabled in <u>Logfile</u>, the <u>Verbosity</u> will instead map to the following logging levels.
<ul><li>0: Level.WARNING</li><li>1: Level.INFO</li><li>2: Level.CONFIG</li><li>3: Level.FINE</li><li>4: Level.FINER</li><li>5: Level.FINEST</li></ul></p>

<p>
<h2>Advanced Logging</h2>
</p>

<p>You may want to refine the exact information that is recorded to the log file. This can be accomplished using the <u>LogModules</u> property.</p>

<p>This property allows you to filter the logging using a semicolon-separated list of logging modules.</p>

<p>All modules are four characters long. <b>Please note that modules containing three letters have a required trailing blank space</b>. The available modules are:
<ul><li><b>EXEC</b>: Query Execution. Includes execution messages for original SQL queries, parsed SQL queries, and normalized SQL queries. Query and page success/failure messages appear here as well.</li><li><b>INFO</b>: General Information. Includes the connection string, driver version (build number), and initial connection messages.</li><li><b>HTTP</b>: HTTP Protocol messages. Includes HTTP requests/responses (including POST messages), as well as Kerberos related messages.</li><li><b>SSL </b>: SSL certificate messages.</li><li><b>OAUT</b>: OAuth related failure/success messages.</li><li><b>SQL </b>: Includes SQL transactions, SQL bulk transfer messages, and SQL result set messages.</li><li><b>META</b>: Metadata cache and schema messages.</li><li><b>TCP </b>: Incoming and Ongoing raw bytes on TCP transport layer messages.</li></ul>
An example value for this property would be.
<br/><pre lang="">LogModules=INFO;EXEC;SSL ;SQL ;META;</pre></p>

<p>Note that these modules refine the information as it is pulled after taking the <u>Verbosity</u> into account.</p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - SQL Compliance" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - SQL Compliance: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>SQL Compliance</h1>
       

    <div class="chapter_content" id="pg_overview">

<p>The CData JDBC Driver for Apache Kafka supports several operations on data, including querying, deleting, modifying, and inserting. </p>

<p>




<h2>SELECT Statements</h2>
</p>

<p></p>

<p>See <a href="#pg_select">SELECT Statements</a> for a syntax reference and examples. </p>

<p>See <a href="#pg_datamodel">Data Model</a> for information on the capabilities of the Apache Kafka API. 


<h2>INSERT Statements</h2>
</p>

<p>See <a href="#pg_insert">INSERT Statements</a> for a syntax reference and examples. 










<h2>CACHE Statements</h2>
</p>

<p>CACHE statements allow granular control over the driver's caching functionality. For a syntax reference and examples, see <a href="#pg_cache">CACHE Statements</a>.</p>

<p>For more information on the caching feature, see <a href="#pg_caching">Caching Data</a>. 

<h2>EXECUTE Statements</h2>
</p>

<p>Use EXECUTE or EXEC statements to execute stored procedures. See <a href="#pg_exec">EXECUTE Statements</a> for a syntax reference and examples.

<h2>Names and Quoting</h2>
</p>

<p><ul><li>Table and column names are considered identifier names; as such, they are restricted to the following characters: [A-Z, a-z, 0-9, _:@]. 
  </li><li>To use a table or column name with characters not listed above, the name must be quoted using square brackets 
      ([name]) in any SQL statement.
  </li><li>Parameter names can optionally start with the @ symbol (e.g., @p1 or @CustomerName) and cannot be quoted. 
  </li><li>Strings must be quoted using single quotes (e.g., 'John Doe').</li></ul>



</p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka -  SQL Functions" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka -  SQL Functions: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1> SQL Functions</h1>
       

    <div class="chapter_content" id="pg_sqlfunctions">

<p>The driver provides functions that are similar to those that are available with most standard databases. These functions are implemented in the CData provider engine and thus are available across all data sources with the same consistent API. Three categories of functions are available: string, date, and math.</p>

<p>The driver interprets all SQL function inputs as either strings or column identifiers, so you need to escape all literals as strings, with single quotes. For example, contrast the SQL Server syntax and driver syntax for the <b>DATENAME</b> function: 
  <ul><li><b>SQL Server</b>: <br/><pre lang="">SELECT DATENAME(yy,GETDATE())</pre>
  </li><li><b>driver</b>: <br/><pre lang="">SELECT DATENAME('yy',GETDATE())</pre>
  </li></ul>

</p>

<p>
<h2>String Functions</h2>
</p>

<p>These functions perform string manipulations and return a string value. See <a href="#pg_sqlstringfunctions">STRING Functions</a> for more details.
</p>

<p>
<h2>Date Functions</h2>
</p>

<p>These functions perform date and date time manipulations. See <a href="#pg_sqldatefunctions">DATE Functions</a> for more details.
</p>

<p>
<h2>Math Functions</h2>
</p>

<p>These functions provide mathematical operations. See <a href="#pg_sqlmathfunctions">MATH Functions</a> for more details.
</p>

<p></p>

<p></p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka -  STRING Functions" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka -  STRING Functions: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1> STRING Functions</h1>
       

    <div class="chapter_content" id="pg_sqlstringfunctions">

<p>
<h2>ASCII(character_expression)</h2>
</p>

<p>                Returns the ASCII code value of the left-most character of the character expression.</p>

<p>                    <ul><li><b>character_expression</b>: The character expression.
                    </li></ul>
                    <br/><pre lang="">                      SELECT ASCII('0');
                      --  Result: 48
                    </pre>
                
<h2>CHAR(integer_expression)</h2>
</p>

<p>                Converts the integer ASCII code to the corresponding character.</p>

<p>                    <ul><li><b>integer_expression</b>: The integer from 0 through 255.
                    </li></ul>
                    <br/><pre lang="">                      SELECT CHAR(48);
                      -- Result: '0'
                    </pre>
                
<h2>CHARINDEX(expressionToFind ,expressionToSearch [,start_location ])</h2>
</p>

<p>                Returns the starting position of the specified expression in the character string.</p>

<p>                    <ul><li><b>expressionToFind</b>: The character expression to find.
                        </li><li><b>expressionToSearch</b>: The character expression, typically a column, to search.
                        </li><li><b>start_location</b>: The optional character position to start searching for expressionToFind in expressionToSearch.
                    </li></ul>
                    <br/><pre lang="">                      SELECT CHARINDEX('456', '0123456');
                      -- Result: 4

                      SELECT CHARINDEX('456', '0123456', 5);
                      -- Result: -1
                    </pre>
				
<h2>CHAR_LENGTH(character_expression),</h2>
</p>

<p>				Returns the number of UTF-8 characters present in the expression.</p>

<p>					<ul><li><b>character_expression</b>: The set of characters to be be evaluated for length.
					</li></ul>
					
				<br/><pre lang="">				 SELECT CHAR_LENGTH('sample text') FROM Account LIMIT 1
				 -- Result: 11			
				</pre>
                
<h2>CONCAT(string_value1, string_value2 [, string_valueN])</h2>
</p>

<p>                Returns the string that is the concatenation of two or more string values.</p>

<p>                    <ul><li><b>string_value1</b>: The first string to be concatenated.
                        </li><li><b>string_value2</b>: The second string to be concatenated.
                        </li><li><b>*</b>: The optional additional strings to be concatenated.
                    </li></ul>
                    <br/><pre lang="">                      SELECT CONCAT('Hello, ', 'world!');
                      -- Result: 'Hello, world!'
                    </pre>
                
<h2>CONTAINS(expressionToSearch, expressionToFind)</h2>
</p>

<p>                Returns 1 if expressionToFind is found within expressionToSearch; otherwise, 0.</p>

<p>                    <ul><li><b>expressionToSearch</b>: The character expression, typically a column, to search.
                        </li><li><b>expressionToFind</b>: The character expression to find.
                    </li></ul>
                    <br/><pre lang="">                      SELECT CONTAINS('0123456', '456');
                      -- Result: 1

                      SELECT CONTAINS('0123456', 'Not a number');
                      -- Result: 0
                    </pre>
                
<h2>ENDSWITH(character_expression, character_suffix)</h2>
</p>

<p>                Returns 1 if character_expression ends with character_suffix; otherwise, 0.</p>

<p>                    <ul><li><b>character_expression</b>: The character expression.
                        </li><li><b>character_suffix</b>: The character suffix to search for.
                    </li></ul>
                    <br/><pre lang="">                      SELECT ENDSWITH('0123456', '456');
                      -- Result: 1

                      SELECT ENDSWITH('0123456', '012');
                      -- Result: 0
                    </pre>
				
<h2>FILESIZE(uri)</h2>
</p>

<p>				Returns the number of bytes present in the file at the specified file path.
					<ul><li><b>uri</b>: The path of the file to read the size from.
					</li></ul>
				<br/><pre lang="">				SELECT FILESIZE('C:/Users/User1/Desktop/myfile.txt');
				-- Result: 23684
				</pre>
                
<h2>FORMAT(value [, parseFormat], format )</h2>
</p>

<p>                Returns the value formatted with the specified format.</p>

<p>                    <ul><li><b>value</b>: The string to format.
                        </li><li><b>format</b>: The string specifying the output syntax of the date or numeric format.
                        </li><li><b>parseFormat</b>: The string specifying the input syntax of the date value. Not applicable to numeric types.
                    </li></ul>
                    <br/><pre lang="">                      SELECT FORMAT(12.34, '#');
                      -- Result: 12

                      SELECT FORMAT(12.34, '#.###');
                      -- Result: 12.34

                      SELECT FORMAT(1234, '0.000E0');
                      -- Result: 1.234E3
                      
                      SELECT FORMAT('2019/01/01', 'yyyy-MM-dd');
                      -- Result: 2019-01-01
                      
                      SELECT FORMAT('20190101', 'yyyyMMdd', 'yyyy-MM-dd');
                      -- Result: '2019-01-01'
                    </pre>
                
<h2>HASHBYTES(algorithm, value)</h2>
</p>

<p>                Returns the hash of the input value as a byte array using the given algorithm. The supported algorithms are MD5, SHA1, SHA2_256, SHA2_512, SHA3_224, SHA3_256, SHA3_384, and SHA3_512.</p>

<p>                    <ul><li><b>algorithm</b>: The algorithm to use for hashing. Must be one of MD5, SHA1, SHA2_256, SHA2_512, SHA3_224, SHA3_256, SHA3_384, or SHA3_512.
                        </li><li><b>value</b>: The value to hash. Must be either a string or byte array.
                    </li></ul>
                    <br/><pre lang="">                      SELECT HASHBYTES('MD5', 'Test');
                      -- Result (byte array): 0x0CBC6611F5540BD0809A388DC95A615B
                    </pre>  
                
<h2>INDEXOF(expressionToSearch, expressionToFind [,start_location ])</h2>
</p>

<p>                Returns the starting position of the specified expression in the character string.</p>

<p>                    <ul><li><b>expressionToSearch</b>: The character expression, typically a column, to search.
                        </li><li><b>expressionToFind</b>: The character expression to find.
                        </li><li><b>start_location</b>: The optional character position to start searching for expressionToFind in expressionToSearch.
                    </li></ul>
                    <br/><pre lang="">                      SELECT INDEXOF('0123456', '456');
                      -- Result: 4

                      SELECT INDEXOF('0123456', '456', 5);
                      -- Result: -1
                    </pre>
				
                
<h2>JSON_EXTRACT(json, jsonpath)</h2>
</p>

<p>                Selects any value in a JSON array or object. The path to the array is specified in the jsonpath argument.  Return value is numeric or null.</p>

<p>                    <ul><li><b>json</b>: The JSON document to extract.
                        </li><li><b>jsonpath</b>: The XPath used to select the nodes. The JSONPath must be a string constant. The values of the nodes selected will be returned in a token-separated list.
                    </li></ul>
                    <br/><pre lang="">                      SELECT JSON_EXTRACT('{"test": {"data": 1}}', '$.test');
                      -- Result: '{"data":1}'

                      SELECT JSON_EXTRACT('{"test": {"data": 1}}', '$.test.data');
                      -- Result: 1

                      SELECT JSON_EXTRACT('{"test": {"data": [1, 2, 3]}}', '$.test.data[1]');
                      -- Result: 2
                    </pre>
				
                
<h2>LEFT ( character_expression , integer_expression )</h2>
</p>

<p>                Returns the specified number of characters counting from the left of the specified string.</p>

<p>                    <ul><li><b>character_expression</b>: The character expression.
                        </li><li><b>integer_expression</b>: The positive integer that specifies how many characters will be returned counting from the left of character_expression.
                    </li></ul>
                    <br/><pre lang="">                      SELECT LEFT('1234567890', 3);
                      -- Result: '123'
                    </pre>
                
<h2>LEN(string_expression)</h2>
</p>

<p>                Returns the number of characters of the specified string expression.</p>

<p>                    <ul><li><b>string_expression</b>: The string expression.
                    </li></ul>
                    <br/><pre lang="">                      SELECT LEN('12345');
                      -- Result: 5
                    </pre>
				
<h2>LOCATE(substring,string)</h2>
</p>

<p>				Returns an integer representing how many characters into the string the substring appears.
					<ul><li><b>substring</b>: The substring to find inside larger string.
						</li><li><b>string</b>: The larger string that will be searched for the substring.
					</li></ul>
				<br/><pre lang="">				SELECT LOCATE('sample','XXXXXsampleXXXXX');
				-- Result: 6
				</pre>
                
<h2>LOWER ( character_expression )</h2>
</p>

<p>                Returns the character expression with the uppercase character data converted to lowercase.</p>

<p>                    <ul><li><b>character_expression</b>: The character expression.
                    </li></ul>
                    <br/><pre lang="">                      SELECT LOWER('MIXED case');
                      -- Result: 'mixed case'
                    </pre>
                
<h2>LTRIM(character_expression)</h2>
</p>

<p>                Returns the character expression with leading blanks removed.</p>

<p>                    <ul><li><b>character_expression</b>: The character expression.
                    </li></ul>
                    <br/><pre lang="">                      SELECT LTRIM('     trimmed');
                      -- Result: 'trimmed'
                    </pre>
                
<h2>MASK(string_expression, mask_character [, start_index [, end_index ]])</h2>
</p>

<p>                Replaces the characters between start_index and end_index with the mask_character within the string.</p>

<p>                    <ul><li><b>string_expression</b>: The string expression to be searched.
                        </li><li><b>mask_character</b>: The character to mask with.
                        </li><li><b>start_index</b>: The optional number of characters to leave unmasked at beginning of string. Defaults to 0.
                        </li><li><b>end_index</b>: The optional number of characters to leave unmasked at end of string. Defaults to 0.
                    </li></ul>
                    <br/><pre lang="">                        SELECT MASK('1234567890','*',);
                        -- Result: '**********'
                        SELECT MASK('1234567890','*', 4);
                        -- Result: '1234******'
                        SELECT MASK('1234567890','*', 4, 2);
                        -- Result: '1234****90'  
                    </pre>
                
<h2>NCHAR(integer_expression)</h2>
</p>

<p>                Returns the Unicode character with the specified integer code as defined by the Unicode standard.</p>

<p>                    <ul><li><b>integer_expression</b>: The integer from 0 through 255. 
                    </li></ul>
				
<h2>OCTET_LENGTH(character_expression),</h2>
</p>

<p>				Returns the number of bytes present in the expression.</p>

<p>					<ul><li><b>character_expression</b>: The set of characters to be be evaluated.
					</li></ul>
					
				<br/><pre lang="">				 SELECT OCTET_LENGTH('text') FROM Account LIMIT 1
				 -- Result: 4
				</pre>
                
<h2>PATINDEX(pattern, expression)</h2>
</p>

<p>                Returns the starting position of the first occurrence of the pattern in the expression. Returns 0 if the pattern is not found.</p>

<p>                    <ul><li><b>pattern</b>: The character expression that contains the sequence to be found. The wild-card character % can be used only at the start or end of the expression.
                        </li><li><b>expression</b>: The expression, typically a column, to search for the pattern.
                    </li></ul>
                    <br/><pre lang="">                      SELECT PATINDEX('123%', '1234567890');
                      -- Result: 1

                      SELECT PATINDEX('%890', '1234567890');
                      -- Result: 8

                      SELECT PATINDEX('%456%', '1234567890');
                      -- Result: 4
                    </pre>
                
<h2>POSITION(expressionToFind IN expressionToSearch)</h2>
</p>

<p>                Returns the starting position of the specified expression in the character string.</p>

<p>                    <ul><li><b>expressionToFind</b>: The character expression to find.
                        </li><li><b>expressionToSearch</b>: The character expression, typically a column, to search.
                    </li></ul>
                    <br/><pre lang="">                      SELECT POSITION('456' IN '123456');
                      -- Result: 4

                      SELECT POSITION('x' IN '123456');
                      -- Result: 0
                    </pre>
                
<h2>QUOTENAME(character_string [, quote_character])</h2>
</p>

<p>                Returns a valid SQL Server-delimited identifier by adding the necessary delimiters to the specified Unicode string.</p>

<p>                    <ul><li><b>character_string</b>: The string of Unicode character data. The string is limited to 128 characters. Inputs greater than 128 characters return null.
                        </li><li><b>quote_character</b>: The optional single character to be used as the delimiter. Can be a single quotation mark, a left or right bracket, or a double quotation mark. If quote_character is not specified brackets are used.
                    </li></ul>
                    <br/><pre lang="">                      SELECT QUOTENAME('table_name');
                      -- Result: '[table_name]'

                      SELECT QUOTENAME('table_name', '"');
                      -- Result: '"table_name"'

                      SELECT QUOTENAME('table_name', '[');
                      -- Result: '[table_name]'
                    </pre>
                
<h2>REPLACE(string_expression, string_pattern, string_replacement)</h2>
</p>

<p>                Replaces all occurrences of a string with another string.</p>

<p>                    <ul><li><b>string_expression</b>: The string expression to be searched. Can be a character or binary data type.
                        </li><li><b>string_pattern</b>: The substring to be found. Cannot be an empty string.
                        </li><li><b>string_replacement</b>: The replacement string.
                    </li></ul>
                    <br/><pre lang="">                      SELECT REPLACE('1234567890', '456', '|');
                      -- Result: '123|7890'

                      SELECT REPLACE('123123123', '123', '.');
                      -- Result: '...'

                      SELECT REPLACE('1234567890', 'a', 'b');
                      -- Result: '1234567890'
                    </pre>
                
<h2>REPLICATE ( string_expression ,integer_expression ) </h2>
</p>

<p>                Repeats the string value the specified number of times.</p>

<p>                    <ul><li><b>string_expression</b>: The string to replicate.
                        </li><li><b>integer_expression</b>: The repeat count.
                    </li></ul>
                    <br/><pre lang="">                      SELECT REPLACE('x', 5);
                      -- Result: 'xxxxx'
                    </pre>
                
<h2>REVERSE ( string_expression )</h2>
</p>

<p>                Returns the reverse order of the string expression.</p>

<p>                    <ul><li><b>string_expression</b>: The string.
                    </li></ul>
                    <br/><pre lang="">                      SELECT REVERSE('1234567890');
                      -- Result: '0987654321'
                    </pre>
                
<h2>RIGHT ( character_expression , integer_expression )</h2>
</p>

<p>                Returns the right part of the string with the specified number of characters.</p>

<p>                    <ul><li><b>character_expression</b>: The character expression.
                        </li><li><b>integer_expression</b>: The positive integer that specifies how many characters of the character expression will be returned.
                    </li></ul>
                    <br/><pre lang="">                      SELECT RIGHT('1234567890', 3);
                      -- Result: '890'
                    </pre>
                
<h2>RTRIM(character_expression)</h2>
</p>

<p>                Returns the character expression after it removes trailing blanks.</p>

<p>                    <ul><li><b>character_expression</b>: The character expression.
                    </li></ul>
                    <br/><pre lang="">                      SELECT RTRIM('trimmed     ');
                      -- Result: 'trimmed'
                    </pre>
                
<h2>SOUNDEX(character_expression)</h2>
</p>

<p>                Returns the four-character Soundex code, based on how the string sounds when spoken.</p>

<p>                    <ul><li><b>character_expression</b>: The alphanumeric expression of character data.
                    </li></ul>
                    <br/><pre lang="">                      SELECT SOUNDEX('smith');
                      -- Result: 'S530'
                    </pre>
                
<h2>SPACE(repeatcount)</h2>
</p>

<p>                Returns the string that consists of repeated spaces.</p>

<p>                    <ul><li><b>repeatcount</b>: The number of spaces.
                    </li></ul>
                    <br/><pre lang="">                      SELECT SPACE(5);
                      -- Result: '     '
                    </pre>
                
<h2>SPLIT(string, delimiter, offset)</h2>
</p>

<p>                Returns a section of the string between to delimiters.</p>

<p>                    <ul><li><b>string</b>: The string to split.
                        </li><li><b>delimiter</b>: The character to split the string with.
                        </li><li><b>offset</b>: The number of the split to return. Positive numbers are treated as offsets from the left, and negative numbers are treated as offsets from the right.
                    </li></ul>
                    <br/><pre lang="">                      SELECT SPLIT('a/b/c/d', '/', 1);
                      -- Result: 'a'
                      SELECT SPLIT('a/b/c/d', '/', -2);
                      -- Result: 'c'
                    </pre>
                
<h2>STARTSWITH(character_expression, character_prefix)</h2>
</p>

<p>                Returns 1 if character_expression starts with character_prefix; otherwise, 0.</p>

<p>                    <ul><li><b>character_expression</b>: The character expression.
                        </li><li><b>character_prefix</b>: The character prefix to search for.
                    </li></ul>
                    <br/><pre lang="">                      SELECT STARTSWITH('0123456', '012');
                      -- Result: 1

                      SELECT STARTSWITH('0123456', '456');
                      -- Result: 0
                    </pre>
                
<h2>STR ( float_expression [ , integer_length [ , integer_decimal ] ] )</h2>
</p>

<p>                Returns the character data converted from the numeric data. For example, STR(123.45, 6, 1) returns 123.5.</p>

<p>                    <ul><li><b>float_expression</b>: The float expression.
                        </li><li><b>length</b>: The optional total length to return. This includes decimal point, sign, digits, and spaces. The default is 10.
                        </li><li><b>decimal</b>: The optional number of places to the right of the decimal point. The decimal must be less than or equal to 16.
                    </li></ul>
                    <br/><pre lang="">                      SELECT STR('123.456');
                      -- Result: '123'

                      SELECT STR('123.456', 2);
                      -- Result: '**'

                      SELECT STR('123.456', 10, 2);
                      -- Result: '123.46'
                    </pre>
                
<h2>STUFF(character_expression , integer_start , integer_length , replaceWith_expression)</h2>
</p>

<p>                Inserts a string into another string. It deletes the specified length of characters in the first string at the start position and then inserts the second string into the first string at the start position.</p>

<p>                    <ul><li><b>character_expression</b>: The string expression.
                        </li><li><b>start</b>: The integer value that specifies the location to start deletion and insertion. If start or length is negative, null is returned. If start is longer than the string to be modified, character_expression, null is returned.
                        </li><li><b>length</b>: The integer that specifies the number of characters to delete. If length is longer than character_expression, deletion occurs up to the last character in replaceWith_expression.
                        </li><li><b>replaceWith_expression</b>: The expression of character data that will replace length characters of character_expression beginning at the start value.
                    </li></ul>
                    <br/><pre lang="">                      SELECT STUFF('1234567890', 3, 2, 'xx');
                      -- Result: '12xx567890'
                    </pre>
                
<h2>SUBSTRING(string_value FROM start FOR length)</h2>
</p>

<p>                Returns the part of the string with the specified length; starts at the specified index.</p>

<p>                    <ul><li><b>string_value</b>: The character string.
                        </li><li><b>start</b>: The positive integer that specifies the start index of characters to return.
                        </li><li><b>length</b>: Optional. The positive integer that specifies how many characters will be returned.
                    </li></ul>
                    <br/><pre lang="">                      SELECT SUBSTRING('1234567890' FROM 3 FOR 2);
                      -- Result: '34'

                      SELECT SUBSTRING('1234567890' FROM 3);
                      -- Result: '34567890'
                    </pre>
                
<h2>TOSTRING(string_value1)</h2>
</p>

<p>                Converts the value of this instance to its equivalent string representation.</p>

<p>                    <ul><li><b>string_value1</b>: The string to be converted.
                    </li></ul>
                    <br/><pre lang="">                      SELECT TOSTRING(123);
                      -- Result: '123'

                      SELECT TOSTRING(123.456);
                      -- Result: '123.456'

                      SELECT TOSTRING(null);
                      -- Result: ''
                    </pre>
                
<h2>TRIM(trimspec trimchar FROM string_value)</h2>
</p>

<p>                Returns the character expression with leading and/or trailing blanks removed.</p>

<p>                    <ul><li><b>trimspec</b>: Optional. If included must be one of the keywords <b>BOTH</b>, <b>LEADING</b> or <b>TRAILING</b>.
                        </li><li><b>trimchar</b>: Optional. If included should be a one-character string value.
                        </li><li><b>string_value</b>: The string value to trim.
                    </li></ul>
                    <br/><pre lang="">                      SELECT TRIM('     trimmed     ');
                      -- Result: 'trimmed'

                      SELECT TRIM(LEADING FROM '     trimmed     ');
                      -- Result: 'trimmed     '

                      SELECT TRIM('-' FROM '-----trimmed-----');
                      -- Result: 'trimmed'

                      SELECT TRIM(BOTH '-' FROM '-----trimmed-----');
                      -- Result: 'trimmed'

                      SELECT TRIM(TRAILING '-' FROM '-----trimmed-----');
                      -- Result: '-----trimmed'
                    </pre>
                
<h2>UNICODE(ncharacter_expression)</h2>
</p>

<p>                Returns the integer value defined by the Unicode standard of the first character of the input expression.</p>

<p>                    <ul><li><b>ncharacter_expression</b>: The Unicode character expression.
                    </li></ul>
                
<h2>UPPER ( character_expression )</h2>
</p>

<p>                Returns the character expression with lowercase character data converted to uppercase.</p>

<p>                    <ul><li><b>character_expression</b>: The character expression.
                    </li></ul>
                    <br/><pre lang="">                      SELECT UPPER('MIXED case');
                      -- Result: 'MIXED CASE'
                    </pre>
                
<h2>XML_EXTRACT(xml, xpath [, separator])</h2>
</p>

<p>                Extracts an XML document using the specified XPath to flatten the XML. A comma is used to separate the outputs by default, but this can be changed by specifying the third parameter.</p>

<p>                    <ul><li><b>xml</b>: The XML document to extract.
                        </li><li><b>xpath</b>: The XPath used to select the nodes. The nodes selected will be returned in a token-separated list.
                        </li><li><b>separator</b>: The optional token used to separate the items in the flattened response. If this is not specified, the separator will be a comma.
                    </li></ul>
                    <br/><pre lang="">                      SELECT XML_EXTRACT('&lt;vowels&gt;&lt;ch&gt;a&lt;/ch&gt;&lt;ch&gt;e&lt;/ch&gt;&lt;ch&gt;i&lt;/ch&gt;&lt;ch&gt;o&lt;/ch&gt;&lt;ch&gt;u&lt;/ch&gt;&lt;/vowels&gt;', '/vowels/ch');
                      -- Result: 'a,e,i,o,u'

                      SELECT XML_EXTRACT('&lt;vowels&gt;&lt;ch&gt;a&lt;/ch&gt;&lt;ch&gt;e&lt;/ch&gt;&lt;ch&gt;i&lt;/ch&gt;&lt;ch&gt;o&lt;/ch&gt;&lt;ch&gt;u&lt;/ch&gt;&lt;/vowels&gt;', '/vowels/ch', ';');
                      -- Result: 'a;e;i;o;u'
                    </pre>
</p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka -  MATH Functions" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka -  MATH Functions: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1> MATH Functions</h1>
       

    <div class="chapter_content" id="pg_sqlmathfunctions">

<p>
<h2>ABS ( numeric_expression )</h2>
</p>

<p>                Returns the absolute (positive) value of the specified numeric expression.</p>

<p>                    <ul><li><b>numeric_expression</b>: The expression of an indeterminate numeric data type except for the bit data type.
                    </li></ul>
                    <br/><pre lang="">                      SELECT ABS(15);
                      -- Result: 15

                      SELECT ABS(-15);
                      -- Result: 15
                    </pre>
                
<h2>ACOS ( float_expression )</h2>
</p>

<p>                Returns the arc cosine, the angle in radians whose cosine is the specified float expression.</p>

<p>                    <ul><li><b>float_expression</b>: The float expression that specifies the cosine of the angle to be returned. Values outside the range from -1 to 1 return null.
                    </li></ul>
                    <br/><pre lang="">                      SELECT ACOS(0.5);
                      -- Result: 1.0471975511966
                    </pre>
                
<h2>ASIN ( float_expression )</h2>
</p>

<p>                Returns the arc sine, the angle in radians whose sine is the specified float expression.</p>

<p>                    <ul><li><b>float_expression</b>: The float expression that specifies the sine of the angle to be returned. Values outside the range from -1 to 1 return null.
                    </li></ul>
                    <br/><pre lang="">                      SELECT ASIN(0.5);
                      -- Result: 0.523598775598299
                    </pre>
                
<h2>ATAN ( float_expression )</h2>
</p>

<p>                Returns the arc tangent, the angle in radians whose tangent is the specified float expression.</p>

<p>                    <ul><li><b>float_expression</b>: The float expression that specifies the tangent of the angle to be returned.
                    </li></ul>
                    <br/><pre lang="">                      SELECT ATAN(10);
                      -- Result: 1.47112767430373
                    </pre>
                
<h2>ATN2 ( float_expression1 , float_expression2 )</h2>
</p>

<p>                Returns the angle in radians between the positive x-axis and the ray from the origin to the point (y, x) where x and y are the values of the two specified float expressions.</p>

<p>                    <ul><li><b>float_expression1</b>: The float expression that is the y-coordinate.
                        </li><li><b>float_expression2</b>: The float expression that is the x-coordinate.
                    </li></ul>
                    <br/><pre lang="">                      SELECT ATN2(1, 1);
                      -- Result: 0.785398163397448
                    </pre>
                
<h2>CEILING ( numeric_expression ) or CEIL( numeric_expression )</h2>
</p>

<p>                Returns the smallest integer greater than or equal to the specified numeric expression.</p>

<p>                    <ul><li><b>numeric_expression</b>: The expression of an indeterminate numeric data type except for the bit data type.
                    </li></ul>
                    <br/><pre lang="">                      SELECT CEILING(1.3);
                      -- Result: 2

                      SELECT CEILING(1.5);
                      -- Result: 2

                      SELECT CEILING(1.7);
                      -- Result: 2
                    </pre>
                
<h2>COS ( float_expression )</h2>
</p>

<p>                Returns the trigonometric cosine of the specified angle in radians in the specified expression.</p>

<p>                    <ul><li><b>float_expression</b>: The float expression of the specified angle in radians.
                    </li></ul>
                    <br/><pre lang="">                      SELECT COS(1);
                      -- Result: 0.54030230586814
                    </pre>
                
<h2>COT ( float_expression )</h2>
</p>

<p>                Returns the trigonometric cotangent of the angle in radians specified by float_expression.</p>

<p>                    <ul><li><b>float_expression</b>: The float expression of the angle in radians.
                    </li></ul>
                    <br/><pre lang="">                      SELECT COT(1);
                      -- Result: 0.642092615934331
                    </pre>
                
<h2>DEGREES ( numeric_expression )</h2>
</p>

<p>                Returns the angle in degrees for the angle specified in radians.</p>

<p>                    <ul><li><b>numeric_expression</b>: The angle in radians, an expression of an indeterminate numeric data type except for the bit data type.
                    </li></ul>
                    <br/><pre lang="">                      SELECT DEGREES(3.1415926);
                      -- Result: 179.999996929531
                    </pre>
                
<h2>EXP ( float_expression )</h2>
</p>

<p>                Returns the exponential value of the specified float expression. For example, EXP(LOG(20)) is 20.</p>

<p>                    <ul><li><b>float_expression</b>: The float expression.
                    </li></ul>
                    <br/><pre lang="">                      SELECT EXP(2);
                      -- Result: 7.38905609893065
                    </pre>
                
<h2>EXPR ( expression )</h2>
</p>

<p>                Evaluates the expression.</p>

<p>                    <ul><li><b>expression</b>: The expression. Operators allowed are +, -, *, /, ==, !=, &gt;, &lt;, &gt;=, and &lt;=.
                    </li></ul>
                    <br/><pre lang="">                      SELECT EXPR('1 + 2 * 3');
                      -- Result: 7

                      SELECT EXPR('1 + 2 * 3 == 7');
                      -- Result: true
                    </pre>
                
<h2>FLOOR ( numeric_expression )</h2>
</p>

<p>                Returns the largest integer less than or equal to the numeric expression.</p>

<p>                    <ul><li><b>numeric_expression</b>: The expression of an indeterminate numeric data type except for the bit data type.
                    </li></ul>
                    <br/><pre lang="">                      SELECT FLOOR(1.3);
                      -- Result: 1

                      SELECT FLOOR(1.5);
                      -- Result: 1

                      SELECT FLOOR(1.7);
                      -- Result: 1
                    </pre>
				
<h2>GREATEST(int1,int2,....)</h2>
</p>

<p>				Returns the greatest of the supplied integers.
				<br/><pre lang="">				SELECT GREATEST(3,5,8,10,1)
				-- Result: 10			
				</pre>
				
<h2>HEX(value)</h2>
</p>

<p>				Returns a the equivalent hex for the input value.
					<ul><li><b>value</b>: A string or numerical value to be converted into hex.
					</li></ul>
				<br/><pre lang="">				SELECT HEX(866849198);
				-- Result: 33AB11AE
				
				SELECT HEX('Sample Text');
				-- Result: 53616D706C652054657874
				</pre>
								
                
<h2>JSON_AVG(json, jsonpath)</h2>
</p>

<p>                Computes the average value of a JSON array within a JSON object. The path to the array is specified in the jsonpath argument. Return value is numeric or null.</p>

<p>                    <ul><li><b>json</b>: The JSON document to compute.
                        </li><li><b>jsonpath</b>: The JSONPath used to select the nodes. [x], [2..], [..8], or [1..12] are accepted. [x] selects all nodes.
                    </li></ul>
                    <br/><pre lang="">                      SELECT JSON_AVG('[1,2,3,4,5]', '$[x]');
                      -- Result: 3

                      SELECT JSON_AVG('{"test": {"data": [1,2,3,4,5]}}', '$.test.data[x]');
                      -- Result: 3

                      SELECT JSON_AVG('{"test": {"data": [1,2,3,4,5]}}', '$.test.data[3..]');
                      -- Result: 4.5
                    </pre>
                
<h2>JSON_COUNT(json, jsonpath)</h2>
</p>

<p>                Returns the number of elements in a JSON array within a JSON object. The path to the array is specified in the jsonpath argument. Return value is numeric or null.</p>

<p>                    <ul><li><b>json</b>: The JSON document to compute.
                        </li><li><b>jsonpath</b>: The JSONPath used to select the nodes. [x], [2..], [..8], or [1..12] are accepted. [x] selects all nodes.
                    </li></ul>
                    <br/><pre lang="">                      SELECT JSON_COUNT('[1,2,3,4,5]', '$[x]');
                      -- Result: 5

                      SELECT JSON_COUNT('{"test": {"data": [1,2,3,4,5]}}', '$.test.data[x]');
                      -- Result: 5

                      SELECT JSON_COUNT('{"test": {"data": [1,2,3,4,5]}}', '$.test.data[3..]');
                      -- Result: 2
                    </pre>
                
<h2>JSON_MAX(json, jsonpath)</h2>
</p>

<p>                Gets the maximum value in a JSON array within a JSON object. The path to the array is specified in the jsonpath argument. Return value is numeric or null.</p>

<p>                    <ul><li><b>json</b>: The JSON document to compute.
                        </li><li><b>jsonpath</b>: The JSONPath used to select the nodes. [x], [2..], [..8], or [1..12] are accepted. [x] selects all nodes.
                    </li></ul>
                    <br/><pre lang="">                      SELECT JSON_MAX('[1,2,3,4,5]', '$[x]');
                      -- Result: 5

                      SELECT JSON_MAX('{"test": {"data": [1,2,3,4,5]}}', '$.test.data[x]');
                      -- Result: 5

                      SELECT JSON_MAX('{"test": {"data": [1,2,3,4,5]}}', '$.test.data[..3]');
                      -- Result: 4
                    </pre>
                
<h2>JSON_MIN(json, jsonpath)</h2>
</p>

<p>                Gets the minimum value in a JSON array within a JSON object. The path to the array is specified in the jsonpath argument. Return value is numeric or null.</p>

<p>                    <ul><li><b>json</b>: The JSON document to compute.
                        </li><li><b>jsonpath</b>: The JSONPath used to select the nodes. [x], [2..], [..8], or [1..12] are accepted. [x] selects all nodes.
                    </li></ul>
                    <br/><pre lang="">                      SELECT JSON_MIN('[1,2,3,4,5]', '$[x]');
                      -- Result: 1

                      SELECT JSON_MIN('{"test": {"data": [1,2,3,4,5]}}', '$.test.data[x]');
                      -- Result: 1

                      SELECT JSON_MIN('{"test": {"data": [1,2,3,4,5]}}', '$.test.data[3..]');
                      -- Result: 4
                    </pre>
                
<h2>JSON_SUM(json, jsonpath)</h2>
</p>

<p>                Computes the summary value in JSON according to the JSONPath expression. Return value is numeric or null.</p>

<p>                    <ul><li><b>json</b>: The JSON document to compute.
                        </li><li><b>jsonpath</b>: The JSONPath used to select the nodes. [x], [2..], [..8], or [1..12] are accepted. [x] selects all nodes.
                    </li></ul>
                    <br/><pre lang="">                      SELECT JSON_SUM('[1,2,3,4,5]', '$[x]');
                      -- Result: 15

                      SELECT JSON_SUM('{"test": {"data": [1,2,3,4,5]}}', '$.test.data[x]');
                      -- Result: 15

                      SELECT JSON_SUM('{"test": {"data": [1,2,3,4,5]}}', '$.test.data[3..]');
                      -- Result: 9
                    </pre>
				
				
<h2>LEAST(int1,int2,....)</h2>
</p>

<p>				Returns the least of the supplied integers.
				<br/><pre lang="">				SELECT LEAST(3,5,8,10,1)
				-- Result: 1			
				</pre>
                
<h2>LOG ( float_expression [, base ] )</h2>
</p>

<p>                Returns the natural logarithm of the specified float expression.</p>

<p>                    <ul><li><b>float_expression</b>: The float expression.
                        </li><li><b>base</b>: The optional integer argument that sets the base for the logarithm.
                    </li></ul>
                    <br/><pre lang="">                      SELECT LOG(7.3890560);
                      -- Result: 1.99999998661119
                    </pre>
                
<h2>LOG10 ( float_expression )</h2>
</p>

<p>                Returns the base-10 logarithm of the specified float expression.</p>

<p>                    <ul><li><b>float_expression</b>: The expression of type float.
                    </li></ul>
                    <br/><pre lang="">                      SELECT LOG10(10000);
                      -- Result: 4
                    </pre>
				
<h2>MOD(dividend,divisor)</h2>
</p>

<p>				Returns the integer value associated with the remainder when dividing the dividend by the divisor.
					<ul><li><b>dividend</b>: The number to take the modulus of.
						</li><li><b>divisor</b>: The number to divide the dividend by when determining the modulus.
					</li></ul>
				<br/><pre lang="">				SELECT MOD(10,3);
				-- Result: 1
				</pre>
				
<h2>NEGATE(real_number)</h2>
</p>

<p>				Returns the opposite to the real number input.
					<ul><li><b>real_number</b>: The real number to find the opposite of.
					</li></ul>
				<br/><pre lang="">				SELECT NEGATE(10);
				-- Result: -10
				
				SELECT NEGATE(-12.4)
				--Result: 12.4
				</pre>
                
<h2>PI ( )</h2>
</p>

<p>                Returns the constant value of pi.
                <br/><pre lang="">                  SELECT PI()
                  -- Result: 3.14159265358979 
                </pre>
                
<h2>POWER ( float_expression , y )</h2>
</p>

<p>                Returns the value of the specified expression raised to the specified power.</p>

<p>                    <ul><li><b>float_expression</b>: The float expression.
                        </li><li><b>y</b>: The power to raise float_expression to.
                    </li></ul>
                    <br/><pre lang="">                      SELECT POWER(2, 10);
                      -- Result: 1024

                      SELECT POWER(2, -2);
                      -- Result: 0.25
                    </pre>
                
<h2>RADIANS ( float_expression )</h2>
</p>

<p>                Returns the angle in radians of the angle in degrees.</p>

<p>                    <ul><li><b>float_expression</b>: The degrees of the angle as a float expression.
                    </li></ul>
                    <br/><pre lang="">                      SELECT RADIANS(180);
                      -- Result: 3.14159265358979
                    </pre>
                
<h2>RAND ( [ integer_seed ] )</h2>
</p>

<p>                Returns a pseudorandom float value from 0 through 1, exclusive.</p>

<p>                    <ul><li><b>seed</b>: The optional integer expression that specifies the seed value. If seed is not specified, a seed value at random will be assigned.
                    </li></ul>
                    <br/><pre lang="">                      SELECT RAND();
                      -- This result may be different, since the seed is randomized
                      -- Result: 0.873159630165044

                      SELECT RAND(1);
                      -- This result will always be the same, since the seed is constant
                      -- Result: 0.248668584157093
                    </pre>
                
<h2>ROUND ( numeric_expression [ ,integer_length] [ ,function ] )</h2>
</p>

<p>                Returns the numeric value rounded to the specified length or precision.</p>

<p>                    <ul><li><b>numeric_expression</b>: The expression of a numeric data type.
                        </li><li><b>length</b>: The optional precision to round the numeric expression to.  When this is ommitted, the default behavior will be to round to the nearest whole number.
                        </li><li><b>function</b>: The optional type of operation to perform. When the function parameter is omitted or has a value of 0 (default), numeric_expression is rounded. When a value other than 0 is specified, numeric_expression is truncated.
                    </li></ul>
                    <br/><pre lang="">                      SELECT ROUND(1.3, 0);
                      -- Result: 1

                      SELECT ROUND(1.55, 1);
                      -- Result: 1.6

                      SELECT ROUND(1.7, 0, 0);
                      -- Result: 2

                      SELECT ROUND(1.7, 0, 1);
                      -- Result: 1
                      
                      SELECT ROUND (1.24);
                      -- Result: 1.0
                    </pre>
                
<h2>SIGN ( numeric_expression )</h2>
</p>

<p>                Returns the positive sign (1), 0, or negative sign (-1) of the specified expression.</p>

<p>                    <ul><li><b>numeric_expression</b>: The expression of an indeterminate data type except for the bit data type.
                    </li></ul>
                    <br/><pre lang="">                      SELECT SIGN(0);
                      -- Result: 0

                      SELECT SIGN(10);
                      -- Result: 1

                      SELECT SIGN(-10);
                      -- Result: -1
                    </pre>
                
<h2>SIN ( float_expression )</h2>
</p>

<p>                Returns the trigonometric sine of the angle in radians.</p>

<p>                    <ul><li><b>float_expression</b>: The float expression specifying the angle in radians.
                    </li></ul>
                    <br/><pre lang="">                     SELECT SIN(1);
                     -- Result: 0.841470984807897
                    </pre>
                
<h2>SQRT ( float_expression )</h2>
</p>

<p>                Returns the square root of the specified float value.</p>

<p>                    <ul><li><b>float_expression</b>: The expression of type float.
                    </li></ul>
                    <br/><pre lang="">                      SELECT SQRT(100);
                      -- Result: 10
                    </pre>
                
<h2>SQUARE ( float_expression )</h2>
</p>

<p>                Returns the square of the specified float value.</p>

<p>                    <ul><li><b>float_expression</b>: The expression of type float.
                    </li></ul>
                    <br/><pre lang="">                      SELECT SQUARE(10);
                      -- Result: 100

                      SELECT SQUARE(-10);
                      -- Result: 100
                    </pre>
                
<h2>TAN ( float_expression )</h2>
</p>

<p>                Returns the tangent of the input expression.</p>

<p>                    <ul><li><b>float_expression</b>: The expression of type float.
                    </li></ul>
                    <br/><pre lang="">                      SELECT TAN(1);
                      -- Result: 1.5574077246549
                    </pre>
				
<h2>TRUNC(decimal_number,precision)</h2>
</p>

<p>				Returns the supplied decimal number truncated to have the supplied decimal precision.
					<ul><li><b>decimal_number</b>: The decimal value to truncate.
						</li><li><b>precision</b>: The number of decimal places to truncate the decimal number to.
					</li></ul>
				<br/><pre lang="">				SELECT TRUNC(10.3423,2);
				-- Result: 10.34
				</pre>
				
<h2>_ROW_NUMBER_()</h2>
</p>

<p>				Returns a row index as an additional column.
				<br/><pre lang="">				SELECT ColumnName, _ROW_NUMBER_() FROM TableName
				-- Result: ColumnData, 0
				ColumnData2, 1
				ColumnData3, 2
				</pre></p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka -  DATE Functions" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka -  DATE Functions: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1> DATE Functions</h1>
       

    <div class="chapter_content" id="pg_sqldatefunctions">

<p>
<h2>CURRENT_DATE()</h2>
</p>

<p>                Returns the current date value.
                <br/><pre lang="">                  SELECT CURRENT_DATE();
                  -- Result: 2018-02-01
                </pre>

                
<h2>CURRENT_TIMESTAMP()</h2>
</p>

<p>                Returns the current time stamp of the database system as a datetime value. This value is equal to GETDATE and SYSDATETIME, and is always in the local timezone.
                <br/><pre lang="">                  SELECT CURRENT_TIMESTAMP();
                  -- Result: 2018-02-01 03:04:05
                </pre>
                
<h2>DATEADD (datepart , integer_number , date [, dateformat])</h2>
</p>

<p>                Returns the datetime value that results from adding the specified number (a signed integer) to the specified date part of the date.</p>

<p>                    <ul><li><b>datepart</b>: The part of the date to add the specified number to. The valid values and abbreviations are year (yy, yyyy), quarter (qq, q), month (mm, m), dayofyear (dy, y), day (dd, d), week (wk, ww), weekday (dw), hour (hh), minute (mi, n), second (ss, s), and millisecond (ms).
                        </li><li><b>number</b>: The number to be added.
                        </li><li><b>date</b>: The expression of the datetime data type.
                        </li><li><b>dateformat</b>: The optional output date format.
                    </li></ul>
                <br/><pre lang="">                  SELECT DATEADD('d', 5, '2018-02-01');
                  -- Result: 2018-02-06

                  SELECT DATEADD('hh', 5, '2018-02-01 00:00:00');
                  -- Result: 2018-02-01 05:00:00
                </pre>
                
<h2>DATEDIFF ( datepart , startdate , enddate )</h2>
</p>

<p>                Returns the difference (a signed integer) of the specified time interval between the specified start date and end date.</p>

<p>                    <ul><li><b>datepart</b>: The part of the date that is the time interval of the difference between the start date and end date. The valid values and abbreviations are day (dd, d), hour (hh), minute (mi, n), second (ss, s), and millisecond (ms).
                        </li><li><b>startdate</b>: The datetime expression of the start date.
                        </li><li><b>enddate</b>: The datetime expression of the end date.
                    </li></ul>
                <br/><pre lang="">                  SELECT DATEDIFF('d', '2018-02-01', '2018-02-10');
                  -- Result: 9

                  SELECT DATEDIFF('hh', '2018-02-01 00:00:00', '2018-02-01 12:00:00');
                  -- Result: 12
                </pre>
				
<h2>DATE_FORMAT(date,format)</h2>
</p>

<p>				Returns the date or timestamp in the format specified. This function mirrors the <a href="https://dev.mysql.com/doc/refman/8.0/en/date-and-time-functions.html#function_date-format">MySQL DATE_FORMAT function</a>.</p>

<p>				    <ul><li><b>date</b>: A date or timestamp string.
						</li><li><b>format</b>: The specifier string of the desired output format. The list of supported format specifiers comes from the MySQL DATE_FORMAT function (see link to MySQL documentation above).
					</li></ul>
				  <br/><pre lang="">					SELECT DATE_FORMAT('9/4/2021 3:11:53 AM','%h')
					-- Result: 03
				  </pre>
                
<h2>DATEFROMPARTS(integer_year, integer_month, integer_day)</h2>
</p>

<p>                Returns the datetime value for the specified year, month, and day.</p>

<p>                    <ul><li><b>year</b>: The integer expression specifying the year.
                        </li><li><b>month</b>: The integer expression specifying the month.
                        </li><li><b>day</b>: The integer expression specifying the day.
                    </li></ul>
                  <br/><pre lang="">                    SELECT DATEFROMPARTS(2018, 2, 1);
                    -- Result: 2018-02-01
                  </pre>
                
<h2>DATENAME(datepart , date)</h2>
</p>

<p>                Returns the character string that represents the specified date part of the specified date.</p>

<p>                    <ul><li><b>datepart</b>: The part of the date to return. The valid values and abbreviations are year (yy, yyyy), quarter (qq, q), month (mm, m), dayofyear (dy, y), day (dd, d), week (wk, ww), weekday (dw), hour (hh), minute (mi, n), second (ss, s), millisecond (ms), microsecond (mcs), nanosecond (ns), and TZoffset (tz).
                        </li><li><b>date</b>: The datetime expression.
                    </li></ul>
                   <br/><pre lang="">                     SELECT DATENAME('yy', '2018-02-01');
                     -- Result: '2018'

                     SELECT DATENAME('dw', '2018-02-01');
                     -- Result: 'Thursday'
                   </pre>
                
<h2>DATEPART(datepart, date [,integer_datefirst])</h2>
</p>

<p>                Returns a character string that represents the specified date part of the specified date.</p>

<p>                    <ul><li><b>datepart</b>: The part of the date to return. The valid values and abbreviations are year (yy, yyyy), quarter (qq, q), month (mm, m), dayofyear (dy, y), day (dd, d), week (wk, ww), weekday (dw), hour (hh), minute (mi, n), second (ss, s), millisecond (ms), microsecond (mcs), nanosecond (ns), TZoffset (tz), ISODOW, ISO_WEEK (isoweek, isowk,isoww), and ISOYEAR.
                        </li><li><b>date</b>: The datetime string.
                        </li><li><b>datefirst</b>: The optional integer representing the first day of the week. The default is 7, Sunday.
                    </li></ul>
                  <br/><pre lang="">                    SELECT DATEPART('yy', '2018-02-01');
                    -- Result: 2018

                    SELECT DATEPART('dw', '2018-02-01');
                    -- Result: 5
                  </pre>
                
<h2>DATETIMEFROMPARTS(integer_year, integer_month, integer_day, integer_hour, integer_minute, integer_seconds, integer_milliseconds)</h2>
</p>

<p>                Returns the datetime value for the specified date parts.</p>

<p>                    <ul><li><b>year</b>: The integer expression specifying the year.
                        </li><li><b>month</b>: The integer expression specifying the month.
                        </li><li><b>day</b>: The integer expression specifying the day.
                        </li><li><b>hour</b>: The integer expression specifying the hour.
                        </li><li><b>minute</b>: The integer expression specifying the minute.
                        </li><li><b>seconds</b>: The integer expression specifying the seconds.
                        </li><li><b>milliseconds</b>: The integer expression specifying the milliseconds.
                    </li></ul>
                  <br/><pre lang="">                    SELECT DATETIMEFROMPARTS(2018, 2, 1, 1, 2, 3, 456);
                    -- Result: 2018-02-01 01:02:03.456
                  </pre>
                
<h2>DATETIME2FROMPARTS(integer_year, integer_month, integer_day, integer_hour, integer_minute, integer_seconds, integer_fractions, integer_precision)</h2>
</p>

<p>                Returns the datetime value for the specified date parts.</p>

<p>                    <ul><li><b>year</b>: The integer expression specifying the year.
                        </li><li><b>month</b>: The integer expression specifying the month.
                        </li><li><b>day</b>: The integer expression specifying the day.
                        </li><li><b>hour</b>: The integer expression specifying the hour.
                        </li><li><b>minute</b>: The integer expression specifying the minute.
                        </li><li><b>seconds</b>: The integer expression specifying the seconds.
                        </li><li><b>fractions</b>: The integer expression specifying the fractions of the second.
                        </li><li><b>precision</b>: The integer expression specifying the precision of the fraction.
                    </li></ul>
                  <br/><pre lang="">				    SELECT DATETIME2FROMPARTS(2018, 2, 1, 1, 2, 3, 456, 3);
                    -- Result: 2018-02-01 01:02:03.456
                  </pre>
				
<h2>DATE_TRUNC(date, datepart)</h2>
</p>

<p>                Truncates the date to the precision of the given date part. Modeled after the Oracle TRUNC function.
                    <ul><li><b>date</b>: The datetime string that specifies the date.
                        </li><li><b>datepart</b>: Refer to the  <a href="https://docs.oracle.com/cd/B19306_01/server.102/b14200/functions230.htm#i1002084">Oracle documentation</a> for valid datepart syntax.    
                    </li></ul>
                  <br/><pre lang="">				    SELECT DATE_TRUNC('05-04-2005', 'YY');
                    -- Result: '1/1/2005'
					
                    SELECT DATE_TRUNC('05-04-2005', 'MM');
                    -- Result: '5/1/2005'                    
                  </pre>
                
<h2>DATE_TRUNC2(datepart, date, [weekday])</h2>
</p>

<p>                Truncates the date to the precision of the given date part. Modeled after the PostgreSQL date_trunc function.
                    <ul><li><b>datepart</b>: One of 'millennium', 'century', 'decade', 'year', 'quarter', 'month', 'week', 'day', 'hour', 'minute' or 'second'.
                        </li><li><b>date</b>: The datetime string that specifies the date.
                        </li><li><b>weekday</b>: The optional day of the week to use as the first day for 'week'. One of 'sunday', 'monday', etc.
                    </li></ul>
                  <br/><pre lang="">                    SELECT DATE_TRUNC2('year', '2020-02-04');
                    -- Result: '2020-01-01'

                    SELECT DATE_TRUNC2('week', '2020-02-04', 'monday');
                    -- Result: '2020-02-02', which is the previous Monday
                  </pre>
                
<h2>DAY(date)</h2>
</p>

<p>                Returns the integer that specifies the day component of the specified date.</p>

<p>                    <ul><li><b>date</b>: The datetime string that specifies the date.
                    </li></ul>
                  <br/><pre lang="">                    SELECT DAY('2018-02-01');
                    -- Result: 1
                  </pre>
				
<h2>DAYNAME(date)</h2>
</p>

<p>                Returns the name of the day of the week of the specified date.</p>

<p>                    <ul><li><b>date</b>: The datetime string that specifies the date.
                    </li></ul>
                  <br/><pre lang="">                    SELECT DAYNAME('8/18/2021');
                    -- Result: Wednesday
                  </pre>
				
<h2>DAYOFMONTH(date)</h2>

				Returns the day of the month of the given date part.
					<ul><li><b>date</b>: The datetime string that specifies the date.
					</li></ul>
				  <br/><pre lang="">				  SELECT DAYOFMONTH('04/15/2000');
				  -- Result: 15
				  </pre>
				
<h2>DAYOFWEEK(date)</h2>

				Returns the day of the week of the given date part.
					<ul><li><b>date</b>: The datetime string that specifies the date.
					</li></ul>
				  <br/><pre lang="">				  SELECT DAYOFWEEK('04/15/2000');
				  -- Result: 7
				  </pre>
				
<h2>DAYOFYEAR(date)</h2>

				Returns the day of the year of the given date part.
					<ul><li><b>date</b>: The datetime string that specifies the date.
					</li></ul>
				  <br/><pre lang="">				  SELECT DAYOFYEAR('04/15/2000');
				  -- Result: 106
				  </pre>
                
<h2>EOMONTH(date [, integer_month_to_add ]) or LAST_DAY(date)</h2>
</p>

<p>                Returns the last day of the month that contains the specified date with an optional offset.</p>

<p>                    <ul><li><b>date</b>: The datetime expression specifying the date for which to return the last day of the month.
                        </li><li><b>integer_month_to_add</b>: The optional integer expression specifying the number of months to add to the date before calculating the end of the month.
                    </li></ul>
                <br/><pre lang="">                  SELECT EOMONTH('2018-02-01');
                  -- Result: 2018-02-28
                  
                  SELECT LAST_DAY('2018-02-01');
                  -- Result: 2018-02-28

                  SELECT EOMONTH('2018-02-01', 2);
                  -- Result: 2018-04-30
                </pre>
				
<h2>EXTRACT(date_part FROM date_column_name)</h2>
</p>

<p>                Returns the last day of the month that contains the specified date with an optional offset.</p>

<p>                    <ul><li><b>date_part</b>: One of the following date components: YEAR, MONTH, DAY, HOUR, MINUTE, SECOND.
						</li><li><b>date_column_name</b>: The name of a date column in a table.
                    </li></ul>
                <br/><pre lang="">                  SELECT EXTRACT(YEAR FROM DateColumn)
                  -- Result: 2021
                </pre>
				
<h2>FDWEEK(date)</h2>

				Returns the first day of the week of the given date part.
					<ul><li><b>date</b>: The datetime string that specifies the date.
					</li></ul>
				  <br/><pre lang="">				  SELECT FDWEEK('02-08-2018');
				  -- Result: 2/4/2018
				  </pre>
				
<h2>FDMONTH(date)</h2>

				Returns the first day of the month of the given date part.
					<ul><li><b>date</b>: The datetime string that specifies the date.
					</li></ul>
				  <br/><pre lang="">				  SELECT FDMONTH('02-08-2018');
				  -- Result: 2/1/2018
				  </pre>
				
<h2>FDQUARTER(date)</h2>

				Returns the first day of the quarter of the given date part.
					<ul><li><b>date</b>: The datetime string that specifies the date.
					</li></ul>
				  <br/><pre lang="">				  SELECT FDQUARTER('05-08-2018');
				  -- Result: 4/1/2018
				  </pre>
				
<h2>FILEMODIFIEDTIME(uri)</h2>
</p>

<p>				Returns the time stamp associated with the Date Modified of the relevant file.
					<ul><li><b>uri</b>: An absolute path pointing to a file on the local file system.
					</li></ul>
			     <br/><pre lang="">				 SELECT FILEMODIFIEDTIME('C:/Documents/myfile.txt');
				 -- Result: 6/25/2019 10:06:58 AM
				 </pre>
				
<h2>FROM_DAYS(datevalue)</h2>
</p>

<p>				Returns a date derived from the number of days after 1582-10-15 (based upon the Gregorian calendar). This will be equivalent to the MYSQL FROM_DAYS function.
					<ul><li><b>datevalue</b>: A integer value representing the number of days since 1582-10-15.
					</li></ul>
				<br/><pre lang="">				SELECT FROM_DAYS(736000);
				-- Result: 2/6/2015
				</pre>
				 
<h2>FROM_UNIXTIME(time, issecond)</h2>
</p>

<p>                Returns a representation of the unix_timestamp argument as a value in YYYY-MM-DD HH:MM:SS expressed in the current time zone.</p>

<p>                    <ul><li><b>time</b>: The time stamp value from epoch time. Milliseconds are accepted.
                        </li><li><b>issecond</b>: Indicates the time stamp value is milliseconds to epoch time.
                    </li></ul>
                    <br/><pre lang="">                      SELECT FROM_UNIXTIME(1540495231, 1);
                      -- Result: 2018-10-25 19:20:31

                      SELECT FROM_UNIXTIME(1540495357385, 0);
                      -- Result: 2018-10-25 19:22:37
                    </pre>				
                
<h2>GETDATE()</h2>
</p>

<p>                Returns the current time stamp of the database system as a datetime value. This value is equal to CURRENT_TIMESTAMP and SYSDATETIME, and is always in the local timezone.
                <br/><pre lang="">                  SELECT GETDATE();
                  -- Result: 2018-02-01 03:04:05
                </pre>
                
<h2>GETUTCDATE()</h2>
</p>

<p>                Returns the current time stamp of the database system formatted as a UTC datetime value. This value is equal to SYSUTCDATETIME.
                <br/><pre lang="">                  SELECT GETUTCDATE();
                  -- For example, if the local timezone is Eastern European Time (GMT+2)
                  -- Result: 2018-02-01 05:04:05
                </pre>
				
<h2>HOUR(date)</h2>
</p>

<p>				Returns the hour component from the provided datetime.
				<ul><li><b>date</b>: The datetime string that specifies the date.
				</li></ul>
				<br/><pre lang="">				SELECT HOUR('02-02-2020 11:30:00');
				-- Result: 11
				</pre>
                
<h2>ISDATE(date, [date_format])</h2>
</p>

<p>                Returns 1 if the value is a valid date, time, or datetime value; otherwise, 0.</p>

<p>                    <ul><li><b>date</b>: The datetime string.
                        </li><li><b>date_format</b>: The optional datetime format.
                    </li></ul>
                    <br/><pre lang="">                      SELECT ISDATE('2018-02-01', 'yyyy-MM-dd');
                      -- Result: 1

                      SELECT ISDATE('Not a date');
                      -- Result: 0
                    </pre>
				
<h2>LAST_WEEK()</h2>
</p>

<p>				Returns a time stamp equivalent to exactly one week before the current date.
				<br/><pre lang="">				SELECT LAST_WEEK();	//Assume the date is 3/17/2020	
				-- Result: 3/10/2020
				</pre>
				
<h2>LAST_MONTH()</h2>
</p>

<p>				Returns a time stamp equivalent to exactly one month before the current date.
				<br/><pre lang="">				SELECT LAST_MONTH(); //Assume the date is 3/17/2020	
				-- Result: 2/17/2020
				</pre>
				
<h2>LAST_YEAR()</h2>
</p>

<p>				Returns a time stamp equivalent to exactly one year before the current date.
				<br/><pre lang="">				SELECT LAST_YEAR();	//Assume the date is 3/17/2020	
				-- Result: 3/10/2019
				</pre>
				
<h2>LDWEEK(date)</h2>
</p>

<p>				Returns the last day of the provided week.
					<ul><li><b>date</b>: The datetime string.
					</li></ul>
				<br/><pre lang="">				SELECT LDWEEK('02-02-2020');
				-- Result: 2/8/2020
				</pre>
				
				
<h2>LDMONTH(date)</h2>
</p>

<p>				Returns the last day of the provided month.
					<ul><li><b>date</b>: The datetime string.
					</li></ul>
				<br/><pre lang="">				SELECT LDMONTH('02-02-2020');
				-- Result: 2/29/2020
				</pre>
				
				
<h2>LDQUARTER(date)</h2>
</p>

<p>				Returns the last day of the provided quarter.
					<ul><li><b>date</b>: The datetime string.
					</li></ul>
				<br/><pre lang="">				SELECT LDQUARTER('02-02-2020');
				-- Result: 3/31/2020
				</pre>
				
        
<h2>MAKEDATE(year, days)</h2>
</p>

<p>        Returns a date value from a year and a number of days.
        <ul><li><b>year</b>: The year
          </li><li><b>days</b>: The number of days into the year. Value must be greater than 0.
        </li></ul>
        <br/><pre lang="">          SELECT MAKEDATE(2020, 1);
          -- Result: 2020-01-01
        </pre>
        
				
<h2>MINUTE(date)</h2>
</p>

<p>				Returns the minute component from the provided datetime.
				<ul><li><b>date</b>: The datetime string that specifies the date.
				</li></ul>
				<br/><pre lang="">				SELECT MINUTE('02-02-2020 11:15:00');
				-- Result: 15
				</pre>
				
				
<h2>MONTH(date)</h2>
</p>

<p>				Returns the month component from the provided datetime.
				<ul><li><b>date</b>: The datetime string that specifies the date.
				</li></ul>
				<br/><pre lang="">				SELECT MONTH('02-02-2020');
				-- Result: 2
				</pre>
				
				
<h2>QUARTER(date)</h2>
</p>

<p>				Returns the quarter associated with the provided datetime.
				<ul><li><b>date</b>: The datetime string that specifies the date.
				</li></ul>
				<br/><pre lang="">				SELECT QUARTER('02-02-2020');
				-- Result: 1
				</pre>
				
				
<h2>SECOND(date)</h2>
</p>

<p>				Returns the second component from the provided datetime.
				<ul><li><b>date</b>: The datetime string that specifies the date.
				</li></ul>
				<br/><pre lang="">				SELECT SECOND('02-02-2020 11:15:23');
				-- Result: 23
				</pre>
				
                
<h2>SMALLDATETIMEFROMPARTS(integer_year, integer_month, integer_day, integer_hour, integer_minute)</h2>
</p>

<p>                Returns the datetime value for the specified date and time.</p>

<p>                    <ul><li><b>year</b>: The integer expression specifying the year.
                        </li><li><b>month</b>: The integer expression specifying the month.
                        </li><li><b>day</b>: The integer expression specifying the day.
                        </li><li><b>hour</b>: The integer expression specifying the hour.
                        </li><li><b>minute</b>: The integer expression specifying the minute.
                    </li></ul>
                    <br/><pre lang="">                      SELECT SMALLDATETIMEFROMPARTS(2018, 2, 1, 1, 2);
                      -- Result: 2018-02-01 01:02:00
                    </pre>
					
				
<h2>STRTODATE(string,format)</h2>
</p>

<p>				Parses the provided string value and returns the corresponding datetime.
					<ul><li><b>string</b>: The string value to be converted to datetime format.
						</li><li><b>format</b>: A format string which describes how to interpret the first string input. Follows standard  <a href="https://docs.oracle.com/javase/7/docs/api/java/text/SimpleDateFormat.html">Oracle date format syntax</a>.  A few special formats are available as well, including UNIX, UNIXMILIS, TICKS, and FILETICKS.
					</li></ul>
				<br/><pre lang="">				SELECT STRTODATE('03*04*2020','dd*MM*yyyy');
				-- Result: 4/3/2020
				</pre>
				
                
<h2>SYSDATETIME()</h2>
</p>

<p>                Returns the current time stamp as a datetime value of the database system. It is equal to GETDATE and CURRENT_TIMESTAMP, and is always in the local timezone.
                <br/><pre lang="">                  SELECT SYSDATETIME();
                  -- Result: 2018-02-01 03:04:05
                </pre>
                
<h2>SYSUTCDATETIME()</h2>
</p>

<p>                Returns the current system date and time as a UTC datetime value. It is equal to GETUTCDATE.
                <br/><pre lang="">                  SELECT SYSUTCDATETIME();
                  -- For example, if the local timezone is Eastern European Time (GMT+2)
                  -- Result: 2018-02-01 05:04:05
                </pre>
                
<h2>TIMEFROMPARTS(integer_hour, integer_minute, integer_seconds, integer_fractions, integer_precision)</h2>
</p>

<p>                Returns the time value for the specified time and with the specified precision.</p>

<p>                    <ul><li><b>hour</b>: The integer expression specifying the hour.
                        </li><li><b>minute</b>: The integer expression specifying the minute.
                        </li><li><b>seconds</b>: The integer expression specifying the seconds.
                        </li><li><b>fractions</b>: The integer expression specifying the fractions of the second.
                        </li><li><b>precision </b>: The integer expression specifying the precision of the fraction.
                    </li></ul>
                    <br/><pre lang="">                      SELECT TIMEFROMPARTS(1, 2, 3, 456, 3);
                      -- Result: 01:02:03.456
                    </pre>
			    
<h2>TO_DAYS(date)</h2>
</p>

<p>				Returns the number of days since 0000-00-01. This will only return a value for dates on or after 1582-10-15 (based upon the Gregorian calendar). This will be equivalent to the MYSQL TO_DAYS function.
					<ul><li><b>date</b>: The datetime string that specifies the date.
					</li></ul>
				<br/><pre lang="">				SELECT TO_DAYS('02-06-2015');
				-- Result: 736000
				</pre>		
				
<h2>WEEK(date)</h2>
</p>

<p>				Returns the week (of the year) associated with the provided datetime.
				<ul><li><b>date</b>: The datetime string that specifies the date.
				</li></ul>
				<br/><pre lang="">				SELECT WEEK('02-17-2020 11:15:23');
				-- Result: 8
				</pre>
                
<h2>YEAR(date)</h2>
</p>

<p>                Returns the integer that specifies the year of the specified date.</p>

<p>                    <ul><li><b>date</b>: The datetime string.
                    </li></ul>
                    <br/><pre lang="">                      SELECT YEAR('2018-02-01');
                      -- Result: 2018
                    </pre>
</p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - Date Literal Functions" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - Date Literal Functions: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>Date Literal Functions</h1>
       

    <div class="chapter_content" id="pg_dateliteralfunctions">

<p>The following date literal functions can be used to filter date fields using relative intervals. Note that while the &lt;, &gt;, and  = operators are supported for these functions, &lt;= and &gt;= are not.


<h2>L_TODAY()</h2>
</p>

<p>The current day.
<br/><pre lang="">  SELECT * FROM MyTable WHERE MyDateField = L_TODAY()</pre>



<h2>L_YESTERDAY()</h2>
</p>

<p>The previous day.
<br/><pre lang="">  SELECT * FROM MyTable WHERE MyDateField = L_YESTERDAY()</pre>



<h2>L_TOMORROW()</h2>
</p>

<p>The following day.
<br/><pre lang="">  SELECT * FROM MyTable WHERE MyDateField = L_TOMORROW()</pre>



<h2>L_LAST_WEEK()</h2>
</p>

<p>Every day in the preceding week.
<br/><pre lang="">  SELECT * FROM MyTable WHERE MyDateField = L_LAST_WEEK()</pre>



<h2>L_THIS_WEEK()</h2>
</p>

<p>Every day in the current week.
<br/><pre lang="">  SELECT * FROM MyTable WHERE MyDateField = L_THIS_WEEK()</pre>



<h2>L_NEXT_WEEK()</h2>
</p>

<p>Every day in the following week.
<br/><pre lang="">  SELECT * FROM MyTable WHERE MyDateField = L_NEXT_WEEK()</pre>

<b>Also available:</b>
	<ul><li>L_LAST/L_THIS/L_NEXT MONTH
		</li><li>L_LAST/L_THIS/L_NEXT QUARTER
		</li><li>L_LAST/L_THIS/L_NEXT YEAR
	</li></ul>


<h2>L_LAST_N_DAYS(n)</h2>
</p>

<p>The previous n days, excluding the current day.
<br/><pre lang="">  SELECT * FROM MyTable WHERE MyDateField = L_LAST_N_DAYS(3)</pre>



<h2>L_NEXT_N_DAYS(n)</h2>
</p>

<p>The following n days, including the current day.
<br/><pre lang="">  SELECT * FROM MyTable WHERE MyDateField = L_NEXT_N_DAYS(3)</pre>


<b>Also available:</b>
	<ul><li>L_LAST/L_NEXT_90_DAYS
	</li></ul>



<h2>L_LAST_N_WEEKS(n)</h2>
</p>

<p>Every day in every week, starting n weeks before current week, and ending in the previous week.
<br/><pre lang="">  SELECT * FROM MyTable WHERE MyDateField = L_LAST_N_WEEKS(3)</pre>



<h2>L_NEXT_N_WEEKS(n)</h2>
</p>

<p>Every day in every week, starting the following week, and ending n weeks in the future.
<br/><pre lang="">  SELECT * FROM MyTable WHERE MyDateField = L_NEXT_N_WEEKS(3)</pre>


<b>Also available:</b>
	<ul><li>L_LAST/L_NEXT_N_MONTHS(n)
		</li><li>L_LAST/L_NEXT_N_QUARTERS(n)
		</li><li>L_LAST/L_NEXT_N_YEARS(n)
	</li></ul></p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - SELECT Statements" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - SELECT Statements: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>SELECT Statements</h1>
       

    <div class="chapter_content" id="pg_select">

<p>A SELECT statement can consist of the following basic clauses. 

<ul><li>SELECT </li><li>INTO </li><li>FROM</li><li>JOIN</li><li>WHERE</li><li>GROUP BY</li><li>HAVING</li><li>UNION</li><li>ORDER BY</li><li>LIMIT</li></ul>


<h2>SELECT Syntax</h2>
</p>

<p>The following syntax diagram outlines the syntax supported by the SQL engine of the driver:
 
<br/><pre lang="sql">SELECT {
  [ TOP &lt;numeric_literal&gt; | DISTINCT ]
  { 
    * 
    | { 
        &lt;expression&gt; [ [ AS ] &lt;column_reference&gt; ] 
        | { &lt;table_name&gt; | &lt;correlation_name&gt; } .* 
      } [ , ... ] 
  }
  [ INTO csv:// [ filename= ] &lt;file_path&gt; [ ;delimiter=tab ] ]
  { 
    FROM &lt;table_reference&gt; [ [ AS ] &lt;identifier&gt; ] 
  } [ , ... ]
  [ [  
      INNER | { { LEFT | RIGHT | FULL } [ OUTER ] } 
    ] JOIN &lt;table_reference&gt; [ ON &lt;search_condition&gt; ] [ [ AS ] &lt;identifier&gt; ] 
  ] [ ... ] 
  [ WHERE &lt;search_condition&gt; ]
  [ GROUP BY &lt;column_reference&gt; [ , ... ]
  [ HAVING &lt;search_condition&gt; ]
  [ UNION [ ALL ] &lt;select_statement&gt; ]
  [ 
    ORDER BY 
    &lt;column_reference&gt; [ ASC | DESC ] [ NULLS FIRST | NULLS LAST ]
  ]
  [ 
    LIMIT &lt;expression&gt;
    [ 
      { OFFSET | , }
      &lt;expression&gt; 
    ]
  ] 
} | SCOPE_IDENTITY() 

&lt;expression&gt; ::=
  | &lt;column_reference&gt;
  | @ &lt;parameter&gt; 
  | ?
  | COUNT( * | { [ DISTINCT ] &lt;expression&gt; } )
  | { AVG | MAX | MIN | SUM | COUNT } ( &lt;expression&gt; ) 
  | NULLIF ( &lt;expression&gt; , &lt;expression&gt; ) 
  | COALESCE ( &lt;expression&gt; , ... ) 
  | CASE &lt;expression&gt;
      WHEN { &lt;expression&gt; | &lt;search_condition&gt; } THEN { &lt;expression&gt; | NULL } [ ... ]
    [ ELSE { &lt;expression&gt; | NULL } ]
    END 
  | {RANK() | DENSE_RANK()} OVER ([PARTITION BY &lt;column_reference&gt;] {ORDER BY &lt;column_reference&gt;})
  | &lt;literal&gt;
  | &lt;sql_function&gt; 

&lt;search_condition&gt; ::= 
  {
    &lt;expression&gt; { = | &gt; | &lt; | &gt;= | &lt;= | &lt;&gt; | != | LIKE | NOT LIKE | IN | NOT IN | IS NULL | IS NOT NULL | AND | OR | CONTAINS | BETWEEN } [ &lt;expression&gt; ]
  } [ { AND | OR } ... ] </pre>

 


<h2>Examples</h2>

</p>

<p><ol><li>Return all columns:
<br/><pre lang="">SELECT * FROM SampleTable_1</pre></li><li>Rename a column:
<br/><pre lang="">SELECT [Column1] AS MY_Column1 FROM SampleTable_1</pre></li><li>Cast a column's data as a different data type:
<br/><pre lang="">SELECT CAST(AnnualRevenue AS VARCHAR) AS Str_AnnualRevenue FROM SampleTable_1</pre></li><li>Search data:
<br/><pre lang="">SELECT * FROM SampleTable_1 WHERE Column2 = 'Bob'</pre></li><li>Return the number of items matching the query criteria:
<br/><pre lang="">SELECT COUNT(*) AS MyCount FROM SampleTable_1 </pre></li><li>Return the number of unique items matching the query criteria:
<br/><pre lang="">SELECT COUNT(DISTINCT Column1) FROM SampleTable_1 </pre></li><li>Return the unique items matching the query criteria:
<br/><pre lang="">SELECT DISTINCT Column1 FROM SampleTable_1 </pre></li><li>Sort a result set in ascending order:
<br/><pre lang="">SELECT Id, Column1 FROM SampleTable_1  ORDER BY Column1 ASC</pre></li><li>Restrict a result set to the specified number of rows:

<br/><pre lang="">SELECT Id, Column1 FROM SampleTable_1 LIMIT 10 </pre></li><li>Parameterize a query to pass in inputs at execution time. This enables you to create prepared statements and mitigate SQL injection attacks. 
<br/><pre lang="">SELECT * FROM SampleTable_1 WHERE Column2 = @param</pre></li></ol>
See <a href="#pg_cacheExplicitly">Explicitly Caching Data</a> for information on using the SELECT statement in offline mode.

  
    
<h2>Pseudo Columns</h2>
</p>

<p>    Some input-only fields are available in SELECT statements. These fields, called pseudo columns, do not 
    appear as regular columns in the results, yet may be specified as part of the WHERE clause. You can use pseudo columns to access additional features from Apache Kafka.

    <br/><pre lang="">    SELECT * FROM SampleTable_1 WHERE Query = 'Column3 &gt; 100'
    </pre>
  </p>

<p></p>

<p>
<h2>Aggregate Functions</h2>
</p>

<p>For SELECT examples using aggregate functions, see <a href="#pg_sfagg">Aggregate Functions</a>.
</p>

<p>
<h2>JOIN Queries</h2>
</p>

<p>See <a href="#pg_sfjoin">JOIN Queries</a> for SELECT query examples using JOINs.

</p>

<p>
<h2>Date Literal Functions</h2>
</p>

<p><a href="#pg_dateliteralfunctions">Date Literal Functions</a> contains SELECT examples with date literal functions.


</p>

<p>
<h2>Window Functions</h2>
</p>

<p>See <a href="#pg_window">Window Functions</a> for SELECT examples containing window functions.</p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - Aggregate Functions" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - Aggregate Functions: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>Aggregate Functions</h1>
       

    <div class="chapter_content" id="pg_sfagg">

<p>

<h3>COUNT</h3>
</p>

<p>Returns the number of rows matching the query criteria.
<br/><pre lang="">SELECT COUNT(*) FROM SampleTable_1 WHERE Column2 = 'Bob'</pre>


    
    

<h3>COUNT(DISTINCT)</h3>
</p>

<p>Returns the number of distinct, non-null field values matching the query criteria.
<br/><pre lang="">SELECT COUNT(DISTINCT Id) AS DistinctValues FROM SampleTable_1 WHERE Column2 = 'Bob'</pre>


<h3>AVG</h3>
</p>

<p>Returns the average of the column values.
<br/><pre lang="">SELECT Column1, AVG(AnnualRevenue) FROM SampleTable_1 WHERE Column2 = 'Bob'  GROUP BY Column1</pre>


<h3>MIN</h3>
</p>

<p>Returns the minimum column value.
<br/><pre lang="">SELECT MIN(AnnualRevenue), Column1 FROM SampleTable_1 WHERE Column2 = 'Bob' GROUP BY Column1</pre>


<h3>MAX</h3>
</p>

<p>Returns the maximum column value.

<br/><pre lang="">SELECT Column1, MAX(AnnualRevenue) FROM SampleTable_1 WHERE Column2 = 'Bob' GROUP BY Column1</pre>


<h3>SUM</h3>
</p>

<p>Returns the total sum of the column values.
<br/><pre lang="">SELECT SUM(AnnualRevenue) FROM SampleTable_1 WHERE Column2 = 'Bob'</pre>
</p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - JOIN Queries" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - JOIN Queries: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>JOIN Queries</h1>
       

    <div class="chapter_content" id="pg_sfjoin">

<p>The CData JDBC Driver for Apache Kafka supports standard SQL joins like the following examples.


<h2>Inner Join</h2>

</p>

<p>An inner join selects only rows from both tables that match the join condition: 

<br/><pre lang="">SELECT c.SampleCol1, o.SampleCol2, o.SampleCol3, o.SampleCol4 FROM SampleTable_1 c INNER JOIN SampleTable_2 o ON c.Id = o.Id2</pre>


<h2>Left Join</h2>

</p>

<p>A left join selects all rows in the FROM table and only matching rows in the JOIN table:

<br/><pre lang=""> SELECT c.SampleCol1, o.SampleCol2, o.SampleCol3, o.SampleCol4 FROM SampleTable_1 c LEFT JOIN SampleTable_2 o ON c.Id = o.Id2</pre>
</p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - Window Functions" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - Window Functions: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>Window Functions</h1>
       

    <div class="chapter_content" id="pg_window">

<p>Window functions allow you to create computed fields from a group of rows (a <i>window</i>) that return a result for each row, as opposed to one computed result for a set of rows, as is the case with aggregate functions. The driver supports the following window function syntax.</p>

<p><b>Note:</b> Window function support is an experimental feature of the driver. This functionality extends beyond the driver's core scope of being SQL-92 compliant. As such, performance with window functions may not be optimal.</p>

<p>
<h2>Window Function Clauses</h2>
</p>

<p>
<h3>OVER</h3>
</p>

<p>The OVER clause defines the window over which window functions are performed.
<br/><pre lang="">SELECT A, B, &lt;window function&gt; OVER (&lt;window frame&gt;) FROM TableName</pre></p>

<p>The &lt;window function&gt; refers to any supported window function clause, and the &lt;window frame&gt; refers to one or more clauses that specifies the logic by which the window is defined.

<h3>PARTITION BY</h3>
</p>

<p>The PARTITION BY clause subdivides a window into sub-windows called <i>partitions</i>. For each unique value in the column specified in the PARTITION BY clause, every record with that value collectively forms an individual partition.
<br/><pre lang="">SELECT A, B, &lt;window function&gt; OVER (PARTITION BY A ORDER BY B) From SampleTable_1</pre></p>

<p>The &lt;window function&gt; refers to any supported window function clause.

<h2>Window Functions</h2>
</p>

<p>
<h3>RANK()</h3>
</p>

<p>Assigns a rank number to each record in a window based on the value of the column specified in the required ORDER BY clause.</p>

<p>If two or more records have an equal value in the in ranked column, they all receive the same rank number and the rank count increments internally, skipping ahead one rank number for each record with a duplicate value in the ORDER BY column.
<br/><pre lang="">SELECT Id, Column1, RANK() OVER (ORDER BY Column1) AS Rank FROM SampleTable_1</pre></p>

<p>If you add a PARTITION BY clause, a separate set of ranks is calculated for each partition.
<br/><pre lang="">SELECT Id, Column1, RANK() OVER (PARTITION BY Id ORDER BY Column1) AS Rank FROM SampleTable_1</pre></p>

<p>
<h3>DENSE_RANK()</h3>
</p>

<p>Operates like the RANK() function, but it doesn't increment the internal rank counter for each record with a duplicate value in the ranked column.</p>

<p>This means that, while records with identical values in the ORDER BY column still share a rank number, the function never skips a rank number. 
<br/><pre lang="">SELECT Id, Column1, DENSE_RANK() OVER (PARTITION BY Id ORDER BY Column1) AS Rank FROM SampleTable_1</pre></p>

<p>If you add a PARTITION BY clause, a separate set of ranks is calculated for each partition.
<br/><pre lang="">SELECT Id, Column1, DENSE_RANK() OVER (PARTITION BY Id ORDER BY Column1) AS Rank FROM SampleTable_1</pre>


<h3>NTILE()</h3>


Distributes rows of an ordered partition into a specified number of approximately equal groups, or <i>buckets</i>. It assigns each group a bucket number starting from one. For each row in a group, the NTILE() function assigns a bucket number representing the group to which the row belongs.</p>

<p>The syntax of NTILE() is:
<br/><pre lang="">NTILE(buckets) OVER (
    [PARTITION BY partition_expression, ... ]
    ORDER BY sort_expression [ASC | DESC], ...
)</pre>

The following are paramaters that NTILE() supports:</p>

<p><ul><li>buckets: The number of buckets into which the rows are divided. The buckets can be an expression or subquery that evaluates to a positive integer. It cannot be a window function.</li><li>PARTITION BY: distributes rows of a result set into partitions to which the NTILE() function is applied.</li><li>ORDER BY is clause that specifies the logical order of rows in each partition to which the NTILE() is applied.</li></ul></p>

<p>If the number of rows is not divisible by the buckets, the NTILE() function returns groups of two sizes with the difference by one. The larger groups always precedes the smaller group in the order set by ORDER BY in the OVER() clause.</p>

<p>If the total of rows is divisible by the number of buckets, the function divides the rows evenly among buckets.


<h4>Example</h4>


The following statement creates a new table named <b>ntile_demo</b> that stores 10 integers:
<br/><pre lang="">CREATE TABLE sales.ntile_demo (
	v INT NOT NULL
);
	
INSERT INTO sales.ntile_demo(v) 
VALUES(1),(2),(3),(4),(5),(6),(7),(8),(9),(10);	
	
SELECT * FROM sales.ntile_demo;</pre>

This statement uses the NTILE() function to divide ten rows into three groups:
<br/><pre lang="">SELECT 
	v, 
	NTILE (3) OVER (
		ORDER BY v
	) buckets
FROM 
	sales.ntile_demo;</pre>


<h3>PERCENT_RANK()</h3>


Calculates the relative rank SQL Percentile of each row. It returns values greater than zero, but the maximum value is one. It does not count any NULL values. This function is nondeterministic.</p>

<p>The syntax of PERCENT_RANK() is:
<br/><pre lang="">PERCENT_RANK() OVER (
      [PARTITION BY partition_expression, ... ]
      ORDER BY sort_expression [ASC | DESC], ...
  )
  </pre>
  
This syntax uses the following parameters.</p>

<p><ul><li>PARTITION BY: By default, SQL Server treats the whole data set as a single set. You can specify the PARTITION BY clause to divide data into multiple sets. The Percent_Rank function performs the analytical calculations on each set. This parameter is optional.</li><li>ORDER BY: Sorts the data in either ascending or descending order. This parameter is required.</li></ul></p>

<p></p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - INSERT Statements" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - INSERT Statements: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>INSERT Statements</h1>
       

    <div class="chapter_content" id="pg_insert">

<p>To create new records, use INSERT statements.

<h2>INSERT Syntax</h2>
</p>

<p>The INSERT statement specifies the columns to be inserted and the new column values. You can specify the column values in a comma-separated list in the VALUES clause, as shown in the following example:
<br/><pre lang="sql">INSERT INTO &lt;table_name&gt; 
( &lt;column_reference&gt; [ , ... ] )
VALUES 
( { &lt;expression&gt; | NULL } [ , ... ] ) 
  

&lt;expression&gt; ::=
  | @ &lt;parameter&gt; 
  | ?
  | &lt;literal&gt;</pre>



You can use the executeUpdate method of the Statement and PreparedStatement classes to execute data manipulation commands and retrieve the rows affected. 
<br/><pre lang="java">String cmd = "INSERT INTO SampleTable_1 (Column1) VALUES (?)";
PreparedStatement pstmt = connection.prepareStatement(cmd);
pstmt.setString(1, "John");
int count = pstmt.executeUpdate();
System.out.println(count+" rows were affected");
connection.close();	
</pre>
</p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - CACHE Statements" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - CACHE Statements: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>CACHE Statements</h1>
       

    <div class="chapter_content" id="pg_cache">

<p>When caching is enabled, CACHE statements provide complete control over the data that is cached and the table to which it is cached. The CACHE statement executes the SELECT statement specified and caches its results to a table with the same name in the cache database or to table specified in <i>&lt;cached_table_name&gt;</i>. The driver updates or inserts rows to the cache depending on whether or not they already exist in the cache, so the primary key, which is used to identify existing rows, must be included in the selected columns. 
  </p>

<p></p>

<p>See <a href="#pg_caching">Caching Data</a> for more information on different caching strategies.


<h2>CACHE Statement Syntax</h2>
</p>

<p>The cache statement may include the following options that alter its behavior:
<br/><pre lang="">CACHE [ &lt;cached_table_name&gt; ] [ WITH TRUNCATE | AUTOCOMMIT | SCHEMA ONLY | DROP EXISTING | ALTER SCHEMA ] &lt;select_statement&gt; </pre>
</p>

<p><b>WITH TRUNCATE</b></p>

<p>If this option is set, the driver removes existing rows in the cache table before adding the selected rows. Use this option if you want to refresh the entire cache table but keep its existing schema.</p>

<p>
<b>AUTOCOMMIT</b></p>

<p>If this option is set, the driver commits each row individually. Use this option if you want to ignore the rows that could not be cached due to some reason. By default, the entire result set is cached as a single transaction.</p>

<p>
<b>DROP EXISTING</b></p>

<p>If this option is set, the driver drops the existing cache table before caching the new results. Use this option if you want to refresh the entire cache table, including its schema.</p>

<p><b>SCHEMA ONLY</b></p>

<p>If this option is set, the driver creates the cache table based on the SELECT statement without executing the query.</p>

<p><b>ALTER SCHEMA</b></p>

<p>If this option is set, the driver alters the schema of the existing table in the cache if it does not match the schema of the SELECT statement. This option results in new columns or dropped columns, if the schema of the SELECT statement does not match the cached table.


<h2>Common Queries</h2>
</p>

<p>Use the following cache statement to cache all rows of a table:
<br/><pre lang="">CACHE SELECT * FROM SampleTable_1</pre></p>

<p>Use the following cache statement to cache all rows of a table into the cache table CachedSampleTable_1:
<br/><pre lang="">CACHE CachedSampleTable_1 SELECT * FROM SampleTable_1</pre></p>

<p>Use the following cache statement for incremental caching. The DateModified column may not exist in all tables. The cache statement shows how incremental caching would work if there were such a column. Also, notice that, in this case, the WITH TRUNCATE and DROP EXISTING options are specifically omitted, which would have deleted all existing rows.
<br/><pre lang="">CACHE CachedSampleTable_1 SELECT * FROM SampleTable_1 WHERE DateModified &gt; '2013-04-04'</pre>
</p>

<p>Use the following cache statements to create a table with all available columns that will then cache only a few of them. The sequence of statements cache only Id and Column1 even though the cache table CachedSampleTable_1 has all the columns in SampleTable_1. 
<br/><pre lang="">CACHE CachedSampleTable_1 SCHEMA ONLY SELECT * FROM SampleTable_1
CACHE CachedSampleTable_1 SELECT Id, Column1 FROM SampleTable_1</pre></p>

<p></p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - EXECUTE Statements" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - EXECUTE Statements: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>EXECUTE Statements</h1>
       

    <div class="chapter_content" id="pg_exec">

<p>To execute stored procedures, you can use EXECUTE or EXEC statements. 
</p>

<p>EXEC and EXECUTE assign stored procedure inputs, referenced by name, to values or parameter names. 


<h2>Stored Procedure Syntax</h2>
</p>

<p>To execute a stored procedure as an SQL statement, use the following syntax:
<br/><pre lang="sql"> 
{ EXECUTE | EXEC } &lt;stored_proc_name&gt; 
{
  [ @ ] &lt;input_name&gt; = &lt;expression&gt;
} [ , ... ]

&lt;expression&gt; ::=
  | @ &lt;parameter&gt; 
  | ?
  | &lt;literal&gt;</pre>


<h2>Example Statements</h2>
</p>

<p>Reference stored procedure inputs by name:
<br/><pre lang="plain">EXECUTE my_proc @second = 2, @first = 1, @third = 3;</pre>
</p>

<p>Execute a parameterized stored procedure statement:
<br/><pre lang="">EXECUTE my_proc second = @p1, first = @p2, third = @p3; </pre>
</p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - PIVOT and UNPIVOT" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - PIVOT and UNPIVOT: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>PIVOT and UNPIVOT</h1>
       

    <div class="chapter_content" id="pg_pivotunpivot">

<p><b>PIVOT and UNPIVOT</b> can be used to change a table-valued expression into another table. 


<h3>PIVOT</h3>

PIVOT rotates a table-value expression by turning unique values from one column into multiple columns in the output. PIVOT can run aggregations where required on any column value. 


<h5>PIVOT Synax</h5>

<br/><pre lang="sql"> 
"SELECT 'AverageCost' AS Cost_Sorted_By_Production_Days, [0], [1], [2], [3], [4]
FROM
(
SELECT DaysToManufacture, StandardCost
FROM Production.Product
) AS SourceTable
PIVOT
(
AVG(StandardCost)
FOR DaysToManufacture IN ([0], [1], [2], [3], [4])
) AS PivotTable;"</pre>


<h3>UNPIVOT</h3>

UNPIVOT carries out nearly the opposite to PIVOT by rotating columns of a table-valued expressions into column values.

<h5>UNPIVOT Sytax</h5>

<br/><pre lang="sql"> 
"SELECT VendorID, Employee, Orders
FROM
(SELECT VendorID, Emp1, Emp2, Emp3, Emp4, Emp5
FROM pvt) p
UNPIVOT
(Orders FOR Employee IN
(Emp1, Emp2, Emp3, Emp4, Emp5)
)AS unpvt;"</pre>
</p>

<p>For further information on PIVOT and UNPIVOT, see 
<a href="https://docs.microsoft.com/en-us/sql/t-sql/queries/from-transact-sql?view=sql-server-ver15">FROM clause plus JOIN, APPLY, PIVOT (Transact-SQL)</a></p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - Data Model" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - Data Model: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>Data Model</h1>
       

    <div class="chapter_content" id="pg_datamodel">

<p>
<h2>Tables</h2>

The CData JDBC Driver for Apache Kafka dynamically models Apache Kafka topics as tables. A complete list of discovered topics can be obtained from the sys_tables system table.
</p>

<p>SELECTing from a topic returns existing messages on the topic, as well as live messages posted before the number of seconds specified by the <u>ReadDuration</u> have elapsed.


<h2>Stored Procedures</h2>
</p>

<p><a href="#pg_allsps">Stored Procedures</a> are function-like interfaces to Apache Kafka. They can be used to create schema files, commit messages, and more.


<h2>Consumer Groups</h2>
</p>

<p>Connections that the driver makes to Apache Kafka are always part of a consumer group.
You can control the consumer group by setting a value for the <u>ConsumerGroupId</u> connection property.
Using the same consumer group ID across multiple connections puts those connections into the same consumer group.
The driver generates a random consumer group ID if one is not provided.
</p>

<p>All members of a consumer group share an offset that determines what messages are read next within each topic and partition.
The driver supports two ways of updating the offset:

<ul><li>If <u>AutoCommit</u> is enabled, the driver periodically commits the offset for any topics and partitions that have been read by SELECT queries.
    The exact interval is determined by the auto-commit properties in the native library.
    See <u>ConsumerProperties</u> for details on how to configure these properties.</li><li>The CommitOffset stored procedure stores the offset of the last item read by the current query.
    Note that this must be called while the query resultset is still open.
    The driver resets the offset when the resultset is closed.</li></ul>
</p>

<p>If there is no existing offset, the driver uses the <u>OffsetResetStrategy</u> to determine what the offset should be.
This may happen if the broker does not recognize the consumer group or if the consumer group never committed an offset.


<h2>Bulk Messages</h2>
</p>

<p>The driver supports reading bulk messages from topics using the CSV, JSON, or XML <u>SerializationFormat</u>.
For example, if a single message contained this CSV data, the driver would expand it into three rows:

<br/><pre lang="">"1","alpha"
"2","beta"
"3","gamma"</pre>
</p>

<p>Apache Kafka does not natively support bulk messages, which can lead to rows being skipped in some circumstances.
For example:

<ol><li>A driver connection is created with <u>ConsumerGroupId</u>=x</li><li>The connection executes the query <var>SELECT * FROM topic LIMIT 3</var>.</li><li>The connection commits its offset and closes.</li><li>Another connection is created with the same <u>ConsumerGroupId</u></li><li>The connection executes the query <var>SELECT * FROM topic</var>.</li></ol>
</p>

<p>Consider what happens if this procedure is performed on the following topic.
The first connection consumes all rows from the first message and one row from the second.
However, the driver has no way to report to Apache Kafka that only part of the second message was read.
This means that step 3 commits the offset 3 and the second connection starts on row 5, skipping row 4.

<br/><pre lang="">"row 1"
"row 2"
/* End of message 1 */

"row 3"
"row 4"
/* End of message 2 */

"row 5"
"row 6"
/* End of message 3 */</pre>
</p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka -  Stored Procedures" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka -  Stored Procedures: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1> Stored Procedures</h1>
       

    <div class="chapter_content" id="pg_allsps">

<p>Stored procedures are function-like interfaces that extend the functionality of the driver beyond simple SELECT/INSERT operations with Apache Kafka.</p>

<p>Stored procedures accept a list of parameters, perform their intended function, and then return any relevant response data from Apache Kafka, along with an indication of whether the procedure succeeded or failed.
          
<h2>CData JDBC Driver for Apache Kafka Stored Procedures</h2>
</p>

<p>              
    <p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap"><b>Name</b> </td><td><b>Description</b>
      </td></tr><tr><td style="white-space:nowrap"><a href="#pg_sp-commitoffset">CommitOffset</a>
      </td><td>Commits the messages to the connection ConsumerGroupId.
      </td></tr><tr><td style="white-space:nowrap"><a href="#pg_sp-createschema">CreateSchema</a>
      </td><td>Creates a schema definition of a table in Kafka.
      </td></tr><tr><td style="white-space:nowrap"><a href="#pg_sp-getadminconsenturl">GetAdminConsentURL</a>
      </td><td>Gets the admin consent URL that must be opened separately by an admin of a given domain to grant access to your application. Only needed when using custom OAuth credentials.
      </td></tr><tr><td style="white-space:nowrap"><a href="#pg_sp-getoauthaccesstoken">GetOAuthAccessToken</a>
      </td><td>Gets an OAuth authentication token.
      </td></tr><tr><td style="white-space:nowrap"><a href="#pg_sp-getoauthauthorizationurl">GetOAuthAuthorizationURL</a>
      </td><td>Gets the authorization URL that must be opened separately by the user to grant access to your application. You will request the OAuthAccessToken from this URL.
      </td></tr><tr><td style="white-space:nowrap"><a href="#pg_sp-producemessage">ProduceMessage</a>
      </td><td>Sends a raw message to Kafka.
      </td></tr><tr><td style="white-space:nowrap"><a href="#pg_sp-refreshoauthaccesstoken">RefreshOAuthAccessToken</a>
      </td><td>Exchanges a access token for a new access token.
 
  </td></tr></table></center><p /></p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - CommitOffset" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - CommitOffset: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>CommitOffset</h1>
       

    <div class="chapter_content" id="pg_sp-commitoffset">

<p>Commits the messages to the connection ConsumerGroupId.
      
      </p>

<p>      
<h3>Result Set Columns</h3>

        <p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap"><b>Name</b>        </td><td><b>Type</b> </td><td><b>Description</b>
    </td></tr><tr><td style="white-space:nowrap">      Success
          </td><td><i>String</i>
      </td><td>        Whether the operation was successful or not.
          </td></tr></table></center><p />
          </p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - CreateSchema" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - CreateSchema: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>CreateSchema</h1>
       

    <div class="chapter_content" id="pg_sp-createschema">

<p>Creates a schema definition of a table in Kafka.
      
          
<h2>CreateSchema</h2>
</p>

<p>Creates a local schema file (.rsd) from an existing table or view in the data model.</p>

<p>The schema file is created in the directory set in the <u>Location</u> connection property when this procedure is executed. You can edit the file to include or exclude columns, rename columns, or adjust column datatypes.</p>

<p>The driver checks the <u>Location</u> to determine if the names of any .rsd files match a table or view in the data model. If there is a duplicate, the schema file will take precedence over the default instance of this table in the data model. If a schema file is present in <u>Location</u> that does not match an existing table or view, a new table or view entry is added to the data model of the driver.

    </p>

<p>      
<h3>Input</h3>

        <p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap">          <b>Name</b>
          </td><td><b>Type</b>
        
          </td><td><b>Required</b>
        
          </td><td><b>Accepts Output Streams</b>
          </td><td><b>Description</b>
    </td></tr><tr><td style="white-space:nowrap">      TableName
          </td><td><i>String</i>
      
      </td><td><i>True</i>
      
      </td><td><i>False</i>
     </td><td>        The name of the table.
    </td></tr><tr><td style="white-space:nowrap">      FileName
          </td><td><i>String</i>
      
      </td><td><i>False</i>
      
      </td><td><i>False</i>
     </td><td>        The full file path and name of the schema to generate. Ex: 'C:\\Users\\User\\Desktop\\ApacheKafka\\company.rsd'
    </td></tr><tr><td style="white-space:nowrap">      FileStream
          </td><td><i>String</i>
      
      </td><td><i>False</i>
      
      </td><td><i>True</i>
     </td><td>        Stream to write the schema to.
          </td></tr></table></center><p />
          </p>

<p>      
<h3>Result Set Columns</h3>

        <p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap"><b>Name</b>        </td><td><b>Type</b> </td><td><b>Description</b>
    </td></tr><tr><td style="white-space:nowrap">      Result
          </td><td><i>String</i>
      </td><td>        Whether or not the schema was successfully downloaded.
    </td></tr><tr><td style="white-space:nowrap">      FileData
          </td><td><i>String</i>
      </td><td>        The generated schema as base64. Only set if FileName and FileStream are not provided.
          </td></tr></table></center><p />
          </p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - GetAdminConsentURL" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - GetAdminConsentURL: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>GetAdminConsentURL</h1>
       

    <div class="chapter_content" id="pg_sp-getadminconsenturl">

<p>Gets the admin consent URL that must be opened separately by an admin of a given domain to grant access to your application. Only needed when using custom OAuth credentials.
      
    </p>

<p>      
<h3>Input</h3>

        <p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap">          <b>Name</b>
          </td><td><b>Type</b>
        
          </td><td><b>Required</b>
          </td><td><b>Description</b>
    </td></tr><tr><td style="white-space:nowrap">      CallbackUrl
          </td><td><i>String</i>
      
      </td><td><i>False</i>
     </td><td>        The URL the user will be redirected to after authorizing your application. This value must match the Reply URL in the Azure AD app settings.
    </td></tr><tr><td style="white-space:nowrap">      State
          </td><td><i>String</i>
      
      </td><td><i>False</i>
     </td><td>        The same value for state that you sent when you requested the authorization code.
          </td></tr></table></center><p />
          </p>

<p>      
<h3>Result Set Columns</h3>

        <p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap"><b>Name</b>        </td><td><b>Type</b> </td><td><b>Description</b>
    </td></tr><tr><td style="white-space:nowrap">      URL
          </td><td><i>String</i>
      </td><td>        The authorization URL, entered into a Web browser to obtain the verifier token and authorize your app.
          </td></tr></table></center><p />
          </p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - GetOAuthAccessToken" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - GetOAuthAccessToken: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>GetOAuthAccessToken</h1>
       

    <div class="chapter_content" id="pg_sp-getoauthaccesstoken">

<p>Gets an OAuth authentication token.
      
    </p>

<p>      
<h3>Input</h3>

        <p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap">          <b>Name</b>
          </td><td><b>Type</b>
        
          </td><td><b>Required</b>
          </td><td><b>Description</b>
    </td></tr><tr><td style="white-space:nowrap">      AuthMode
          </td><td><i>String</i>
      
      </td><td><i>False</i>
     </td><td>        The type of authentication mode to use. Select App for getting authentication tokens via a desktop app. Select Web for getting authentication tokens via a Web app.
          
<p>The allowed values are <i>APP, WEB</i>.</p>

          
<p>The default value is <i>WEB</i>.</p>

    </td></tr><tr><td style="white-space:nowrap">      Verifier
          </td><td><i>String</i>
      
      </td><td><i>False</i>
     </td><td>        The verifier returned after the user has authorized your app to have access to their data. This value will be returned as a parameter to the callback URL.
    </td></tr><tr><td style="white-space:nowrap">      CallbackURL
          </td><td><i>String</i>
      
      </td><td><i>False</i>
     </td><td>        Determines where the response is sent.
    </td></tr><tr><td style="white-space:nowrap">      State
          </td><td><i>String</i>
      
      </td><td><i>False</i>
     </td><td>        Indicates any state which may be useful to your application upon receipt of the response. Your application receives the same value it sent, as this parameter makes a round-trip to the authorization server and back. Uses include redirecting the user to the correct resource in your site, nonces, and cross-site-request-forgery mitigations.
          </td></tr></table></center><p />
          </p>

<p>      
<h3>Result Set Columns</h3>

        <p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap"><b>Name</b>        </td><td><b>Type</b> </td><td><b>Description</b>
    </td></tr><tr><td style="white-space:nowrap">      OAuthAccessToken
          </td><td><i>String</i>
      </td><td>        The access token used for communication with the API.
    </td></tr><tr><td style="white-space:nowrap">      OAuthRefreshToken
          </td><td><i>String</i>
      </td><td>        The refresh access token used to refresh your connection.
    </td></tr><tr><td style="white-space:nowrap">      ExpiresIn
          </td><td><i>String</i>
      </td><td>        The lifetime of the access token, in seconds.
          </td></tr></table></center><p />
          </p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - GetOAuthAuthorizationURL" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - GetOAuthAuthorizationURL: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>GetOAuthAuthorizationURL</h1>
       

    <div class="chapter_content" id="pg_sp-getoauthauthorizationurl">

<p>Gets the authorization URL that must be opened separately by the user to grant access to your application. You will request the OAuthAccessToken from this URL.
      
    </p>

<p>      
<h3>Input</h3>

        <p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap">          <b>Name</b>
          </td><td><b>Type</b>
        
          </td><td><b>Required</b>
          </td><td><b>Description</b>
    </td></tr><tr><td style="white-space:nowrap">      CallbackURL
          </td><td><i>String</i>
      
      </td><td><i>False</i>
     </td><td>        Determines where the response is sent. The value of this parameter must exactly match one of the values registered in the APIs Console (including the HTTP or HTTPS schemes, capitalization, and trailing '/').
    </td></tr><tr><td style="white-space:nowrap">      State
          </td><td><i>String</i>
      
      </td><td><i>False</i>
     </td><td>        Indicates any state which may be useful to your application upon receipt of the response. Your application receives the same value it sent, as this parameter makes a round-trip to the Google authorization server and back. Uses include redirecting the user to the correct resource in your site, nonces, and cross-site-request-forgery mitigations.
          </td></tr></table></center><p />
          </p>

<p>      
<h3>Result Set Columns</h3>

        <p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap"><b>Name</b>        </td><td><b>Type</b> </td><td><b>Description</b>
    </td></tr><tr><td style="white-space:nowrap">      URL
          </td><td><i>String</i>
      </td><td>        The authorization URL, entered into a Web browser to obtain the verifier token and authorize your app.
          </td></tr></table></center><p />
          </p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - ProduceMessage" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - ProduceMessage: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>ProduceMessage</h1>
       

    <div class="chapter_content" id="pg_sp-producemessage">

<p>Sends a raw message to Kafka.
      
          
<h2>Minimum Required Inputs</h2>
</p>

<p>Only the Topic input must be set when producing a message:

<br/><pre lang="">EXEC ProduceMessage Topic = 'mytopic'</pre>
</p>

<p>The rest of the properties have these default behaviors:

<ul><li>Partition: the native library determines the partition automatically, taking into account any relevant options from <u>ProducerProperties</u>.</li><li>KeyText and KeyBytes: if neither of these is given, the driver produces a message without a key.</li><li>MessageText and MessageBytes: if neither of these is given, the driver produces a message without any content.</li></ul>
</p>

<p>Omitting the key or message content may make the message unreadable with certain driver settings.
For example, setting <u>MessageKeyType</u> to integer may cause read failures with messages that have no keys.
<u>TypeDetectionScheme</u> MessageOnly and <u>MessageKeyType</u> Binary are recommended if you need to read messages containing arbitrary content or keys.


<h2>KeyText/KeyBytes and MessageText/MessageBytes</h2>
</p>

<p>The key and message can each be provided either as text or base64-encoded strings.
KeyBytes and MessageBytes accept base64-encoded strings.
The driver decodes these values into bytes before sending the message to Kafka. 
</p>

<p>KeyText and MessageText accept any text.
The driver encodes these values as UTF-8 before sending the message to Kafka.
</p>

<p>For example, this statement inserts a message with no key and content that is a single NUL byte:

<br/><pre lang="">EXEC ProduceMessage Topic = 'mytopic', MessageBytes = 'AA=='</pre>

    </p>

<p>      
<h3>Input</h3>

        <p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap">          <b>Name</b>
          </td><td><b>Type</b>
        
          </td><td><b>Required</b>
          </td><td><b>Description</b>
    </td></tr><tr><td style="white-space:nowrap">      Topic
          </td><td><i>String</i>
      
      </td><td><i>True</i>
     </td><td>        The topic that contains the message.
    </td></tr><tr><td style="white-space:nowrap">      Partition
          </td><td><i>String</i>
      
      </td><td><i>False</i>
     </td><td>        The partition the message is assigned to. Must be valid for the given topic. Automatically assigned by the native client if not set.
    </td></tr><tr><td style="white-space:nowrap">      KeyText
          </td><td><i>String</i>
      
      </td><td><i>False</i>
     </td><td>        The message given as text. The value is encoded as UTF-8 before being sent to Kafka. Do not set if KeyBytes is given.
    </td></tr><tr><td style="white-space:nowrap">      KeyBytes
          </td><td><i>String</i>
      
      </td><td><i>False</i>
     </td><td>        The message key given as a base64 encoded string. Do not set if KeyText is given.
    </td></tr><tr><td style="white-space:nowrap">      MessageText
          </td><td><i>String</i>
      
      </td><td><i>False</i>
     </td><td>        The message key given as text. The value is encoded as UTF-8 before being sent to Kafka. Do not set if MessageBytes is given.
    </td></tr><tr><td style="white-space:nowrap">      MessageBytes
          </td><td><i>String</i>
      
      </td><td><i>False</i>
     </td><td>        The message value given as a base64 encoded string. Do not set if MessageText is given.
          </td></tr></table></center><p />
          </p>

<p>      
<h3>Result Set Columns</h3>

        <p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap"><b>Name</b>        </td><td><b>Type</b> </td><td><b>Description</b>
    </td></tr><tr><td style="white-space:nowrap">      PartitionWritten
          </td><td><i>Int</i>
      </td><td>        The partition that the message was written to. Is the same as Partition if that parameter is given.
    </td></tr><tr><td style="white-space:nowrap">      OffsetWritten
          </td><td><i>Long</i>
      </td><td>        The position in the partition that the message was written to.
    </td></tr><tr><td style="white-space:nowrap">      TimestampWritten
          </td><td><i>Long</i>
      </td><td>        The Unix timestamp of the instant the message was committed to the partition.
    </td></tr><tr><td style="white-space:nowrap">      KeyWritten
          </td><td><i>String</i>
      </td><td>        The base64 data of the key that was written. Is NULL if neither Key nor KeyBytes was provided.
    </td></tr><tr><td style="white-space:nowrap">      MessageWritten
          </td><td><i>String</i>
      </td><td>        The base64 data of the value that was written. Is NULL if neither Message nor MessageBytes was provided.
    </td></tr><tr><td style="white-space:nowrap">      Success
          </td><td><i>Bool</i>
      </td><td>        Whether the operation was successful.
          </td></tr></table></center><p />
          </p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - RefreshOAuthAccessToken" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - RefreshOAuthAccessToken: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>RefreshOAuthAccessToken</h1>
       

    <div class="chapter_content" id="pg_sp-refreshoauthaccesstoken">

<p>Exchanges a access token for a new access token.
      
    </p>

<p>      
<h3>Input</h3>

        <p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap">          <b>Name</b>
          </td><td><b>Type</b>
        
          </td><td><b>Required</b>
          </td><td><b>Description</b>
    </td></tr><tr><td style="white-space:nowrap">      OAuthRefreshToken
          </td><td><i>String</i>
      
      </td><td><i>True</i>
     </td><td>        The refresh token returned from the original authorization code exchange.
          </td></tr></table></center><p />
          </p>

<p>      
<h3>Result Set Columns</h3>

        <p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap"><b>Name</b>        </td><td><b>Type</b> </td><td><b>Description</b>
    </td></tr><tr><td style="white-space:nowrap">      OAuthAccessToken
          </td><td><i>String</i>
      </td><td>        The access token used for communication with the API.
    </td></tr><tr><td style="white-space:nowrap">      OAuthRefreshToken
          </td><td><i>String</i>
      </td><td>        The refresh access token used to refresh your connection.
    </td></tr><tr><td style="white-space:nowrap">      ExpiresIn
          </td><td><i>String</i>
      </td><td>        The lifetime of the access token, in seconds.
          </td></tr></table></center><p />
          </p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka -  System Tables" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka -  System Tables: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1> System Tables</h1>
       

    <div class="chapter_content" id="pg_allsystables">

<p>You can query the system tables described in this section to access schema information, information on data source functionality, and batch operation statistics.

<h2>Schema Tables</h2>
</p>

<p>The following tables return database metadata for Apache Kafka:
<ul><li><a href="#pg_table-syscatalogs">sys_catalogs</a>: Lists the available databases.</li><li><a href="#pg_table-sysschemas">sys_schemas</a>: Lists the available schemas.</li><li><a href="#pg_table-systables">sys_tables</a>: Lists the available tables and views.</li><li><a href="#pg_table-systablecolumns">sys_tablecolumns</a>: Describes the columns of the available tables and views.</li><li><a href="#pg_table-sysprocedures">sys_procedures</a>: Describes the available stored procedures.</li><li><a href="#pg_table-sysprocedureparameters">sys_procedureparameters</a>: Describes stored procedure parameters.</li><li><a href="#pg_table-syskeycolumns">sys_keycolumns</a>: Describes the primary and foreign keys.</li><li><a href="#pg_table-sysindexes">sys_indexes</a>: Describes the available indexes.</li></ul>


<h2>Data Source Tables</h2>
</p>

<p>The following tables return information about how to connect to and query the data source:
<ul><li><a href="#pg_table-sysconnectionprops">sys_connection_props</a>: Returns information on the available connection properties.</li><li><a href="#pg_table-syssqlinfo">sys_sqlinfo</a>: Describes the SELECT queries that the driver can offload to the data source.</li></ul>

  
<h2>Query Information Tables</h2>

  </p>

<p>  The following table returns query statistics for data modification queries:
  <ul><li><a href="#pg_table-sysidentity">sys_identity</a>: Returns information about batch operations or single updates.
  </li></ul>

  </p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - sys_catalogs" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - sys_catalogs: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>sys_catalogs</h1>
       

    <div class="chapter_content" id="pg_table-syscatalogs">

<p>Lists the available databases.
</p>

<p>The following query retrieves all databases determined by the connection string:
<br/><pre lang="sql">SELECT * FROM sys_catalogs</pre>

          
<h2>Columns</h2>

          </p>

<p>        
        <p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap"><b>Name</b>       </td><td><b>Type</b> 
        </td><td><b>Description</b>
        </td></tr><tr><td style="white-space:nowrap">CatalogName
          </td><td><i>String</i>
        </td><td>The database name.
          </td></tr></table></center><p />


  </p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - sys_schemas" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - sys_schemas: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>sys_schemas</h1>
       

    <div class="chapter_content" id="pg_table-sysschemas">

<p>Lists the available schemas.
          </p>

<p>The following query retrieves all available schemas:	
          <br/><pre lang="sql">          SELECT * FROM sys_schemas
          </pre>



          
<h2>Columns</h2>

          </p>

<p>        
        <p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap"><b>Name</b>       </td><td><b>Type</b> 
        </td><td><b>Description</b>
        </td></tr><tr><td style="white-space:nowrap">CatalogName
          </td><td><i>String</i>
        </td><td>The database name.
        </td></tr><tr><td style="white-space:nowrap">SchemaName
          </td><td><i>String</i>
        </td><td>The schema name.
          </td></tr></table></center><p />

          
  </p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - sys_tables" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - sys_tables: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>sys_tables</h1>
       

    <div class="chapter_content" id="pg_table-systables">

<p>Lists the available tables.
</p>

<p>The following query retrieves the available tables and views:

          <br/><pre lang="sql">          SELECT * FROM sys_tables
          </pre>


          
<h2>Columns</h2>

          </p>

<p>        
        <p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap"><b>Name</b>       </td><td><b>Type</b> 
        </td><td><b>Description</b>
        </td></tr><tr><td style="white-space:nowrap">CatalogName
          </td><td><i>String</i>
        </td><td>The database containing the table or view.
        </td></tr><tr><td style="white-space:nowrap">SchemaName
          </td><td><i>String</i>
        </td><td>The schema containing the table or view.
        </td></tr><tr><td style="white-space:nowrap">TableName
          </td><td><i>String</i>
        </td><td>The name of the table or view.
        </td></tr><tr><td style="white-space:nowrap">TableType
          </td><td><i>String</i>
        </td><td>The table type (table or view).
        </td></tr><tr><td style="white-space:nowrap">Description
          </td><td><i>String</i>
        </td><td>A description of the table or view.
		</td></tr><tr><td style="white-space:nowrap">IsUpdateable
		  </td><td><i>Boolean</i>
		</td><td>Whether the table can be updated.		
          </td></tr></table></center><p />
</p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - sys_tablecolumns" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - sys_tablecolumns: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>sys_tablecolumns</h1>
       

    <div class="chapter_content" id="pg_table-systablecolumns">

<p>Describes the columns of the available tables and views.
 </p>

<p>The following query returns the columns and data types for the SampleTable_1 table:
<br/><pre lang="sql">SELECT ColumnName, DataTypeName FROM sys_tablecolumns WHERE TableName='SampleTable_1' </pre>

          
<h2>Columns</h2>

          </p>

<p>        
        <p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap"><b>Name</b>       </td><td><b>Type</b> 
        </td><td><b>Description</b>
        </td></tr><tr><td style="white-space:nowrap">CatalogName
          </td><td><i>String</i>
        </td><td>The name of the database containing the table or view.
        </td></tr><tr><td style="white-space:nowrap">SchemaName
          </td><td><i>String</i>
        </td><td>The schema containing the table or view.
        </td></tr><tr><td style="white-space:nowrap">TableName
          </td><td><i>String</i>
        </td><td>The name of the table or view containing the column.
        </td></tr><tr><td style="white-space:nowrap">ColumnName
          </td><td><i>String</i>
        </td><td>The column name.
        </td></tr><tr><td style="white-space:nowrap">DataTypeName
          </td><td><i>String</i>
        </td><td>The data type name.
        </td></tr><tr><td style="white-space:nowrap">DataType
          </td><td><i>Int32</i>
        </td><td>An integer indicating the data type. This value is determined at run time based on the environment.
        </td></tr><tr><td style="white-space:nowrap">Length
          </td><td><i>Int32</i>
        </td><td>The storage size of the column.
		</td></tr><tr><td style="white-space:nowrap">DisplaySize
          </td><td><i>Int32</i>
        </td><td>The designated column's normal maximum width in characters.
        </td></tr><tr><td style="white-space:nowrap">NumericPrecision
          </td><td><i>Int32</i>
        </td><td>The maximum number of digits in numeric data. The column length in characters for character and date-time data.  
        </td></tr><tr><td style="white-space:nowrap">NumericScale
          </td><td><i>Int32</i>
        </td><td>The column scale or number of digits to the right of the decimal point.
        </td></tr><tr><td style="white-space:nowrap">IsNullable
          </td><td><i>Boolean</i>
        </td><td>Whether the column can contain null.
        </td></tr><tr><td style="white-space:nowrap">Description
          </td><td><i>String</i>
        </td><td>A brief description of the column.
        </td></tr><tr><td style="white-space:nowrap">Ordinal
          </td><td><i>Int32</i>
        </td><td>The sequence number of the column.
        </td></tr><tr><td style="white-space:nowrap">IsAutoIncrement
          </td><td><i>String</i>
        </td><td>Whether the column value is assigned in fixed increments.
        </td></tr><tr><td style="white-space:nowrap">IsGeneratedColumn
          </td><td><i>String</i>
        </td><td>Whether the column is generated.
        </td></tr><tr><td style="white-space:nowrap">IsHidden
          </td><td><i>Boolean</i>
        </td><td>Whether the column is hidden.
		</td></tr><tr><td style="white-space:nowrap">IsArray
          </td><td><i>Boolean</i>
        </td><td>Whether the column is an array.
        </td></tr><tr><td style="white-space:nowrap">IsReadOnly
        </td><td><i>Boolean</i>
        </td><td>Whether the column is read-only. 
    </td></tr><tr><td style="white-space:nowrap">IsKey
        </td><td><i>Boolean</i>
        </td><td>Indicates whether a field returned from sys_tablecolumns is the primary key of the table.
          </td></tr></table></center><p />
</p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - sys_procedures" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - sys_procedures: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>sys_procedures</h1>
       

    <div class="chapter_content" id="pg_table-sysprocedures">

<p>Lists the available stored procedures.
</p>

<p>The following query retrieves the available stored procedures:

          <br/><pre lang="">          SELECT * FROM sys_procedures
          </pre>
          </p>

<p>          
<h2>Columns</h2>

          </p>

<p>        
        <p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap"><b>Name</b>       </td><td><b>Type</b> 
        </td><td><b>Description</b>
        </td></tr><tr><td style="white-space:nowrap">CatalogName
          </td><td><i>String</i>
        </td><td>The database containing the stored procedure.
        </td></tr><tr><td style="white-space:nowrap">SchemaName
          </td><td><i>String</i>
        </td><td>The schema containing the stored procedure.
        </td></tr><tr><td style="white-space:nowrap">ProcedureName
          </td><td><i>String</i>
        </td><td>The name of the stored procedure.
        </td></tr><tr><td style="white-space:nowrap">Description
          </td><td><i>String</i>
        </td><td>A description of the stored procedure.
		</td></tr><tr><td style="white-space:nowrap">ProcedureType
          </td><td><i>String</i>
        </td><td>The type of the procedure, such as PROCEDURE or FUNCTION.
          </td></tr></table></center><p />
</p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - sys_procedureparameters" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - sys_procedureparameters: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>sys_procedureparameters</h1>
       

    <div class="chapter_content" id="pg_table-sysprocedureparameters">

<p>Describes stored procedure parameters.
          </p>

<p>The following query returns information about all of the input parameters for the SampleProcedure stored procedure:
<br/><pre lang="sql">SELECT * FROM sys_procedureparameters WHERE ProcedureName='SampleProcedure' AND Direction=1 OR Direction=2</pre>

          
<h2>Columns</h2>

          </p>

<p>        
        <p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap"><b>Name</b>       </td><td><b>Type</b> 
        </td><td><b>Description</b>
        </td></tr><tr><td style="white-space:nowrap">CatalogName
          </td><td><i>String</i>
        </td><td>The name of the database containing the stored procedure.
        </td></tr><tr><td style="white-space:nowrap">SchemaName
          </td><td><i>String</i>
        </td><td>The name of the schema containing the stored procedure.
        </td></tr><tr><td style="white-space:nowrap">ProcedureName
          </td><td><i>String</i>
        </td><td>The name of the stored procedure containing the parameter.
        </td></tr><tr><td style="white-space:nowrap">ColumnName
          </td><td><i>String</i>
        </td><td>The name of the stored procedure parameter.
        </td></tr><tr><td style="white-space:nowrap">Direction
          </td><td><i>Int32</i>
        </td><td>An integer corresponding to the type of the parameter: input (1), input/output (2), or output(4). input/output type parameters can be both input and output parameters. 
        </td></tr><tr><td style="white-space:nowrap">DataTypeName
          </td><td><i>String</i>
        </td><td>The name of the data type.
        </td></tr><tr><td style="white-space:nowrap">DataType
          </td><td><i>Int32</i>
        </td><td>An integer indicating the data type. This value is determined at run time based on the environment.
        </td></tr><tr><td style="white-space:nowrap">Length
          </td><td><i>Int32</i>
        </td><td>The number of characters allowed for character data. The number of digits allowed for numeric data.
        </td></tr><tr><td style="white-space:nowrap">NumericPrecision
          </td><td><i>Int32</i>
        </td><td>The maximum precision for numeric data. The column length in characters for character and date-time data.
        </td></tr><tr><td style="white-space:nowrap">NumericScale
          </td><td><i>Int32</i>
        </td><td>The number of digits to the right of the decimal point in numeric data.
        </td></tr><tr><td style="white-space:nowrap">IsNullable
          </td><td><i>Boolean</i>
        </td><td>Whether the parameter can contain null.
		</td></tr><tr><td style="white-space:nowrap">IsRequired
          </td><td><i>Boolean</i>
        </td><td>Whether the parameter is required for execution of the procedure.
		</td></tr><tr><td style="white-space:nowrap">IsArray
          </td><td><i>Boolean</i>
        </td><td>Whether the parameter is an array.
        </td></tr><tr><td style="white-space:nowrap">Description
          </td><td><i>String</i>
        </td><td>The description of the parameter.
        </td></tr><tr><td style="white-space:nowrap">Ordinal
          </td><td><i>Int32</i>
        </td><td>The index of the parameter.
          </td></tr></table></center><p />
</p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - sys_keycolumns" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - sys_keycolumns: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>sys_keycolumns</h1>
       

    <div class="chapter_content" id="pg_table-syskeycolumns">

<p>Describes the primary and foreign keys.
          </p>

<p>		
          The following query retrieves the primary key for the SampleTable_1 table:
          <br/><pre lang="sql">         SELECT * FROM sys_keycolumns WHERE IsKey='True' AND TableName='SampleTable_1' 
          </pre>


          
<h2>Columns</h2>

          </p>

<p>        
        <p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap"><b>Name</b>       </td><td><b>Type</b> 
        </td><td><b>Description</b>
        </td></tr><tr><td style="white-space:nowrap">CatalogName
          </td><td><i>String</i>
        </td><td>The name of the database containing the key.
        </td></tr><tr><td style="white-space:nowrap">SchemaName
          </td><td><i>String</i>
        </td><td>The name of the schema containing the key.
        </td></tr><tr><td style="white-space:nowrap">TableName
          </td><td><i>String</i>
        </td><td>The name of the table containing the key.
        </td></tr><tr><td style="white-space:nowrap">ColumnName
          </td><td><i>String</i>
        </td><td>The name of the key column.
        </td></tr><tr><td style="white-space:nowrap">IsKey
          </td><td><i>Boolean</i>
        </td><td>Whether the column is a primary key in the table referenced in the TableName field.
        </td></tr><tr><td style="white-space:nowrap">IsForeignKey
          </td><td><i>Boolean</i>
        </td><td>Whether the column is a foreign key referenced in the TableName field.
         </td></tr><tr><td style="white-space:nowrap">PrimaryKeyName
          </td><td><i>String</i>
        </td><td>The name of the primary key.
         </td></tr><tr><td style="white-space:nowrap">ForeignKeyName
          </td><td><i>String</i>
        </td><td>The name of the foreign key.
        </td></tr><tr><td style="white-space:nowrap">ReferencedCatalogName
          </td><td><i>String</i>
        </td><td>The database containing the primary key.
        </td></tr><tr><td style="white-space:nowrap">ReferencedSchemaName
          </td><td><i>String</i>
        </td><td>The schema containing the primary key.
        </td></tr><tr><td style="white-space:nowrap">ReferencedTableName
          </td><td><i>String</i>
        </td><td>The table containing the primary key.
        </td></tr><tr><td style="white-space:nowrap">ReferencedColumnName
          </td><td><i>String</i>
        </td><td>The column name of the primary key.
          </td></tr></table></center><p />
</p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - sys_foreignkeys" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - sys_foreignkeys: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>sys_foreignkeys</h1>
       

    <div class="chapter_content" id="pg_table-foreignkeys">

<p>Describes the foreign keys.
          </p>

<p>		
          The following query retrieves all foreign keys which refer to other tables:
          <br/><pre lang="sql">         SELECT * FROM sys_foreignkeys WHERE ForeignKeyType = 'FOREIGNKEY_TYPE_IMPORT'
          </pre>


          
<h2>Columns</h2>

          </p>

<p>        
        <p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap"><b>Name</b>       </td><td><b>Type</b> 
        </td><td><b>Description</b>
        </td></tr><tr><td style="white-space:nowrap">CatalogName
          </td><td><i>String</i>
        </td><td>The name of the database containing the key.
        </td></tr><tr><td style="white-space:nowrap">SchemaName
          </td><td><i>String</i>
        </td><td>The name of the schema containing the key.
        </td></tr><tr><td style="white-space:nowrap">TableName
          </td><td><i>String</i>
        </td><td>The name of the table containing the key.
        </td></tr><tr><td style="white-space:nowrap">ColumnName
          </td><td><i>String</i>
        </td><td>The name of the key column.
		  </td></tr><tr><td style="white-space:nowrap">PrimaryKeyName
          </td><td><i>String</i>
        </td><td>The name of the primary key.
		</td></tr><tr><td style="white-space:nowrap">ForeignKeyName
          </td><td><i>String</i>
        </td><td>The name of the foreign key.       
        </td></tr><tr><td style="white-space:nowrap">ReferencedCatalogName
          </td><td><i>String</i>
        </td><td>The database containing the primary key.
        </td></tr><tr><td style="white-space:nowrap">ReferencedSchemaName
          </td><td><i>String</i>
        </td><td>The schema containing the primary key.
        </td></tr><tr><td style="white-space:nowrap">ReferencedTableName
          </td><td><i>String</i>
        </td><td>The table containing the primary key.
        </td></tr><tr><td style="white-space:nowrap">ReferencedColumnName
          </td><td><i>String</i>
        </td><td>The column name of the primary key.
		</td></tr><tr><td style="white-space:nowrap">ForeignKeyType
		   </td><td><i>String</i>
		</td><td>Designates whether the foreign key is an import (points to other tables) or export (referenced from other tables) key.
          </td></tr></table></center><p />
</p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - sys_primarykeys" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - sys_primarykeys: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>sys_primarykeys</h1>
       

    <div class="chapter_content" id="pg_table-primarykeys">

<p>Describes the primary keys.
          </p>

<p>		
          The following query retrieves the primary keys from all tables and views:
          <br/><pre lang="sql">         SELECT * FROM sys_primarykeys
          </pre>


          
<h2>Columns</h2>

          </p>

<p>        
        <p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap"><b>Name</b>       </td><td><b>Type</b> 
        </td><td><b>Description</b>
        </td></tr><tr><td style="white-space:nowrap">CatalogName
          </td><td><i>String</i>
        </td><td>The name of the database containing the key.
        </td></tr><tr><td style="white-space:nowrap">SchemaName
          </td><td><i>String</i>
        </td><td>The name of the schema containing the key.
        </td></tr><tr><td style="white-space:nowrap">TableName
          </td><td><i>String</i>
        </td><td>The name of the table containing the key.
        </td></tr><tr><td style="white-space:nowrap">ColumnName
          </td><td><i>String</i>
        </td><td>The name of the key column.
        </td></tr><tr><td style="white-space:nowrap">KeySeq
          </td><td><i>String</i>
        </td><td>The sequence number of the primary key.
		  </td></tr><tr><td style="white-space:nowrap">KeyName
          </td><td><i>String</i>
        </td><td>The name of the primary key.
		</td></tr></table></center><p />
</p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - sys_indexes" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - sys_indexes: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>sys_indexes</h1>
       

    <div class="chapter_content" id="pg_table-sysindexes">

<p>Describes the available indexes. By filtering on indexes, you can write more selective queries with faster query response times.
</p>

<p>The following query retrieves all indexes that are not primary keys:
          <br/><pre lang="sql">          SELECT * FROM sys_indexes WHERE IsPrimary='false'
          </pre>

          
<h2>Columns</h2>

          </p>

<p>        
        <p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap"><b>Name</b>       </td><td><b>Type</b> 
        </td><td><b>Description</b>
        </td></tr><tr><td style="white-space:nowrap">CatalogName
          </td><td><i>String</i>
        </td><td>The name of the database containing the index.
        </td></tr><tr><td style="white-space:nowrap">SchemaName
          </td><td><i>String</i>
        </td><td>The name of the schema containing the index.
        </td></tr><tr><td style="white-space:nowrap">TableName
          </td><td><i>String</i>
        </td><td>The name of the table containing the index.
        </td></tr><tr><td style="white-space:nowrap">IndexName
          </td><td><i>String</i>
        </td><td>The index name.
        </td></tr><tr><td style="white-space:nowrap">ColumnName
          </td><td><i>String</i>
        </td><td>The name of the column associated with the index.
        </td></tr><tr><td style="white-space:nowrap">IsUnique
          </td><td><i>Boolean</i>
        </td><td>True if the index is unique. False otherwise.
        </td></tr><tr><td style="white-space:nowrap">IsPrimary
          </td><td><i>Boolean</i>
        </td><td>True if the index is a primary key. False otherwise.
        </td></tr><tr><td style="white-space:nowrap">Type
          </td><td><i>Int16</i>
        </td><td>An integer value corresponding to the index type: statistic (0), clustered (1), hashed (2), or other (3).
        </td></tr><tr><td style="white-space:nowrap">SortOrder
          </td><td><i>String</i>
        </td><td>The sort order: A for ascending or D for descending.
        </td></tr><tr><td style="white-space:nowrap">OrdinalPosition
          </td><td><i>Int16</i>
        </td><td>The sequence number of the column in the index.
          </td></tr></table></center><p />
</p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - sys_connection_props" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - sys_connection_props: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>sys_connection_props</h1>
       

    <div class="chapter_content" id="pg_table-sysconnectionprops">

<p>Returns information on the available connection properties and those set in the connection string.
</p>

<p>When querying this table, the config connection string should be used: 
<br/><pre lang="sql">jdbc:cdata:apachekafka:config:</pre></p>

<p>This connection string enables you to query this table without a valid connection.
   </p>

<p>The following query retrieves all connection properties that have been set in the connection string or set through a default value:
<br/><pre lang="sql">SELECT * FROM sys_connection_props WHERE Value &lt;&gt; ''</pre>

          
<h2>Columns</h2>

          </p>

<p>        
        <p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap"><b>Name</b>       </td><td><b>Type</b> 
        </td><td><b>Description</b>
        </td></tr><tr><td style="white-space:nowrap">Name
          </td><td><i>String</i>
        </td><td>The name of the connection property.
        </td></tr><tr><td style="white-space:nowrap">ShortDescription
          </td><td><i>String</i>
        </td><td>A brief description.
        </td></tr><tr><td style="white-space:nowrap">Type
          </td><td><i>String</i>
        </td><td>The data type of the connection property.
        </td></tr><tr><td style="white-space:nowrap">Default
          </td><td><i>String</i>
        </td><td>The default value if one is not explicitly set.
        </td></tr><tr><td style="white-space:nowrap">Values
          </td><td><i>String</i>
        </td><td>A comma-separated list of possible values. A validation error is thrown if another value is specified.
        </td></tr><tr><td style="white-space:nowrap">Value
          </td><td><i>String</i>
        </td><td>The value you set or a preconfigured default.
        </td></tr><tr><td style="white-space:nowrap">Required
          </td><td><i>Boolean</i>
        </td><td>Whether the property is required to connect.
        </td></tr><tr><td style="white-space:nowrap">Category
          </td><td><i>String</i>
        </td><td>The category of the connection property. 
        </td></tr><tr><td style="white-space:nowrap">IsSessionProperty
          </td><td><i>String</i>
        </td><td>Whether the property is a session property, used to save information about the current connection.
		</td></tr><tr><td style="white-space:nowrap">Sensitivity
          </td><td><i>String</i>
        </td><td>The sensitivity level of the property. This informs whether the property is obfuscated in logging and authentication forms.
		</td></tr><tr><td style="white-space:nowrap">PropertyName
          </td><td><i>String</i>
        </td><td>A camel-cased truncated form of the connection property name.
		</td></tr><tr><td style="white-space:nowrap">Ordinal
          </td><td><i>Int32</i>
        </td><td>The index of the parameter.
		</td></tr><tr><td style="white-space:nowrap">CatOrdinal
          </td><td><i>Int32</i>
        </td><td>The index of the parameter category.
		</td></tr><tr><td style="white-space:nowrap">Hierarchy
          </td><td><i>String</i>
        </td><td>Shows dependent properties associated that need to be set alongside this one.
		</td></tr><tr><td style="white-space:nowrap">Visible
          </td><td><i>Boolean</i>
        </td><td>Informs whether the property is visible in the connection UI.
		</td></tr><tr><td style="white-space:nowrap">ETC
          </td><td><i>String</i>
        </td><td>Various miscellaneous information about the property.
          </td></tr></table></center><p />
</p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - sys_sqlinfo" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - sys_sqlinfo: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>sys_sqlinfo</h1>
       

    <div class="chapter_content" id="pg_table-syssqlinfo">

<p>Describes the SELECT query processing that the driver can offload to the data source. 
          </p>

<p>		
  See <a href="#pg_overview">SQL Compliance</a> for SQL syntax details.

<h2>Discovering the Data Source's SELECT Capabilities</h2>
</p>

<p>Below is an example data set of SQL capabilities.
 Some aspects of SELECT functionality are returned in a comma-separated list if supported; otherwise, the column contains NO. </p>

<p>
<p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap"><b>Name</b></td><td>Description</td><td><b>Possible Values</b> </td></tr><tr><td style="white-space:nowrap">AGGREGATE_FUNCTIONS</td><td>Supported aggregation functions.</td><td>AVG, COUNT, MAX, MIN, SUM, DISTINCT</td></tr><tr><td style="white-space:nowrap">COUNT</td><td>Whether COUNT function is supported.</td><td>YES, NO</td></tr><tr><td style="white-space:nowrap">IDENTIFIER_QUOTE_OPEN_CHAR</td><td>The opening character used to escape an identifier.</td><td>[</td></tr><tr><td style="white-space:nowrap">IDENTIFIER_QUOTE_CLOSE_CHAR</td><td>The closing character used to escape an identifier.</td><td>]</td></tr><tr><td style="white-space:nowrap">SUPPORTED_OPERATORS</td><td>A list of supported SQL operators.</td><td>=, &gt;, &lt;, &gt;=, &lt;=, &lt;&gt;, !=, LIKE, NOT LIKE, IN, NOT IN, IS NULL, IS NOT NULL, AND, OR</td></tr><tr><td style="white-space:nowrap">GROUP_BY</td><td>Whether GROUP BY is supported, and, if so, the degree of support.</td><td>NO, NO_RELATION, EQUALS_SELECT, SQL_GB_COLLATE</td></tr><tr><td style="white-space:nowrap">OJ_CAPABILITIES</td><td>The supported varieties of outer joins supported.</td><td>NO, LEFT, RIGHT, FULL, INNER, NOT_ORDERED, ALL_COMPARISON_OPS</td></tr><tr><td style="white-space:nowrap">OUTER_JOINS</td><td>Whether outer joins are supported.</td><td>YES, NO</td></tr><tr><td style="white-space:nowrap">SUBQUERIES</td><td>Whether subqueries are supported, and, if so, the degree of support.</td><td>NO, COMPARISON, EXISTS, IN, CORRELATED_SUBQUERIES, QUANTIFIED</td></tr><tr><td style="white-space:nowrap">STRING_FUNCTIONS</td><td>Supported string functions.</td><td>LENGTH, CHAR, LOCATE, REPLACE, SUBSTRING, RTRIM, LTRIM, RIGHT, LEFT, UCASE, SPACE, SOUNDEX, LCASE, CONCAT, ASCII, REPEAT, OCTET, BIT, POSITION, INSERT, TRIM, UPPER, REGEXP, LOWER, DIFFERENCE, CHARACTER, SUBSTR, STR, REVERSE, PLAN, UUIDTOSTR, TRANSLATE,  TRAILING, TO, STUFF, STRTOUUID, STRING, SPLIT, SORTKEY, SIMILAR, REPLICATE, PATINDEX, LPAD, LEN, LEADING, KEY, INSTR, INSERTSTR, HTML, GRAPHICAL, CONVERT, COLLATION, CHARINDEX, BYTE</td></tr><tr><td style="white-space:nowrap">NUMERIC_FUNCTIONS</td><td>Supported numeric functions.</td><td>ABS, ACOS, ASIN, ATAN, ATAN2, CEILING, COS, COT, EXP, FLOOR, LOG, MOD, SIGN, SIN, SQRT, TAN, PI, RAND, DEGREES, LOG10, POWER, RADIANS, ROUND, TRUNCATE</td></tr><tr><td style="white-space:nowrap">TIMEDATE_FUNCTIONS</td><td>Supported date/time functions.</td><td>NOW, CURDATE, DAYOFMONTH, DAYOFWEEK, DAYOFYEAR, MONTH, QUARTER, WEEK, YEAR, CURTIME, HOUR, MINUTE, SECOND, TIMESTAMPADD, TIMESTAMPDIFF, DAYNAME, MONTHNAME, CURRENT_DATE, CURRENT_TIME, CURRENT_TIMESTAMP, EXTRACT</td></tr><tr><td style="white-space:nowrap">REPLICATION_SKIP_TABLES</td><td>Indicates tables skipped during replication.</td><td></td></tr><tr><td style="white-space:nowrap">REPLICATION_TIMECHECK_COLUMNS</td><td>A string array containing a list of columns which will be used to check for (in the given order) to use as a modified column during replication.</td><td></td></tr><tr><td style="white-space:nowrap">IDENTIFIER_PATTERN</td><td>String value indicating what string is valid for an identifier.</td><td></td></tr><tr><td style="white-space:nowrap">SUPPORT_TRANSACTION</td><td>Indicates if the provider supports transactions such as commit and rollback.</td><td>YES, NO</td></tr><tr><td style="white-space:nowrap">DIALECT</td><td>Indicates the SQL dialect to use.</td><td></td></tr><tr><td style="white-space:nowrap">KEY_PROPERTIES</td><td>Indicates the properties which identify the uniform database.</td><td></td></tr><tr><td style="white-space:nowrap">SUPPORTS_MULTIPLE_SCHEMAS</td><td>Indicates if multiple schemas may exist for the provider.</td><td>YES, NO</td></tr><tr><td style="white-space:nowrap">SUPPORTS_MULTIPLE_CATALOGS</td><td>Indicates if multiple catalogs may exist for the provider.</td><td>YES, NO</td></tr><tr><td style="white-space:nowrap">DATASYNCVERSION</td><td>The CData Data Sync version needed to access this driver.</td><td>Standard, Starter, Professional, Enterprise</td></tr><tr><td style="white-space:nowrap">DATASYNCCATEGORY</td><td>The CData Data Sync category of this driver.</td><td>Source, Destination, Cloud Destination</td></tr><tr><td style="white-space:nowrap">SUPPORTSENHANCEDSQL</td><td>Whether enhanced SQL functionality beyond what is offered by the API is supported.</td><td>TRUE, FALSE</td></tr><tr><td style="white-space:nowrap">SUPPORTS_BATCH_OPERATIONS</td><td>Whether batch operations are supported.</td><td>YES, NO</td></tr><tr><td style="white-space:nowrap">SQL_CAP</td><td>All supported SQL capabilities for this driver.</td><td>SELECT, INSERT, DELETE, UPDATE, TRANSACTIONS, ORDERBY, OAUTH, ASSIGNEDID, LIMIT, LIKE, BULKINSERT, COUNT, BULKDELETE, BULKUPDATE, GROUPBY, HAVING, AGGS, OFFSET, REPLICATE, COUNTDISTINCT, JOINS,  DROP, CREATE, DISTINCT, INNERJOINS, SUBQUERIES, ALTER, MULTIPLESCHEMAS, GROUPBYNORELATION, OUTERJOINS, UNIONALL, UNION, UPSERT, GETDELETED, CROSSJOINS, GROUPBYCOLLATE, MULTIPLECATS, FULLOUTERJOIN, MERGE, JSONEXTRACT, BULKUPSERT, SUM, SUBQUERIESFULL, MIN, MAX, JOINSFULL, XMLEXTRACT, AVG, MULTISTATEMENTS, FOREIGNKEYS, CASE, LEFTJOINS, COMMAJOINS, WITH, LITERALS, RENAME, NESTEDTABLES, EXECUTE, BATCH, BASIC, INDEX</td></tr><tr><td style="white-space:nowrap">PREFERRED_CACHE_OPTIONS</td><td>A string value specifies the preferred cacheOptions.</td><td></td></tr><tr><td style="white-space:nowrap">ENABLE_EF_ADVANCED_QUERY</td><td>Indicates if the driver directly supports advanced queries coming from Entity Framework. If not, queries will be handled client side.</td><td>YES, NO</td></tr><tr><td style="white-space:nowrap">PSEUDO_COLUMNS</td><td>A string array indicating the available pseudo columns.</td><td></td></tr><tr><td style="white-space:nowrap">MERGE_ALWAYS</td><td>If the value is true, The Merge Mode is forcibly executed in Data Sync.</td><td>TRUE, FALSE</td></tr><tr><td style="white-space:nowrap">REPLICATION_MIN_DATE_QUERY</td><td>A select query to return the replicate start datetime.</td><td></td></tr><tr><td style="white-space:nowrap">REPLICATION_MIN_FUNCTION</td><td>Allows a provider to specify the formula name to use for executing a server side min.</td><td></td></tr><tr><td style="white-space:nowrap">REPLICATION_START_DATE</td><td>Allows a provider to specify a replicate startdate.</td><td></td></tr><tr><td style="white-space:nowrap">REPLICATION_MAX_DATE_QUERY</td><td>A select query to return the replicate end datetime.</td><td></td></tr><tr><td style="white-space:nowrap">REPLICATION_MAX_FUNCTION</td><td>Allows a provider to specify the formula name to use for executing a server side max.</td><td></td></tr><tr><td style="white-space:nowrap">IGNORE_INTERVALS_ON_INITIAL_REPLICATE</td><td>A list of tables which will skip dividing the replicate into chunks on the initial replicate.</td><td></td></tr><tr><td style="white-space:nowrap">CHECKCACHE_USE_PARENTID</td><td>Indicates whether the CheckCache statement should be done against the parent key column.</td><td>TRUE, FALSE</td></tr><tr><td style="white-space:nowrap">CREATE_SCHEMA_PROCEDURES</td><td>Indicates stored procedures that can be used for generating schema files.</td><td></td></tr></table></center><p /></p>

<p>The following query retrieves the operators that can be used in the WHERE clause:
<br/><pre lang="sql">SELECT * FROM sys_sqlinfo WHERE Name = 'SUPPORTED_OPERATORS'</pre>
Note that individual tables may have different limitations or requirements on the WHERE clause; refer to the <a href="#pg_datamodel">Data Model</a> section for more information.

          
<h2>Columns</h2>

          </p>

<p>        
        <p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap"><b>Name</b>       </td><td><b>Type</b> 
        </td><td><b>Description</b>
        </td></tr><tr><td style="white-space:nowrap">NAME
          </td><td><i>String</i>
        </td><td>A component of SQL syntax, or a capability that can be processed on the server.
        </td></tr><tr><td style="white-space:nowrap">VALUE
          </td><td><i>String</i>
        </td><td>Detail on the supported SQL or SQL syntax.
          </td></tr></table></center><p />
</p>

    </div>



  

  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - sys_identity" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - sys_identity: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>sys_identity</h1>
       

    <div class="chapter_content" id="pg_table-sysidentity">

<p>Returns information about attempted modifications.</p>

<p>The following query retrieves the Ids of the modified rows in a batch operation: 
          <br/><pre lang="sql">         SELECT * FROM sys_identity
          </pre>

          
<h2>Columns</h2>

          </p>

<p>        
        <p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap"><b>Name</b>       </td><td><b>Type</b> 
        </td><td><b>Description</b>
        </td></tr><tr><td style="white-space:nowrap">Id
          </td><td><i>String</i>
        </td><td>The database-generated Id returned from a data modification operation.
        </td></tr><tr><td style="white-space:nowrap">Batch
          </td><td><i>String</i>
        </td><td>An identifier for the batch. 1 for a single operation.
        </td></tr><tr><td style="white-space:nowrap">Operation
          </td><td><i>String</i>
        </td><td>The result of the operation in the batch: INSERTED, UPDATED, or DELETED.
        </td></tr><tr><td style="white-space:nowrap">Message
          </td><td><i>String</i>
        </td><td>SUCCESS or an error message if the update in the batch failed.
          </td></tr></table></center><p />

</p>

    </div>





  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - Connection String Options" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - Connection String Options: The connection string properties describe the various options that can be used to establish a connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>Connection String Options</h1>
       


<div class="chapter_content" id="Connection">
<p>The connection string properties are the various options that can be used to establish a connection. This section provides a complete list of the options you can configure in the connection string for this provider. Click the links for further details.</p>



For more information on establishing a connection, see <a href="#pg_connectionj">Establishing a Connection</a>.












<p>
<a href="#RSBApacheKafka_c_Authentication"><h2>Authentication</h2></a>
<p>
<hr/>
<center>
<table><tr><td><b>Property</b></td><td><b>Description</b></td></tr>
<tr valign="top"><td><a href="#RSBApacheKafka_p_AuthScheme">AuthScheme</a></td><td>The scheme used for authentication with the Apache Kafka broker.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_User">User</a></td><td>The user who is authenticating to Apache Kafka.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_Password">Password</a></td><td>The password used to authenticate to Apache Kafka.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_BootstrapServers">BootstrapServers</a></td><td>The address of the Apache Kafka BootstrapServers to which you are connecting to.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_Topic">Topic</a></td><td>The topic used for read and write operations.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_UseSSL">UseSSL</a></td><td>This field sets whether SSL is enabled. Automatically enabled if AuthScheme is set to SSL.</td></tr>
</table></center><p>
<a href="#RSBApacheKafka_c_Connection"><h2>Connection</h2></a>
<p>
<hr/>
<center>
<table><tr><td><b>Property</b></td><td><b>Description</b></td></tr>
<tr valign="top"><td><a href="#RSBApacheKafka_p_ConsumerGroupId">ConsumerGroupId</a></td><td>Specifies which group the consumers created by the driver should belong to.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_AutoCommit">AutoCommit</a></td><td>Specifies if the Apache Kafka consumer should autocommit after each poll.</td></tr>
</table></center><p>
<a href="#RSBApacheKafka_c_AzureAuthentication"><h2>Azure Authentication</h2></a>
<p>
<hr/>
<center>
<table><tr><td><b>Property</b></td><td><b>Description</b></td></tr>
<tr valign="top"><td><a href="#RSBApacheKafka_p_AzureTenant">AzureTenant</a></td><td>The Microsoft Online tenant being used to access data. If not specified, your default tenant is used.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_AzureResource">AzureResource</a></td><td>The Azure Active resource to authenticate to (used during Azure OAuth exchange).</td></tr>
</table></center><p>
<a href="#RSBApacheKafka_c_OAuth"><h2>OAuth</h2></a>
<p>
<hr/>
<center>
<table><tr><td><b>Property</b></td><td><b>Description</b></td></tr>
<tr valign="top"><td><a href="#RSBApacheKafka_p_InitiateOAuth">InitiateOAuth</a></td><td>Set this property to initiate the process to obtain or refresh the OAuth access token when you connect.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_OAuthClientId">OAuthClientId</a></td><td>The client Id assigned when you register your application with an OAuth authorization server.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_OAuthClientSecret">OAuthClientSecret</a></td><td>The client secret assigned when you register your application with an OAuth authorization server.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_OAuthAccessToken">OAuthAccessToken</a></td><td>The access token for connecting using OAuth.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_OAuthSettingsLocation">OAuthSettingsLocation</a></td><td>The location of the settings file where OAuth values are saved when InitiateOAuth is set to GETANDREFRESH or REFRESH . Alternatively, you can hold this location in memory by specifying a value starting with 'memory://'.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_OAuthVerifier">OAuthVerifier</a></td><td>The verifier code returned from the OAuth authorization URL.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_OAuthRefreshToken">OAuthRefreshToken</a></td><td>The OAuth refresh token for the corresponding OAuth access token.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_OAuthExpiresIn">OAuthExpiresIn</a></td><td>The lifetime in seconds of the OAuth AccessToken.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_OAuthTokenTimestamp">OAuthTokenTimestamp</a></td><td>The Unix epoch timestamp in milliseconds when the current Access Token was created.</td></tr>
</table></center><p>
<a href="#RSBApacheKafka_c_Kerberos"><h2>Kerberos</h2></a>
<p>
<hr/>
<center>
<table><tr><td><b>Property</b></td><td><b>Description</b></td></tr>
<tr valign="top"><td><a href="#RSBApacheKafka_p_KerberosKeytabFile">KerberosKeytabFile</a></td><td>The Keytab file containing your pairs of Kerberos principals and encrypted keys.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_KerberosSPN">KerberosSPN</a></td><td>The service principal name (SPN) for the Kerberos Domain Controller.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_KerberosServiceName">KerberosServiceName</a></td><td>The name of the Kerberos service you want to authenticate with.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_UseKerberosTicketCache">UseKerberosTicketCache</a></td><td>Set this to use a ticket cache with the logged in user instead of a keytab file.</td></tr>
</table></center><p>
<a href="#RSBApacheKafka_c_SSL"><h2>SSL</h2></a>
<p>
<hr/>
<center>
<table><tr><td><b>Property</b></td><td><b>Description</b></td></tr>
<tr valign="top"><td><a href="#RSBApacheKafka_p_SSLServerCert">SSLServerCert</a></td><td>The SSL server certificate used to validate to the Apache Kafka broker.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_SSLServerCertType">SSLServerCertType</a></td><td>The format of the SSL server certificate used to verify the Apache Kafka broker.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_SSLServerCertPassword">SSLServerCertPassword</a></td><td>The password used to decrypt the certificate in SSLServerCert .</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_SSLClientCert">SSLClientCert</a></td><td>The SSL client certificate used to connect to the Apache Kafka broker.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_SSLClientCertType">SSLClientCertType</a></td><td>The format of the SSL client certificate used to connect to the Apache Kafka broker.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_SSLClientCertPassword">SSLClientCertPassword</a></td><td>The password used to decrypt the certificate in SSLClientCert .</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_SSLIdentificationAlgorithm">SSLIdentificationAlgorithm</a></td><td>The endpoint identification algorithm used by the Apache Kafka data provider client app to validate server host name.</td></tr>
</table></center><p>
<a href="#RSBApacheKafka_c_SchemaRegistry"><h2>Schema Registry</h2></a>
<p>
<hr/>
<center>
<table><tr><td><b>Property</b></td><td><b>Description</b></td></tr>
<tr valign="top"><td><a href="#RSBApacheKafka_p_RegistryUrl">RegistryUrl</a></td><td>The server for the schema registry. When this property is specified, the driver will read Apache Avro schema from the server.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_RegistryType">RegistryType</a></td><td>Type of the schema specified for the a specific topic.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_RegistryService">RegistryService</a></td><td>The Schema Registry service used for working with topic schemas.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_RegistryAuthScheme">RegistryAuthScheme</a></td><td>The scheme used to authenticate to the schema registry.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_RegistryUser">RegistryUser</a></td><td>Username to authorize with the server specified in RegistryUrl .</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_RegistryPassword">RegistryPassword</a></td><td>Password to authorize with the server specified in RegistryUrl .</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_RegistryClientCert">RegistryClientCert</a></td><td>The TLS/SSL client certificate store for SSL Client Authentication (2-way SSL) with the schema registry.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_RegistryClientCertType">RegistryClientCertType</a></td><td>The type of key store used by the TLS/SSL client certificate given in RegistryClientCert .</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_RegistryClientCertPassword">RegistryClientCertPassword</a></td><td>The password for the TLS/SSL client certificate given in RegistryClientCert .</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_RegistryClientCertSubject">RegistryClientCertSubject</a></td><td>The subject of the TLS/SSL client certificate given in RegistryClientCert .</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_RegistryVersion">RegistryVersion</a></td><td>Version of the schema read from RegistryUrl for the specified topic.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_RegistryServerCert">RegistryServerCert</a></td><td>The certificate to be accepted from the schema registry when connecting using TLS/SSL.</td></tr>
</table></center><p>
<a href="#RSBApacheKafka_c_Firewall"><h2>Firewall</h2></a>
<p>
<hr/>
<center>
<table><tr><td><b>Property</b></td><td><b>Description</b></td></tr>
<tr valign="top"><td><a href="#RSBApacheKafka_p_FirewallType">FirewallType</a></td><td>The protocol used by a proxy-based firewall.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_FirewallServer">FirewallServer</a></td><td>The name or IP address of a proxy-based firewall.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_FirewallPort">FirewallPort</a></td><td>The TCP port for a proxy-based firewall.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_FirewallUser">FirewallUser</a></td><td>The user name to use to authenticate with a proxy-based firewall.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_FirewallPassword">FirewallPassword</a></td><td>A password used to authenticate to a proxy-based firewall.</td></tr>
</table></center><p>
<a href="#RSBApacheKafka_c_Proxy"><h2>Proxy</h2></a>
<p>
<hr/>
<center>
<table><tr><td><b>Property</b></td><td><b>Description</b></td></tr>
<tr valign="top"><td><a href="#RSBApacheKafka_p_ProxyAutoDetect">ProxyAutoDetect</a></td><td>This indicates whether to use the system proxy settings or not. This takes precedence over other proxy settings, so you'll need to set ProxyAutoDetect to FALSE in order use custom proxy settings.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_ProxyServer">ProxyServer</a></td><td>The hostname or IP address of a proxy to route HTTP traffic through.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_ProxyPort">ProxyPort</a></td><td>The TCP port the ProxyServer proxy is running on.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_ProxyAuthScheme">ProxyAuthScheme</a></td><td>The authentication type to use to authenticate to the ProxyServer proxy.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_ProxyUser">ProxyUser</a></td><td>A user name to be used to authenticate to the ProxyServer proxy.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_ProxyPassword">ProxyPassword</a></td><td>A password to be used to authenticate to the ProxyServer proxy.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_ProxySSLType">ProxySSLType</a></td><td>The SSL type to use when connecting to the ProxyServer proxy.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_ProxyExceptions">ProxyExceptions</a></td><td>A semicolon separated list of destination hostnames or IPs that are exempt from connecting through the ProxyServer .</td></tr>
</table></center><p>
<a href="#RSBApacheKafka_c_Logging"><h2>Logging</h2></a>
<p>
<hr/>
<center>
<table><tr><td><b>Property</b></td><td><b>Description</b></td></tr>
<tr valign="top"><td><a href="#RSBApacheKafka_p_Logfile">Logfile</a></td><td>A filepath which designates the name and location of the log file.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_Verbosity">Verbosity</a></td><td>The verbosity level that determines the amount of detail included in the log file.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_LogModules">LogModules</a></td><td>Core modules to be included in the log file.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_MaxLogFileSize">MaxLogFileSize</a></td><td>A string specifying the maximum size in bytes for a log file (for example, 10 MB).</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_MaxLogFileCount">MaxLogFileCount</a></td><td>A string specifying the maximum file count of log files.</td></tr>
</table></center><p>
<a href="#RSBApacheKafka_c_Schema"><h2>Schema</h2></a>
<p>
<hr/>
<center>
<table><tr><td><b>Property</b></td><td><b>Description</b></td></tr>
<tr valign="top"><td><a href="#RSBApacheKafka_p_Location">Location</a></td><td>A path to the directory that contains the schema files defining tables, views, and stored procedures.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_BrowsableSchemas">BrowsableSchemas</a></td><td>This property restricts the schemas reported to a subset of the available schemas. For example, BrowsableSchemas=SchemaA,SchemaB,SchemaC.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_Tables">Tables</a></td><td>This property restricts the tables reported to a subset of the available tables. For example, Tables=TableA,TableB,TableC.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_Views">Views</a></td><td>Restricts the views reported to a subset of the available tables. For example, Views=ViewA,ViewB,ViewC.</td></tr>
</table></center><p>
<a href="#RSBApacheKafka_c_Caching"><h2>Caching</h2></a>
<p>
<hr/>
<center>
<table><tr><td><b>Property</b></td><td><b>Description</b></td></tr>
<tr valign="top"><td><a href="#RSBApacheKafka_p_AutoCache">AutoCache</a></td><td>Automatically caches the results of SELECT queries into a cache database specified by either CacheLocation or both of CacheConnection and CacheProvider .</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_CacheDriver">CacheDriver</a></td><td>The database driver used to cache data.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_CacheConnection">CacheConnection</a></td><td>The connection string for the cache database. This property is always used in conjunction with CacheProvider . Setting both properties will override the value set for CacheLocation for caching data.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_CacheLocation">CacheLocation</a></td><td>Specifies the path to the cache when caching to a file.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_CacheTolerance">CacheTolerance</a></td><td>The tolerance for stale data in the cache specified in seconds when using AutoCache .</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_Offline">Offline</a></td><td>Use offline mode to get the data from the cache instead of the live source.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_CacheMetadata">CacheMetadata</a></td><td>This property determines whether or not to cache the table metadata to a file store.</td></tr>
</table></center><p>
<a href="#RSBApacheKafka_c_Miscellaneous"><h2>Miscellaneous</h2></a>
<p>
<hr/>
<center>
<table><tr><td><b>Property</b></td><td><b>Description</b></td></tr>
<tr valign="top"><td><a href="#RSBApacheKafka_p_AggregateMessages">AggregateMessages</a></td><td>Specifies whether or not to return the message as a whole string.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_BatchSize">BatchSize</a></td><td>The maximum size of each batch operation to submit.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_CompressionType">CompressionType</a></td><td>Data compression type. Batches of data will be compressed together.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_ConnectionLifeTime">ConnectionLifeTime</a></td><td>The maximum lifetime of a connection in seconds. Once the time has elapsed, the connection object is disposed.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_ConnectOnOpen">ConnectOnOpen</a></td><td>This property specifies whether to connect to the Apache Kafka when the connection is opened.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_ConsumerProperties">ConsumerProperties</a></td><td>Additional options used to configure Kafka consumers.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_CreateTablePartitions">CreateTablePartitions</a></td><td>The number of partitions assigned to a topic created with CREATE TABLE.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_CreateTableReplicationFactor">CreateTableReplicationFactor</a></td><td>The number of replicas assigned to a topic created with CREATE TABLE.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_EnableIdempotence">EnableIdempotence</a></td><td>If set to true, the Apache Kafka will ensure messages are delivered in the correct order, and without duplicates.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_FlattenArrays">FlattenArrays</a></td><td>By default, nested arrays won't show up if TypeDetectionScheme is set to SchemaRegistry. The FlattenArrays property can be used to flatten the elements of nested arrays into columns of their own. Set FlattenArrays to the number of elements you want to return from nested arrays.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_GenerateSchemaFiles">GenerateSchemaFiles</a></td><td>Indicates the user preference as to when schemas should be generated and saved.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_MaximumBatchSize">MaximumBatchSize</a></td><td>Specifies maximum batch size to gather before sending a request.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_MaxRows">MaxRows</a></td><td>Limits the number of rows returned when no aggregation or GROUP BY is used in the query. This takes precedence over LIMIT clauses.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_MessageKeyColumn">MessageKeyColumn</a></td><td>If specified, the message key sent to Apache Kafka will be read from this column.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_MessageKeyType">MessageKeyType</a></td><td>If MessageKeyColumn is specified, this property must be set to the expected type for the pertinent column.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_OffsetResetStrategy">OffsetResetStrategy</a></td><td>Specifies an offset for the consumer group.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_Other">Other</a></td><td>These hidden properties are used only in specific use cases.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_Pagesize">Pagesize</a></td><td>The maximum number of rows to fetch from Kafka at one time.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_PoolIdleTimeout">PoolIdleTimeout</a></td><td>The allowed idle time for a connection before it is closed.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_PoolMaxSize">PoolMaxSize</a></td><td>The maximum connections in the pool.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_PoolMinSize">PoolMinSize</a></td><td>The minimum number of connections in the pool.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_PoolWaitTime">PoolWaitTime</a></td><td>The max seconds to wait for an available connection.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_ProduceMeta">ProduceMeta</a></td><td>Specifies whether or not to send a meta message while producing the outgoing message.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_ProducerProperties">ProducerProperties</a></td><td>Additional options used to configure Kafka producers.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_PseudoColumns">PseudoColumns</a></td><td>This property indicates whether or not to include pseudo columns as columns to the table.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_ReadDuration">ReadDuration</a></td><td>The duration which additional messages are allowed.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_Readonly">Readonly</a></td><td>You can use this property to enforce read-only access to Apache Kafka from the provider.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_RowScanDepth">RowScanDepth</a></td><td>The maximum number of messages to scan for the columns available in the topic.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_RTK">RTK</a></td><td>The runtime key used for licensing.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_SerializationFormat">SerializationFormat</a></td><td>Specifies how to serialize/deserialize message contents.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_Timeout">Timeout</a></td><td>The value in seconds until the timeout error is thrown, canceling the operation.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_TypeDetectionScheme">TypeDetectionScheme</a></td><td>Comma-separated list of options specifying how the provider will scan the data to determine the fields and datatypes for the bucket.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_UseConfluentAvroFormat">UseConfluentAvroFormat</a></td><td>Specifies how Avro data should be formatted during an INSERT.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_UseConnectionPooling">UseConnectionPooling</a></td><td>This property enables connection pooling.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_UserDefinedViews">UserDefinedViews</a></td><td>A filepath pointing to the JSON configuration file containing your custom views.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_ValidateRegistryTopics">ValidateRegistryTopics</a></td><td>Specifies whether or not to validate schema registry topics against the Apache Kafka broker. Only has an effect when TypeDetectionScheme =SchemaRegistry.</td></tr>
</table></center></div>




  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - Authentication" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - Authentication: The scheme used for authentication with the Apache Kafka broker.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>Authentication</h1>
       
<div class="chapter_content" id="RSBApacheKafka_c_Authentication">
<p>
<p>This section provides a complete list of the Authentication properties you can configure in the connection string for this provider.
<hr/>
<center></a>
<table><tr><td><b>Property</b></td><td><b>Description</b></td></tr>
<tr valign="top"><td><a href="#RSBApacheKafka_p_AuthScheme">AuthScheme</a></td><td>The scheme used for authentication with the Apache Kafka broker.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_User">User</a></td><td>The user who is authenticating to Apache Kafka.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_Password">Password</a></td><td>The password used to authenticate to Apache Kafka.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_BootstrapServers">BootstrapServers</a></td><td>The address of the Apache Kafka BootstrapServers to which you are connecting to.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_Topic">Topic</a></td><td>The topic used for read and write operations.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_UseSSL">UseSSL</a></td><td>This field sets whether SSL is enabled. Automatically enabled if AuthScheme is set to SSL.</td></tr>
</table></center></div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - AuthScheme" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - AuthScheme: The scheme used for authentication with the Apache Kafka broker.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>AuthScheme</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_AuthScheme">
<p>The scheme used for authentication with the Apache Kafka broker.</p>





<h3>Possible Values</h3>
AzureMSI, SCRAM-SHA-512, Auto, AzureServicePrincipalCert, AzureServicePrincipal, Kerberos, None, SSLCertificate, Plain, AzureAD, SCRAM

<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>"Auto"</p>

<h3>Remarks</h3>


<p>The supported schemes are described as follows:
<p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap">Auto</td><td> Lets the driver decide automatically based on the other connection properties you have set.</td></tr><tr><td style="white-space:nowrap">None</td><td> Anonymous authentication will be used, and you can connect to the data source without specifying the user credentials.</td></tr><tr><td style="white-space:nowrap">Plain</td><td> The plain text login module will be used.</td></tr><tr><td style="white-space:nowrap">SCRAM</td><td> The SCRAM login module will be used with SHA-256 hashing.</td></tr><tr><td style="white-space:nowrap">SCRAM-SHA-512</td><td> The SCRAM login module will be used with SHA-512 hashing.</td></tr><tr><td style="white-space:nowrap">Kerberos</td><td> Kerberos authentication will be used, when using this value the system kerberos configuration file should be specified.</td></tr><tr><td style="white-space:nowrap">SSLCertificate</td><td> SSL client certificate authentication will be used.</td></tr></table></center><p />
</p>

<p>In addition, the following schemes are supported when connecting to Azure Event Hubs:

<p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap">AzureAD</td><td> Set this to perform Azure Active Directory OAuth authentication.</td></tr><tr><td style="white-space:nowrap">AzureMSI</td><td> Set this to automatically obtain Managed Service Identity credentials when running on an Azure VM.</td></tr><tr><td style="white-space:nowrap">AzureServicePrincipal</td><td> Set this to authenticate as an Azure Service Principal using a Client Secret.</td></tr><tr><td style="white-space:nowrap">AzureServicePrincipalCert</td><td> Set this to authenticate as an Azure Service Principal using a Certificate.</td></tr></table></center><p />
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - User" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - User: The user who is authenticating to Apache Kafka.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>User</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_User">
<p>The user who is authenticating to Apache Kafka.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>If not specified, the driver will attempt unauthorized connection.
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - Password" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - Password: The password used to authenticate to Apache Kafka.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>Password</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_Password">
<p>The password used to authenticate to Apache Kafka.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>If not specified, the driver will attempt an unauthorized connection.
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - BootstrapServers" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - BootstrapServers: The address of the Apache Kafka BootstrapServers to which you are connecting to.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>BootstrapServers</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_BootstrapServers">
<p>The address of the Apache Kafka BootstrapServers to which you are connecting to.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>Specify both the server and port. The server may be either a hostname or IP address, for example: <i>10.1.2.3:9092</i>.
Multiple comma-separated addresses may be provided.
As long as one of the bootstrap servers is in the list responses, the connection will be successful.
</p>

<p>If you are connecting to Confluent Cloud, you can find this on the Cluster settings.
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - Topic" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - Topic: The topic used for read and write operations.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>Topic</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_Topic">
<p>The topic used for read and write operations.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>By default the driver supports producing into and consuming from all topics in Kafka.
You can limit it to just one topic by setting this option.
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - UseSSL" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - UseSSL: This field sets whether SSL is enabled. Automatically enabled if AuthScheme is set to SSL.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>UseSSL</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_UseSSL">
<p>This field sets whether SSL is enabled. Automatically enabled if AuthScheme is set to SSL.</p>







<h3>Data Type</h3>
    <p>bool</p>

<h3>Default Value</h3>
<p>false</p>

<h3>Remarks</h3>


<p>This field sets whether the driver will attempt to negotiate TLS/SSL connections to the server.
By default, the driver checks the server's certificate against the system's trusted certificate store. 
To specify another certificate, set <a href="#RSBApacheKafka_p_SSLServerCert">SSLServerCert</a>.</p>

<p></p>

</div>





  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - Connection" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - Connection: Specifies which group the consumers created by the driver should belong to.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>Connection</h1>
       
<div class="chapter_content" id="RSBApacheKafka_c_Connection">
<p>
<p>This section provides a complete list of the Connection properties you can configure in the connection string for this provider.
<hr/>
<center></a>
<table><tr><td><b>Property</b></td><td><b>Description</b></td></tr>
<tr valign="top"><td><a href="#RSBApacheKafka_p_ConsumerGroupId">ConsumerGroupId</a></td><td>Specifies which group the consumers created by the driver should belong to.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_AutoCommit">AutoCommit</a></td><td>Specifies if the Apache Kafka consumer should autocommit after each poll.</td></tr>
</table></center></div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - ConsumerGroupId" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - ConsumerGroupId: Specifies which group the consumers created by the driver should belong to.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>ConsumerGroupId</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_ConsumerGroupId">
<p>Specifies which group the consumers created by the driver should belong to.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>If not specified, the driver will assign a random string.

</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - AutoCommit" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - AutoCommit: Specifies if the Apache Kafka consumer should autocommit after each poll.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>AutoCommit</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_AutoCommit">
<p>Specifies if the Apache Kafka consumer should autocommit after each poll.</p>







<h3>Data Type</h3>
    <p>bool</p>

<h3>Default Value</h3>
<p>false</p>

<h3>Remarks</h3>


<p>If true, the consumer's offset will be periodically committed in the background.

</p>

</div>





  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - Azure Authentication" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - Azure Authentication: The Microsoft Online tenant being used to access data. If not specified, your default tenant is used.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>Azure Authentication</h1>
       
<div class="chapter_content" id="RSBApacheKafka_c_AzureAuthentication">
<p>
<p>This section provides a complete list of the Azure Authentication properties you can configure in the connection string for this provider.
<hr/>
<center></a>
<table><tr><td><b>Property</b></td><td><b>Description</b></td></tr>
<tr valign="top"><td><a href="#RSBApacheKafka_p_AzureTenant">AzureTenant</a></td><td>The Microsoft Online tenant being used to access data. If not specified, your default tenant is used.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_AzureResource">AzureResource</a></td><td>The Azure Active resource to authenticate to (used during Azure OAuth exchange).</td></tr>
</table></center></div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - AzureTenant" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - AzureTenant: The Microsoft Online tenant being used to access data. If not specified, your default tenant is used.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>AzureTenant</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_AzureTenant">
<p>The Microsoft Online tenant being used to access data. If not specified, your default tenant is used.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>The Microsoft Online tenant being used to access data. For instance, contoso.onmicrosoft.com. Alternatively, 
specify the tenant Id. This value is the directory Id in the Azure Portal &gt; Azure Active Directory &gt; Properties.</p>

<p>Typically it is not necessary to specify the Tenant. This can be automatically determined by Microsoft when using 
the <a href="#RSBApacheKafka_p_OAuthGrantType">OAuthGrantType</a> set to CODE (default). However, it may fail in the case that the user belongs to multiple 
tenants. For instance, if an Admin of domain A invites a user of domain B to be a guest user. The user will 
now belong to both tenants. It is a good practice to specify the Tenant, although in general things should normally 
work without having to specify it.</p>

<p>The <u>AzureTenant</u> is required when setting <a href="#RSBApacheKafka_p_OAuthGrantType">OAuthGrantType</a> to CLIENT. When using client credentials, there is no 
user context. The credentials are taken from the context of the app itself. While Microsoft still allows client credentials 
to be obtained without specifying which Tenant, it has a much lower probability of picking the specific tenant you 
want to work with. For this reason, we require <u>AzureTenant</u> to be explicitly stated for all client credentials connections 
to ensure you get credentials that are applicable for the domain you intend to connect to.
</p>

<p>The Microsoft Online tenant being used to access data. For instance, contoso.onmicrosoft.com. Alternatively, 
specify the tenant Id. This value is the directory Id in the Azure Portal &gt; Azure Active Directory &gt; Properties.</p>

<p>Typically it is not necessary to specify the Tenant. This can be automatically determined by Microsoft when using 
the <a href="#RSBApacheKafka_p_OAuthGrantType">OAuthGrantType</a> set to CODE (default). However, it may fail in the case that the user belongs to multiple 
tenants. For instance, if an Admin of domain A invites a user of domain B to be a guest user. The user will 
now belong to both tenants. It is a good practice to specify the Tenant, although in general things should normally 
work without having to specify it.</p>

<p>The <u>AzureTenant</u> is required when setting <a href="#RSBApacheKafka_p_OAuthGrantType">OAuthGrantType</a> to CLIENT. When using client credentials, there is no 
user context. The credentials are taken from the context of the app itself. While Microsoft still allows client credentials 
to be obtained without specifying which Tenant, it has a much lower probability of picking the specific tenant you 
want to work with. For this reason, we require <u>AzureTenant</u> to be explicitly stated for all client credentials connections 
to ensure you get credentials that are applicable for the domain you intend to connect to.
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - AzureResource" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - AzureResource: The Azure Active resource to authenticate to (used during Azure OAuth exchange).">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>AzureResource</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_AzureResource">
<p>The Azure Active resource to authenticate to (used during Azure OAuth exchange).</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>The resource must be specified if using Azure OAuth.
It should be set to the App Id URI of the web API (secured resource).</p>

<p></p>

</div>





  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - OAuth" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - OAuth: Set this property to initiate the process to obtain or refresh the OAuth access token when you connect.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>OAuth</h1>
       
<div class="chapter_content" id="RSBApacheKafka_c_OAuth">
<p>
<p>This section provides a complete list of the OAuth properties you can configure in the connection string for this provider.
<hr/>
<center></a>
<table><tr><td><b>Property</b></td><td><b>Description</b></td></tr>
<tr valign="top"><td><a href="#RSBApacheKafka_p_InitiateOAuth">InitiateOAuth</a></td><td>Set this property to initiate the process to obtain or refresh the OAuth access token when you connect.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_OAuthClientId">OAuthClientId</a></td><td>The client Id assigned when you register your application with an OAuth authorization server.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_OAuthClientSecret">OAuthClientSecret</a></td><td>The client secret assigned when you register your application with an OAuth authorization server.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_OAuthAccessToken">OAuthAccessToken</a></td><td>The access token for connecting using OAuth.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_OAuthSettingsLocation">OAuthSettingsLocation</a></td><td>The location of the settings file where OAuth values are saved when InitiateOAuth is set to GETANDREFRESH or REFRESH . Alternatively, you can hold this location in memory by specifying a value starting with 'memory://'.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_OAuthVerifier">OAuthVerifier</a></td><td>The verifier code returned from the OAuth authorization URL.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_OAuthRefreshToken">OAuthRefreshToken</a></td><td>The OAuth refresh token for the corresponding OAuth access token.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_OAuthExpiresIn">OAuthExpiresIn</a></td><td>The lifetime in seconds of the OAuth AccessToken.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_OAuthTokenTimestamp">OAuthTokenTimestamp</a></td><td>The Unix epoch timestamp in milliseconds when the current Access Token was created.</td></tr>
</table></center></div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - InitiateOAuth" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - InitiateOAuth: Set this property to initiate the process to obtain or refresh the OAuth access token when you connect.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>InitiateOAuth</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_InitiateOAuth">
<p>Set this property to initiate the process to obtain or refresh the OAuth access token when you connect.</p>





<h3>Possible Values</h3>
OFF, GETANDREFRESH, REFRESH

<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>"OFF"</p>

<h3>Remarks</h3>


<p>The following options are available:
<ol><li><b>OFF</b>: Indicates that the OAuth flow will be handled entirely by the user. An OAuthAccessToken will be required to authenticate.</li><li><b>GETANDREFRESH</b>: Indicates that the entire OAuth Flow will be handled by the driver. If no token currently exists, it will be obtained by prompting the user via the browser. If a token exists, it will be refreshed when applicable.</li><li><b>REFRESH</b>: Indicates that the driver will only handle refreshing the OAuthAccessToken. The user will never be prompted by the driver to authenticate via the browser. The user must handle obtaining the OAuthAccessToken and OAuthRefreshToken initially.</li></ol>

</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - OAuthClientId" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - OAuthClientId: The client Id assigned when you register your application with an OAuth authorization server.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>OAuthClientId</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_OAuthClientId">
<p>The client Id assigned when you register your application with an OAuth authorization server.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>As part of registering an OAuth application, you will receive the <u>OAuthClientId</u> value, sometimes also called a consumer key, and a client secret, the <a href="#RSBApacheKafka_p_OAuthClientSecret">OAuthClientSecret</a>. 

</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - OAuthClientSecret" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - OAuthClientSecret: The client secret assigned when you register your application with an OAuth authorization server.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>OAuthClientSecret</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_OAuthClientSecret">
<p>The client secret assigned when you register your application with an OAuth authorization server.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>As part of registering an OAuth application, you will receive the <a href="#RSBApacheKafka_p_OAuthClientId">OAuthClientId</a>, also called a consumer key. You will also receive a client secret, also called a consumer secret. Set the client secret in the <u>OAuthClientSecret</u> property. 
 </p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - OAuthAccessToken" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - OAuthAccessToken: The access token for connecting using OAuth.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>OAuthAccessToken</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_OAuthAccessToken">
<p>The access token for connecting using OAuth.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>The <u>OAuthAccessToken</u> property is used to connect using OAuth. The <u>OAuthAccessToken</u> is retrieved from the OAuth server as part of the authentication process. It has a server-dependent timeout and can be reused between requests. </p>

<p>The access token is used in place of your user name and password. The access token protects your credentials by keeping them on the server.  
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - OAuthSettingsLocation" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - OAuthSettingsLocation: The location of the settings file where OAuth values are saved when InitiateOAuth is set to GETANDREFRESH or REFRESH . Alternatively, you can hold this location in memory by specifying a value starting with 'memory://'.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>OAuthSettingsLocation</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_OAuthSettingsLocation">
<p>The location of the settings file where OAuth values are saved when InitiateOAuth is set to GETANDREFRESH or REFRESH . Alternatively, you can hold this location in memory by specifying a value starting with 'memory://'.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>"%APPDATA%\\CData\\ApacheKafka Data Provider\\OAuthSettings.txt"</p>

<h3>Remarks</h3>


<p>When <a href="#RSBApacheKafka_p_InitiateOAuth">InitiateOAuth</a> is set to <b>GETANDREFRESH</b> or <b>REFRESH</b>, the driver saves OAuth values to avoid requiring the user to manually enter OAuth connection properties and to allow the credentials to be shared across connections or processes.
</p>

<p>Instead of specifying a file path, you can use memory storage. Memory locations are specified by using a value starting with 'memory://' followed by a unique identifier for that set of credentials (for example, memory://user1). The identifier can be anything you choose but should be unique to the user. Unlike file-based storage, where credentials persist across connections, memory storage loads the credentials into static memory, and the credentials are shared between connections using the same identifier for the life of the process. To persist credentials outside the current process, you must manually store the credentials prior to closing the connection.  This enables you to set them in the connection when the process is started again. You can retrieve OAuth property values with a query to the <b>sys_connection_props</b> system table. If there are multiple connections using the same credentials, the properties are read from the previously closed connection.
</p>

<p>The default location is "%APPDATA%\\CData\\ApacheKafka Data Provider\\OAuthSettings.txt" with <b>%APPDATA%</b> set to the user's configuration directory. The default values are</p>

<p><ul><li>Windows: "register://%DSN"</li><li>Unix: "%AppData%..."</li><li>Mac: "%AppData%..."</li></ul></p>

<p>where DSN is the name of the current DSN used in the open connection.</p>

<p>The following table lists the value of <b>%APPDATA%</b> by OS:</p>

<p><p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap"><b>Platform</b> </td><td><b>%APPDATA%</b></td></tr><tr><td style="white-space:nowrap">Windows </td><td>The value of the APPDATA environment variable</td></tr><tr><td style="white-space:nowrap">Mac </td><td>~/Library/Application Support</td></tr><tr><td style="white-space:nowrap">Linux </td><td>~/.config</td></tr></table></center><p />
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - OAuthVerifier" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - OAuthVerifier: The verifier code returned from the OAuth authorization URL.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>OAuthVerifier</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_OAuthVerifier">
<p>The verifier code returned from the OAuth authorization URL.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>The verifier code returned from the OAuth authorization URL. This can be used on systems where a browser cannot be launched such as headless systems. </p>

<p></p>

<p>
<h3>Authentication on Headless Machines</h3>
</p>

<p>See <a href="#pg_connectionj">Establishing a Connection</a> to obtain the <u>OAuthVerifier</u> value.</p>

<p>Set
<a href="#RSBApacheKafka_p_OAuthSettingsLocation">OAuthSettingsLocation</a> along with <u>OAuthVerifier</u>. When you connect, the driver exchanges the <u>OAuthVerifier</u> for the OAuth authentication tokens and saves them, encrypted, to the specified location.  Set <a href="#RSBApacheKafka_p_InitiateOAuth">InitiateOAuth</a> to GETANDREFRESH to automate the exchange. 
</p>

<p>Once the OAuth settings file has been generated, you can remove <u>OAuthVerifier</u> from the connection properties and connect with <a href="#RSBApacheKafka_p_OAuthSettingsLocation">OAuthSettingsLocation</a> set. </p>

<p>To automatically refresh the OAuth token values, set <a href="#RSBApacheKafka_p_OAuthSettingsLocation">OAuthSettingsLocation</a> and additionally set <a href="#RSBApacheKafka_p_InitiateOAuth">InitiateOAuth</a> to REFRESH.</p>

<p></p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - OAuthRefreshToken" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - OAuthRefreshToken: The OAuth refresh token for the corresponding OAuth access token.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>OAuthRefreshToken</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_OAuthRefreshToken">
<p>The OAuth refresh token for the corresponding OAuth access token.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>The <u>OAuthRefreshToken</u> property is used to refresh the <a href="#RSBApacheKafka_p_OAuthAccessToken">OAuthAccessToken</a> when using OAuth authentication.
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - OAuthExpiresIn" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - OAuthExpiresIn: The lifetime in seconds of the OAuth AccessToken.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>OAuthExpiresIn</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_OAuthExpiresIn">
<p>The lifetime in seconds of the OAuth AccessToken.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>Pair with OAuthTokenTimestamp to determine when the AccessToken will expire.
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - OAuthTokenTimestamp" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - OAuthTokenTimestamp: The Unix epoch timestamp in milliseconds when the current Access Token was created.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>OAuthTokenTimestamp</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_OAuthTokenTimestamp">
<p>The Unix epoch timestamp in milliseconds when the current Access Token was created.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>Pair with OAuthExpiresIn to determine when the AccessToken will expire.
</p>

</div>





  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - Kerberos" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - Kerberos: The Keytab file containing your pairs of Kerberos principals and encrypted keys.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>Kerberos</h1>
       
<div class="chapter_content" id="RSBApacheKafka_c_Kerberos">
<p>
<p>This section provides a complete list of the Kerberos properties you can configure in the connection string for this provider.
<hr/>
<center></a>
<table><tr><td><b>Property</b></td><td><b>Description</b></td></tr>
<tr valign="top"><td><a href="#RSBApacheKafka_p_KerberosKeytabFile">KerberosKeytabFile</a></td><td>The Keytab file containing your pairs of Kerberos principals and encrypted keys.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_KerberosSPN">KerberosSPN</a></td><td>The service principal name (SPN) for the Kerberos Domain Controller.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_KerberosServiceName">KerberosServiceName</a></td><td>The name of the Kerberos service you want to authenticate with.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_UseKerberosTicketCache">UseKerberosTicketCache</a></td><td>Set this to use a ticket cache with the logged in user instead of a keytab file.</td></tr>
</table></center></div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - KerberosKeytabFile" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - KerberosKeytabFile: The Keytab file containing your pairs of Kerberos principals and encrypted keys.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>KerberosKeytabFile</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_KerberosKeytabFile">
<p>The Keytab file containing your pairs of Kerberos principals and encrypted keys.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>The Keytab file containing your pairs of Kerberos principals and encrypted keys.
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - KerberosSPN" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - KerberosSPN: The service principal name (SPN) for the Kerberos Domain Controller.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>KerberosSPN</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_KerberosSPN">
<p>The service principal name (SPN) for the Kerberos Domain Controller.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>The service principal name (SPN) for the Kerberos Domain Controller.
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - KerberosServiceName" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - KerberosServiceName: The name of the Kerberos service you want to authenticate with.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>KerberosServiceName</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_KerberosServiceName">
<p>The name of the Kerberos service you want to authenticate with.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>The name of the Kerberos service you want to authenticate with.
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - UseKerberosTicketCache" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - UseKerberosTicketCache: Set this to use a ticket cache with the logged in user instead of a keytab file.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>UseKerberosTicketCache</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_UseKerberosTicketCache">
<p>Set this to use a ticket cache with the logged in user instead of a keytab file.</p>







<h3>Data Type</h3>
    <p>bool</p>

<h3>Default Value</h3>
<p>false</p>

<h3>Remarks</h3>


<p>Set this to use a ticket cache with the logged in user instead of a keytab file
</p>

</div>





  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - SSL" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - SSL: The SSL server certificate used to validate to the Apache Kafka broker.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>SSL</h1>
       
<div class="chapter_content" id="RSBApacheKafka_c_SSL">
<p>
<p>This section provides a complete list of the SSL properties you can configure in the connection string for this provider.
<hr/>
<center></a>
<table><tr><td><b>Property</b></td><td><b>Description</b></td></tr>
<tr valign="top"><td><a href="#RSBApacheKafka_p_SSLServerCert">SSLServerCert</a></td><td>The SSL server certificate used to validate to the Apache Kafka broker.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_SSLServerCertType">SSLServerCertType</a></td><td>The format of the SSL server certificate used to verify the Apache Kafka broker.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_SSLServerCertPassword">SSLServerCertPassword</a></td><td>The password used to decrypt the certificate in SSLServerCert .</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_SSLClientCert">SSLClientCert</a></td><td>The SSL client certificate used to connect to the Apache Kafka broker.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_SSLClientCertType">SSLClientCertType</a></td><td>The format of the SSL client certificate used to connect to the Apache Kafka broker.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_SSLClientCertPassword">SSLClientCertPassword</a></td><td>The password used to decrypt the certificate in SSLClientCert .</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_SSLIdentificationAlgorithm">SSLIdentificationAlgorithm</a></td><td>The endpoint identification algorithm used by the Apache Kafka data provider client app to validate server host name.</td></tr>
</table></center></div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - SSLServerCert" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - SSLServerCert: The SSL server certificate used to validate to the Apache Kafka broker.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>SSLServerCert</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_SSLServerCert">
<p>The SSL server certificate used to validate to the Apache Kafka broker.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>The value of this property must be provided in the format described on the <a href="#RSBApacheKafka_p_SSLServerCertType">SSLServerCertType</a> page.
Please refer to it for more details.
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - SSLServerCertType" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - SSLServerCertType: The format of the SSL server certificate used to verify the Apache Kafka broker.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>SSLServerCertType</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_SSLServerCertType">
<p>The format of the SSL server certificate used to verify the Apache Kafka broker.</p>





<h3>Possible Values</h3>
JKSFILE, PEMKEY_FILE, PEMKEY_BLOB

<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>"PEMKEY_FILE"</p>

<h3>Remarks</h3>


<p>This property is used to determine what format the <a href="#RSBApacheKafka_p_SSLServerCert">SSLServerCert</a> property expects.
This property can take one of the following values:

<p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap">JKSFILE</td><td>The certificate store is the name of a Java key store (JKS) file the server certificate.
In this mode, <a href="#RSBApacheKafka_p_SSLServerCert">SSLServerCert</a> controls the <b>ssl.truststore.location</b> option, and <b>ssl.truststore.type</b> is set to JKS.
</td></tr><tr><td style="white-space:nowrap">PEMKEY_FILE</td><td>The certificate store is the name of a PEM-encoded file that contains a the server certificate.
In this mode, <a href="#RSBApacheKafka_p_SSLServerCert">SSLServerCert</a> controls the <b>ssl.truststore.location</b> option, and <b>ssl.truststore.type</b> is set to PEM.
</td></tr><tr><td style="white-space:nowrap">PEMKEY_BLOB</td><td>The certificate store is a string that contains the server certificate.
In this mode, <a href="#RSBApacheKafka_p_SSLServerCert">SSLServerCert</a> controls the <b>ssl.truststore.certificates</b> option.</td></tr></table></center><p />
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - SSLServerCertPassword" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - SSLServerCertPassword: The password used to decrypt the certificate in SSLServerCert .">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>SSLServerCertPassword</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_SSLServerCertPassword">
<p>The password used to decrypt the certificate in SSLServerCert .</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>Leave this blank if the server certificate isn't password protected.
</p>

<p>Corresponds to the <b>ssl.truststore.password</b> property.
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - SSLClientCert" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - SSLClientCert: The SSL client certificate used to connect to the Apache Kafka broker.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>SSLClientCert</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_SSLClientCert">
<p>The SSL client certificate used to connect to the Apache Kafka broker.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>The value of this property must be provided in the format described on the <a href="#RSBApacheKafka_p_SSLClientCertType">SSLClientCertType</a> page.
Please refer to it for more details.
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - SSLClientCertType" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - SSLClientCertType: The format of the SSL client certificate used to connect to the Apache Kafka broker.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>SSLClientCertType</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_SSLClientCertType">
<p>The format of the SSL client certificate used to connect to the Apache Kafka broker.</p>





<h3>Possible Values</h3>
JKSFILE, PFXFILE, PEMKEY_FILE, PEMKEY_BLOB

<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>"PEMKEY_FILE"</p>

<h3>Remarks</h3>


<p>This property is used to determine what format the <a href="#RSBApacheKafka_p_SSLClientCert">SSLClientCert</a> property expects.
This property can take one of the following values:

<p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap">JKSFILE</td><td>The certificate store is the name of a Java key store (JKS) file containing certificates.
In this mode, <a href="#RSBApacheKafka_p_SSLClientCert">SSLClientCert</a> controls the <b>ssl.keystore.location</b> option, and <b>ssl.keystore.type</b> is set to JKS.
</td></tr><tr><td style="white-space:nowrap">PEMKEY_FILE</td><td>The certificate store is the name of a PEM-encoded file that contains a private key and certificate.
In this mode, <a href="#RSBApacheKafka_p_SSLClientCert">SSLClientCert</a> controls the <b>ssl.keystore.location</b> option, and <b>ssl.keystore.type</b> is set to PEM.
</td></tr><tr><td style="white-space:nowrap">PEMKEY_BLOB</td><td>The certificate store is a string that contains a private key and certificate, optionally encoded in base64.
In this mode, <a href="#RSBApacheKafka_p_SSLClientCert">SSLClientCert</a> controls the <b>ssl.keystore.key</b> and <b>ssl.keystore.certificate.chain</b> options. 
The Kafka client libraries do not natively support providing the private key and certificate in a single option.
Internally, the driver will split the blob into its PRIVATE KEY and CERTIFICATE blocks.
<b>ssl.keystore.type</b> is also set to PEM.</td></tr></table></center><p />
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - SSLClientCertPassword" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - SSLClientCertPassword: The password used to decrypt the certificate in SSLClientCert .">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>SSLClientCertPassword</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_SSLClientCertPassword">
<p>The password used to decrypt the certificate in SSLClientCert .</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>Leave this blank if the client certificate isn't password protected.
</p>

<p>This controls different properties based on the <a href="#RSBApacheKafka_p_SSLClientCertType">SSLClientCertType</a>. 
For PEM certificates this sets <b>ssl.key.password</b>, and for JKS certificates this sets <b>ssl.keystore.password</b>.
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - SSLIdentificationAlgorithm" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - SSLIdentificationAlgorithm: The endpoint identification algorithm used by the Apache Kafka data provider client app to validate server host name.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>SSLIdentificationAlgorithm</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_SSLIdentificationAlgorithm">
<p>The endpoint identification algorithm used by the Apache Kafka data provider client app to validate server host name.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>The default value is 'https' and the server host name validation is enabled. You can disable it by setting its value to a blank space.
</p>

</div>





  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - Schema Registry" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - Schema Registry: The server for the schema registry. When this property is specified, the driver will read Apache Avro schema from the server.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>Schema Registry</h1>
       
<div class="chapter_content" id="RSBApacheKafka_c_SchemaRegistry">
<p>
<p>This section provides a complete list of the Schema Registry properties you can configure in the connection string for this provider.
<hr/>
<center></a>
<table><tr><td><b>Property</b></td><td><b>Description</b></td></tr>
<tr valign="top"><td><a href="#RSBApacheKafka_p_RegistryUrl">RegistryUrl</a></td><td>The server for the schema registry. When this property is specified, the driver will read Apache Avro schema from the server.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_RegistryType">RegistryType</a></td><td>Type of the schema specified for the a specific topic.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_RegistryService">RegistryService</a></td><td>The Schema Registry service used for working with topic schemas.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_RegistryAuthScheme">RegistryAuthScheme</a></td><td>The scheme used to authenticate to the schema registry.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_RegistryUser">RegistryUser</a></td><td>Username to authorize with the server specified in RegistryUrl .</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_RegistryPassword">RegistryPassword</a></td><td>Password to authorize with the server specified in RegistryUrl .</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_RegistryClientCert">RegistryClientCert</a></td><td>The TLS/SSL client certificate store for SSL Client Authentication (2-way SSL) with the schema registry.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_RegistryClientCertType">RegistryClientCertType</a></td><td>The type of key store used by the TLS/SSL client certificate given in RegistryClientCert .</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_RegistryClientCertPassword">RegistryClientCertPassword</a></td><td>The password for the TLS/SSL client certificate given in RegistryClientCert .</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_RegistryClientCertSubject">RegistryClientCertSubject</a></td><td>The subject of the TLS/SSL client certificate given in RegistryClientCert .</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_RegistryVersion">RegistryVersion</a></td><td>Version of the schema read from RegistryUrl for the specified topic.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_RegistryServerCert">RegistryServerCert</a></td><td>The certificate to be accepted from the schema registry when connecting using TLS/SSL.</td></tr>
</table></center></div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - RegistryUrl" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - RegistryUrl: The server for the schema registry. When this property is specified, the driver will read Apache Avro schema from the server.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>RegistryUrl</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_RegistryUrl">
<p>The server for the schema registry. When this property is specified, the driver will read Apache Avro schema from the server.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>Note this property provides no additional features when <a href="#RSBApacheKafka_p_SerializationFormat">SerializationFormat</a> is not set to Avro.</p>

<p><ul><li> If you are connecting to Confluent Cloud, this corresponds to the Schema Registry endpoint value in <b>Schemas &gt; Schema Registry &gt; Instructions</b>. </li><li> If you are connecting to AWS Glue service, this corresponds to the ARN value of the AWS Registry you want to connect. </li></ul>
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - RegistryType" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - RegistryType: Type of the schema specified for the a specific topic.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>RegistryType</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_RegistryType">
<p>Type of the schema specified for the a specific topic.</p>





<h3>Possible Values</h3>
Auto, JSON, AVRO

<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>"Auto"</p>

<h3>Remarks</h3>


<p>Currently we do not support Protobuf. If this value is set to Auto, then we driver will try to detect the valid Schema Registry Type for the selected Topic.
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - RegistryService" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - RegistryService: The Schema Registry service used for working with topic schemas.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>RegistryService</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_RegistryService">
<p>The Schema Registry service used for working with topic schemas.</p>





<h3>Possible Values</h3>
Confluent, AWSGlue

<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>"Confluent"</p>

<h3>Remarks</h3>


<p>The Schema Registry service used for working with topic schemas.
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - RegistryAuthScheme" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - RegistryAuthScheme: The scheme used to authenticate to the schema registry.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>RegistryAuthScheme</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_RegistryAuthScheme">
<p>The scheme used to authenticate to the schema registry.</p>





<h3>Possible Values</h3>
Auto, None, Basic, SSLCertificate

<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>"Auto"</p>

<h3>Remarks</h3>


<p>The schemes are as follows. Note that some schemes are only available when connecting to a specific <a href="#RSBApacheKafka_p_RegistryService">RegistryService</a>:

<p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap">Auto</td><td> Lets the driver decide automatically based on the other connection properties you have set.</td></tr><tr><td style="white-space:nowrap">None</td><td> No authentication will be used.</td></tr><tr><td style="white-space:nowrap">Basic</td><td> <a href="#RSBApacheKafka_p_RegistryUser">RegistryUser</a> and <a href="#RSBApacheKafka_p_RegistryPassword">RegistryPassword</a> are used. In Confluent these are the API user/password, while in Glue these are the IAM access key/secret key.</td></tr><tr><td style="white-space:nowrap">SSLCertificate</td><td> <a href="#RSBApacheKafka_p_RegistryClientCert">RegistryClientCert</a> is used with SSL client authentication. This is only supported when connecting to a Confluent registry.</td></tr></table></center><p />
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - RegistryUser" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - RegistryUser: Username to authorize with the server specified in RegistryUrl .">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>RegistryUser</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_RegistryUser">
<p>Username to authorize with the server specified in RegistryUrl .</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>If you are connecting to Confluent Cloud, this corresponds to the Access Key value in <b>Schemas &gt; Schema Registry &gt;</b> API access.
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - RegistryPassword" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - RegistryPassword: Password to authorize with the server specified in RegistryUrl .">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>RegistryPassword</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_RegistryPassword">
<p>Password to authorize with the server specified in RegistryUrl .</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>If you are connecting to Confluent Cloud, this corresponds to the Secret Key value in <b>Schemas &gt; Schema Registry &gt; API access</b>.
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - RegistryClientCert" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - RegistryClientCert: The TLS/SSL client certificate store for SSL Client Authentication (2-way SSL) with the schema registry.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>RegistryClientCert</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_RegistryClientCert">
<p>The TLS/SSL client certificate store for SSL Client Authentication (2-way SSL) with the schema registry.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>The name of the certificate store for the client certificate.</p>

<p>The <a href="#RSBApacheKafka_p_RegistryClientCertType">RegistryClientCertType</a> field specifies the type of the certificate store specified by <u>RegistryClientCert</u>. If the store is password protected, specify the password in <a href="#RSBApacheKafka_p_RegistryClientCertPassword">RegistryClientCertPassword</a>.</p>

<p><u>RegistryClientCert</u> is used in conjunction with the <a href="#RSBApacheKafka_p_RegistryClientCertSubject">RegistryClientCertSubject</a> field in order to specify client certificates. If <u>RegistryClientCert</u> has a value, and <a href="#RSBApacheKafka_p_RegistryClientCertSubject">RegistryClientCertSubject</a> is set, a search for a certificate is initiated. See <a href="#RSBApacheKafka_p_RegistryClientCertSubject">RegistryClientCertSubject</a> for more information.</p>

<p>Designations of certificate stores are platform-dependent.</p>

<p>The following are designations of the most common User and Machine certificate stores in Windows:</p>

<p>
<p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap">MY</td><td>A certificate store holding personal certificates with their associated private keys.
</td></tr><tr><td style="white-space:nowrap">CA</td><td>Certifying authority certificates.
</td></tr><tr><td style="white-space:nowrap">ROOT</td><td>Root certificates.
</td></tr><tr><td style="white-space:nowrap">SPC</td><td>Software publisher certificates.
</td></tr></table></center><p />
</p>

<p>In Java, the certificate store normally is a file containing certificates and optional private keys.
</p>

<p>When the certificate store type is PFXFile, this property must be set to the name of the file. When the type is PFXBlob, the property must be set to the binary contents of a PFX file (for example, PKCS12 certificate store).
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - RegistryClientCertType" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - RegistryClientCertType: The type of key store used by the TLS/SSL client certificate given in RegistryClientCert .">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>RegistryClientCertType</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_RegistryClientCertType">
<p>The type of key store used by the TLS/SSL client certificate given in RegistryClientCert .</p>





<h3>Possible Values</h3>
USER, MACHINE, PFXFILE, PFXBLOB, JKSFILE, JKSBLOB, PEMKEY_FILE, PEMKEY_BLOB, PUBLIC_KEY_FILE, PUBLIC_KEY_BLOB, SSHPUBLIC_KEY_FILE, SSHPUBLIC_KEY_BLOB, P7BFILE, PPKFILE, XMLFILE, XMLBLOB

<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>"USER"</p>

<h3>Remarks</h3>


<p>This property can take one of the following values:

<p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap">USER - default</td><td>For Windows, this specifies that the certificate store is a certificate store owned by the current user. Note that this store type is not available in Java.
</td></tr><tr><td style="white-space:nowrap">MACHINE</td><td>For Windows, this specifies that the certificate store is a machine store. Note that this store type is not available in Java.
</td></tr><tr><td style="white-space:nowrap">PFXFILE</td><td>The certificate store is the name of a PFX (PKCS12) file containing certificates.
</td></tr><tr><td style="white-space:nowrap">PFXBLOB</td><td>The certificate store is a string (base-64-encoded) representing a certificate store in PFX (PKCS12) format.
</td></tr><tr><td style="white-space:nowrap">JKSFILE</td><td>The certificate store is the name of a Java key store (JKS) file containing certificates. Note that this store type is only available in Java.
</td></tr><tr><td style="white-space:nowrap">JKSBLOB</td><td>The certificate store is a string (base-64-encoded) representing a certificate store in JKS format. Note that this store type is only available in Java.
</td></tr><tr><td style="white-space:nowrap">PEMKEY_FILE</td><td>The certificate store is the name of a PEM-encoded file that contains a private key and an optional certificate.
</td></tr><tr><td style="white-space:nowrap">PEMKEY_BLOB</td><td>The certificate store is a string (base64-encoded) that contains a private key and an optional certificate.
</td></tr><tr><td style="white-space:nowrap">PUBLIC_KEY_FILE</td><td>The certificate store is the name of a file that contains a PEM- or DER-encoded public key certificate.
</td></tr><tr><td style="white-space:nowrap">PUBLIC_KEY_BLOB</td><td>The certificate store is a string (base-64-encoded) that contains a PEM- or DER-encoded public key certificate.
</td></tr><tr><td style="white-space:nowrap">SSHPUBLIC_KEY_FILE</td><td>The certificate store is the name of a file that contains an SSH-style public key.
</td></tr><tr><td style="white-space:nowrap">SSHPUBLIC_KEY_BLOB</td><td>The certificate store is a string (base-64-encoded) that contains an SSH-style public key.
</td></tr><tr><td style="white-space:nowrap">P7BFILE</td><td>The certificate store is the name of a PKCS7 file containing certificates.
</td></tr><tr><td style="white-space:nowrap">PPKFILE</td><td>The certificate store is the name of a file that contains a PuTTY Private Key (PPK).
</td></tr><tr><td style="white-space:nowrap">XMLFILE</td><td>The certificate store is the name of a file that contains a certificate in XML format.
</td></tr><tr><td style="white-space:nowrap">XMLBLOB</td><td>The certificate store is a string that contains a certificate in XML format.
</td></tr></table></center><p />
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - RegistryClientCertPassword" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - RegistryClientCertPassword: The password for the TLS/SSL client certificate given in RegistryClientCert .">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>RegistryClientCertPassword</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_RegistryClientCertPassword">
<p>The password for the TLS/SSL client certificate given in RegistryClientCert .</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>If the certificate store is of a type that requires a password, this property is used to specify that password to open the certificate store.
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - RegistryClientCertSubject" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - RegistryClientCertSubject: The subject of the TLS/SSL client certificate given in RegistryClientCert .">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>RegistryClientCertSubject</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_RegistryClientCertSubject">
<p>The subject of the TLS/SSL client certificate given in RegistryClientCert .</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>"*"</p>

<h3>Remarks</h3>


<p>When loading a certificate the subject is used to locate the certificate in the store.
</p>

<p>If an exact match is not found, the store is searched for subjects containing the value of the property. If a match is still not found, the property is set to an empty string, and no certificate is selected.
</p>

<p>The special value "*" picks the first certificate in the certificate store.
</p>

<p>The certificate subject is a comma separated list of distinguished name fields and values. For example, "CN=www.server.com, OU=test, C=US, E=support@company.com". The common fields and their meanings are shown below.
<p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap"><b>Field</b></td><td><b>Meaning</b></td></tr><tr><td style="white-space:nowrap">CN</td><td>Common Name. This is commonly a host name like www.server.com.</td></tr><tr><td style="white-space:nowrap">O</td><td>Organization</td></tr><tr><td style="white-space:nowrap">OU</td><td>Organizational Unit</td></tr><tr><td style="white-space:nowrap">L</td><td>Locality</td></tr><tr><td style="white-space:nowrap">S</td><td>State</td></tr><tr><td style="white-space:nowrap">C</td><td>Country</td></tr><tr><td style="white-space:nowrap">E</td><td>Email Address</td></tr></table></center><p />
</p>

<p>If a field value contains a comma, it must be quoted.
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - RegistryVersion" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - RegistryVersion: Version of the schema read from RegistryUrl for the specified topic.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>RegistryVersion</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_RegistryVersion">
<p>Version of the schema read from RegistryUrl for the specified topic.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>"latest"</p>

<h3>Remarks</h3>


<p>Version of the schema read from <a href="#RSBApacheKafka_p_RegistryUrl">RegistryUrl</a> for the specified topic.
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - RegistryServerCert" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - RegistryServerCert: The certificate to be accepted from the schema registry when connecting using TLS/SSL.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>RegistryServerCert</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_RegistryServerCert">
<p>The certificate to be accepted from the schema registry when connecting using TLS/SSL.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>If using a TLS/SSL connection, this property can be used to specify the TLS/SSL certificate to be accepted from the server. Any other certificate that is not trusted by the machine is rejected. 
</p>

<p>This property can take the following forms:

<p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap"><b>Description</b>      </td><td><b>Example</b></td></tr><tr><td style="white-space:nowrap">A full PEM Certificate (example shortened for brevity) </td><td>-----BEGIN CERTIFICATE-----
MIIChTCCAe4CAQAwDQYJKoZIhv......Qw==
-----END CERTIFICATE-----</td></tr><tr><td style="white-space:nowrap">A path to a local file containing the certificate  </td><td>C:\cert.cer</td></tr><tr><td style="white-space:nowrap">The public key (example shortened for brevity)  </td><td>-----BEGIN RSA PUBLIC KEY-----
MIGfMA0GCSq......AQAB
-----END RSA PUBLIC KEY-----</td></tr><tr><td style="white-space:nowrap">The MD5 Thumbprint (hex values can also be either space or colon separated) </td><td>ecadbdda5a1529c58a1e9e09828d70e4</td></tr><tr><td style="white-space:nowrap">The SHA1 Thumbprint (hex values can also be either space or colon separated) </td><td>34a929226ae0819f2ec14b4a3d904f801cbb150d</td></tr></table></center><p />
</p>

<p>If not specified, any certificate trusted by the machine is accepted. 
</p>

<p>Certificates are validated as trusted by the machine based on the System's trust store.  The trust store used is the 'javax.net.ssl.trustStore' value specified for the system. If no value is specified for this property, Java's default trust store is used (for example, JAVA_HOME\lib\security\cacerts).
</p>

<p>Use '*' to signify to accept all certificates. Note that this is not recommended due to security concerns.
</p>

</div>





  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - Firewall" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - Firewall: The protocol used by a proxy-based firewall.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>Firewall</h1>
       
<div class="chapter_content" id="RSBApacheKafka_c_Firewall">
<p>
<p>This section provides a complete list of the Firewall properties you can configure in the connection string for this provider.
<hr/>
<center></a>
<table><tr><td><b>Property</b></td><td><b>Description</b></td></tr>
<tr valign="top"><td><a href="#RSBApacheKafka_p_FirewallType">FirewallType</a></td><td>The protocol used by a proxy-based firewall.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_FirewallServer">FirewallServer</a></td><td>The name or IP address of a proxy-based firewall.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_FirewallPort">FirewallPort</a></td><td>The TCP port for a proxy-based firewall.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_FirewallUser">FirewallUser</a></td><td>The user name to use to authenticate with a proxy-based firewall.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_FirewallPassword">FirewallPassword</a></td><td>A password used to authenticate to a proxy-based firewall.</td></tr>
</table></center></div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - FirewallType" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - FirewallType: The protocol used by a proxy-based firewall.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>FirewallType</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_FirewallType">
<p>The protocol used by a proxy-based firewall.</p>





<h3>Possible Values</h3>
NONE, TUNNEL, SOCKS4, SOCKS5

<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>"NONE"</p>

<h3>Remarks</h3>


<p>This property specifies the protocol that the driver will use to tunnel traffic through the <a href="#RSBApacheKafka_p_FirewallServer">FirewallServer</a> proxy. Note that by default, the driver connects to the system proxy; to disable this behavior and connect to one of the following proxy types, set <a href="#RSBApacheKafka_p_ProxyAutoDetect">ProxyAutoDetect</a> to false. 

<p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap"><b>Type</b>
    </td><td><b>Default Port</b>
  </td><td><b>Description</b>

  </td></tr><tr><td style="white-space:nowrap">TUNNEL
    </td><td>80
    </td><td>When this is set, the driver opens a connection to Apache Kafka and traffic flows back and forth through the proxy. 

  </td></tr><tr><td style="white-space:nowrap">SOCKS4
    </td><td>1080
    </td><td>When this is set, the driver sends data through the SOCKS 4 proxy specified by <a href="#RSBApacheKafka_p_FirewallServer">FirewallServer</a> and <a href="#RSBApacheKafka_p_FirewallPort">FirewallPort</a> and passes the <a href="#RSBApacheKafka_p_FirewallUser">FirewallUser</a> value to the proxy, which determines if the connection request should be granted.
  </td></tr><tr><td style="white-space:nowrap">SOCKS5
    </td><td>1080
    </td><td>When this is set, the driver sends data through the SOCKS 5 proxy specified by <a href="#RSBApacheKafka_p_FirewallServer">FirewallServer</a> and <a href="#RSBApacheKafka_p_FirewallPort">FirewallPort</a>. If your proxy requires authentication, set <a href="#RSBApacheKafka_p_FirewallUser">FirewallUser</a> and <a href="#RSBApacheKafka_p_FirewallPassword">FirewallPassword</a> to credentials the proxy recognizes.
</td></tr></table></center><p /></p>

<p>To connect to HTTP proxies, use <a href="#RSBApacheKafka_p_ProxyServer">ProxyServer</a> and <a href="#RSBApacheKafka_p_ProxyPort">ProxyPort</a>. To authenticate to HTTP proxies, use <a href="#RSBApacheKafka_p_ProxyAuthScheme">ProxyAuthScheme</a>, <a href="#RSBApacheKafka_p_ProxyUser">ProxyUser</a>, and <a href="#RSBApacheKafka_p_ProxyPassword">ProxyPassword</a>.</p>

<p>
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - FirewallServer" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - FirewallServer: The name or IP address of a proxy-based firewall.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>FirewallServer</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_FirewallServer">
<p>The name or IP address of a proxy-based firewall.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>This property specifies the IP address, DNS name, or host name of a proxy allowing traversal of a firewall. The protocol is specified by <a href="#RSBApacheKafka_p_FirewallType">FirewallType</a>: Use <u>FirewallServer</u> with this property to connect through SOCKS or do tunneling. Use <a href="#RSBApacheKafka_p_ProxyServer">ProxyServer</a> to connect to an HTTP proxy.</p>

<p>Note that the driver uses the system proxy by default. To use a different proxy, set <a href="#RSBApacheKafka_p_ProxyAutoDetect">ProxyAutoDetect</a> to false.
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - FirewallPort" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - FirewallPort: The TCP port for a proxy-based firewall.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>FirewallPort</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_FirewallPort">
<p>The TCP port for a proxy-based firewall.</p>







<h3>Data Type</h3>
    <p>int</p>

<h3>Default Value</h3>
<p>0</p>

<h3>Remarks</h3>


<p>This specifies the TCP port for a proxy allowing traversal of a firewall. Use <a href="#RSBApacheKafka_p_FirewallServer">FirewallServer</a> to specify the name or IP address. Specify the protocol with <a href="#RSBApacheKafka_p_FirewallType">FirewallType</a>.
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - FirewallUser" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - FirewallUser: The user name to use to authenticate with a proxy-based firewall.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>FirewallUser</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_FirewallUser">
<p>The user name to use to authenticate with a proxy-based firewall.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>The <u>FirewallUser</u> and <a href="#RSBApacheKafka_p_FirewallPassword">FirewallPassword</a> properties are used to authenticate against the proxy specified in <a href="#RSBApacheKafka_p_FirewallServer">FirewallServer</a> and <a href="#RSBApacheKafka_p_FirewallPort">FirewallPort</a>, following the authentication method specified in <a href="#RSBApacheKafka_p_FirewallType">FirewallType</a>.
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - FirewallPassword" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - FirewallPassword: A password used to authenticate to a proxy-based firewall.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>FirewallPassword</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_FirewallPassword">
<p>A password used to authenticate to a proxy-based firewall.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>This property is passed to the proxy specified by <a href="#RSBApacheKafka_p_FirewallServer">FirewallServer</a> and <a href="#RSBApacheKafka_p_FirewallPort">FirewallPort</a>, following the authentication method specified by <a href="#RSBApacheKafka_p_FirewallType">FirewallType</a>.
</p>

</div>





  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - Proxy" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - Proxy: This indicates whether to use the system proxy settings or not. This takes precedence over other proxy settings, so you'll need to set ProxyAutoDetect to FALSE in order use custom proxy settings.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>Proxy</h1>
       
<div class="chapter_content" id="RSBApacheKafka_c_Proxy">
<p>
<p>This section provides a complete list of the Proxy properties you can configure in the connection string for this provider.
<hr/>
<center></a>
<table><tr><td><b>Property</b></td><td><b>Description</b></td></tr>
<tr valign="top"><td><a href="#RSBApacheKafka_p_ProxyAutoDetect">ProxyAutoDetect</a></td><td>This indicates whether to use the system proxy settings or not. This takes precedence over other proxy settings, so you'll need to set ProxyAutoDetect to FALSE in order use custom proxy settings.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_ProxyServer">ProxyServer</a></td><td>The hostname or IP address of a proxy to route HTTP traffic through.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_ProxyPort">ProxyPort</a></td><td>The TCP port the ProxyServer proxy is running on.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_ProxyAuthScheme">ProxyAuthScheme</a></td><td>The authentication type to use to authenticate to the ProxyServer proxy.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_ProxyUser">ProxyUser</a></td><td>A user name to be used to authenticate to the ProxyServer proxy.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_ProxyPassword">ProxyPassword</a></td><td>A password to be used to authenticate to the ProxyServer proxy.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_ProxySSLType">ProxySSLType</a></td><td>The SSL type to use when connecting to the ProxyServer proxy.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_ProxyExceptions">ProxyExceptions</a></td><td>A semicolon separated list of destination hostnames or IPs that are exempt from connecting through the ProxyServer .</td></tr>
</table></center></div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - ProxyAutoDetect" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - ProxyAutoDetect: This indicates whether to use the system proxy settings or not. This takes precedence over other proxy settings, so you'll need to set ProxyAutoDetect to FALSE in order use custom proxy settings.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>ProxyAutoDetect</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_ProxyAutoDetect">
<p>This indicates whether to use the system proxy settings or not. This takes precedence over other proxy settings, so you'll need to set ProxyAutoDetect to FALSE in order use custom proxy settings.</p>







<h3>Data Type</h3>
    <p>bool</p>

<h3>Default Value</h3>
<p>false</p>

<h3>Remarks</h3>


<p>This takes precedence over other proxy settings, so you'll need to set ProxyAutoDetect to FALSE in order use custom proxy settings. 
</p>

<p>
</p>

<p>
  NOTE: When this property is set to True, the proxy used is determined as follows:
  <ul><li>A search from the JVM properties (<b>http.proxy, https.proxy, socksProxy, etc.</b>) is performed.
  </li><li>In the case that the JVM properties don't exist, a search from <b>java.home/lib/net.properties</b> is performed.
  </li><li>In the case that java.net.useSystemProxies is set to True, a search from <b>the SystemProxy</b> is performed.
  </li><li>In Windows only, an attempt is made to retrieve these properties from the <b>Internet Options</b> in the <b>registry</b>.
  </li></ul>


</p>

<p>To connect to an HTTP proxy, see <a href="#RSBApacheKafka_p_ProxyServer">ProxyServer</a>. For other proxies, such as SOCKS or tunneling, see <a href="#RSBApacheKafka_p_FirewallType">FirewallType</a>.</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - ProxyServer" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - ProxyServer: The hostname or IP address of a proxy to route HTTP traffic through.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>ProxyServer</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_ProxyServer">
<p>The hostname or IP address of a proxy to route HTTP traffic through.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>The hostname or IP address of a proxy to route HTTP traffic through. The driver can use the HTTP, Windows (NTLM), or Kerberos authentication types to authenticate to an HTTP proxy. </p>

<p>If you need to connect through a SOCKS proxy or tunnel the connection, see <a href="#RSBApacheKafka_p_FirewallType">FirewallType</a>.</p>

<p>By default, the driver uses the system proxy. If you need to use another proxy, set <a href="#RSBApacheKafka_p_ProxyAutoDetect">ProxyAutoDetect</a> to false. 
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - ProxyPort" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - ProxyPort: The TCP port the ProxyServer proxy is running on.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>ProxyPort</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_ProxyPort">
<p>The TCP port the ProxyServer proxy is running on.</p>







<h3>Data Type</h3>
    <p>int</p>

<h3>Default Value</h3>
<p>80</p>

<h3>Remarks</h3>


<p>The port the HTTP proxy is running on that you want to redirect HTTP traffic through. Specify the HTTP proxy in <a href="#RSBApacheKafka_p_ProxyServer">ProxyServer</a>. For other proxy types, see <a href="#RSBApacheKafka_p_FirewallType">FirewallType</a>.
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - ProxyAuthScheme" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - ProxyAuthScheme: The authentication type to use to authenticate to the ProxyServer proxy.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>ProxyAuthScheme</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_ProxyAuthScheme">
<p>The authentication type to use to authenticate to the ProxyServer proxy.</p>





<h3>Possible Values</h3>
BASIC, DIGEST, NONE, NEGOTIATE, NTLM, PROPRIETARY

<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>"BASIC"</p>

<h3>Remarks</h3>


<p>This value specifies the authentication type to use to authenticate to the HTTP proxy specified by <a href="#RSBApacheKafka_p_ProxyServer">ProxyServer</a> and <a href="#RSBApacheKafka_p_ProxyPort">ProxyPort</a>. </p>

<p>Note that the driver will use the system proxy settings by default, without further configuration needed; if you want to connect to another proxy, you will need to set <a href="#RSBApacheKafka_p_ProxyAutoDetect">ProxyAutoDetect</a> to false, in addition to <a href="#RSBApacheKafka_p_ProxyServer">ProxyServer</a> and <a href="#RSBApacheKafka_p_ProxyPort">ProxyPort</a>. To authenticate, set <u>ProxyAuthScheme</u> and set <a href="#RSBApacheKafka_p_ProxyUser">ProxyUser</a> and <a href="#RSBApacheKafka_p_ProxyPassword">ProxyPassword</a>, if needed.</p>

<p>The authentication type can be one of the following:
<ul><li><b>BASIC:</b> The driver performs HTTP BASIC authentication.</li><li><b>DIGEST:</b> The driver performs HTTP DIGEST authentication.</li><li><b>NEGOTIATE:</b> The driver retrieves an NTLM or Kerberos token based on the applicable protocol for authentication. </li><li><b>PROPRIETARY:</b> The driver does not generate an NTLM or Kerberos token. You must supply this token in the Authorization header of the HTTP request.</li></ul>
</p>

<p>If you need to use another authentication type, such as SOCKS 5 authentication, see <a href="#RSBApacheKafka_p_FirewallType">FirewallType</a>.
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - ProxyUser" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - ProxyUser: A user name to be used to authenticate to the ProxyServer proxy.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>ProxyUser</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_ProxyUser">
<p>A user name to be used to authenticate to the ProxyServer proxy.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>The <u>ProxyUser</u> and <a href="#RSBApacheKafka_p_ProxyPassword">ProxyPassword</a> options are used to connect and authenticate against the HTTP proxy specified in <a href="#RSBApacheKafka_p_ProxyServer">ProxyServer</a>.</p>

<p>You can select one of the available authentication types in <a href="#RSBApacheKafka_p_ProxyAuthScheme">ProxyAuthScheme</a>. If you are using HTTP authentication, set this to the user name of a user recognized by the HTTP proxy. If you are using Windows or Kerberos authentication, set this property to a user name in one of the following formats:
<br/><pre lang="">user@domain
domain\user</pre>
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - ProxyPassword" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - ProxyPassword: A password to be used to authenticate to the ProxyServer proxy.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>ProxyPassword</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_ProxyPassword">
<p>A password to be used to authenticate to the ProxyServer proxy.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>This property is used to authenticate to an HTTP proxy server that supports NTLM (Windows), Kerberos, or HTTP authentication. To specify the HTTP proxy, you can set <a href="#RSBApacheKafka_p_ProxyServer">ProxyServer</a> and <a href="#RSBApacheKafka_p_ProxyPort">ProxyPort</a>. To specify the authentication type, set <a href="#RSBApacheKafka_p_ProxyAuthScheme">ProxyAuthScheme</a>. </p>

<p>If you are using HTTP authentication, additionally set <a href="#RSBApacheKafka_p_ProxyUser">ProxyUser</a> and <u>ProxyPassword</u> to  HTTP proxy. </p>

<p>If you are using NTLM authentication, set <a href="#RSBApacheKafka_p_ProxyUser">ProxyUser</a> and <u>ProxyPassword</u> to your Windows password. You may also need these to complete Kerberos authentication.</p>

<p>For SOCKS 5 authentication or tunneling, see <a href="#RSBApacheKafka_p_FirewallType">FirewallType</a>.</p>

<p>By default, the driver uses the system proxy. If you want to connect to another proxy, set <a href="#RSBApacheKafka_p_ProxyAutoDetect">ProxyAutoDetect</a> to false. 
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - ProxySSLType" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - ProxySSLType: The SSL type to use when connecting to the ProxyServer proxy.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>ProxySSLType</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_ProxySSLType">
<p>The SSL type to use when connecting to the ProxyServer proxy.</p>





<h3>Possible Values</h3>
AUTO, ALWAYS, NEVER, TUNNEL

<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>"AUTO"</p>

<h3>Remarks</h3>


<p>This property determines when to use SSL for the connection to an HTTP proxy specified by <a href="#RSBApacheKafka_p_ProxyServer">ProxyServer</a>. This value can be AUTO, ALWAYS, NEVER, or TUNNEL. The applicable values are the following:
<p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap"><b>AUTO</b></td><td>Default setting. If the URL is an HTTPS URL, the driver will use the TUNNEL option. If the URL is an HTTP URL, the component will use the NEVER option.</td></tr><tr><td style="white-space:nowrap"><b>ALWAYS</b></td><td>The connection is always SSL enabled.</td></tr><tr><td style="white-space:nowrap"><b>NEVER</b></td><td>The connection is not SSL enabled.</td></tr><tr><td style="white-space:nowrap"><b>TUNNEL</b></td><td>The connection is through a tunneling proxy. The proxy server opens a connection to the remote host and traffic flows back and forth through the proxy.</td></tr></table></center><p />
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - ProxyExceptions" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - ProxyExceptions: A semicolon separated list of destination hostnames or IPs that are exempt from connecting through the ProxyServer .">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>ProxyExceptions</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_ProxyExceptions">
<p>A semicolon separated list of destination hostnames or IPs that are exempt from connecting through the ProxyServer .</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>The <a href="#RSBApacheKafka_p_ProxyServer">ProxyServer</a> is used for all addresses, except for addresses defined in this property. Use semicolons to separate entries.</p>

<p>Note that the driver uses the system proxy settings by default, without further configuration needed; if you want to explicitly configure proxy exceptions for this connection, you need to set <a href="#RSBApacheKafka_p_ProxyAutoDetect">ProxyAutoDetect</a> = false, and configure <a href="#RSBApacheKafka_p_ProxyServer">ProxyServer</a> and <a href="#RSBApacheKafka_p_ProxyPort">ProxyPort</a>. To authenticate, set <a href="#RSBApacheKafka_p_ProxyAuthScheme">ProxyAuthScheme</a> and set <a href="#RSBApacheKafka_p_ProxyUser">ProxyUser</a> and <a href="#RSBApacheKafka_p_ProxyPassword">ProxyPassword</a>, if needed.</p>

</div>





  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - Logging" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - Logging: A filepath which designates the name and location of the log file.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>Logging</h1>
       
<div class="chapter_content" id="RSBApacheKafka_c_Logging">
<p>
<p>This section provides a complete list of the Logging properties you can configure in the connection string for this provider.
<hr/>
<center></a>
<table><tr><td><b>Property</b></td><td><b>Description</b></td></tr>
<tr valign="top"><td><a href="#RSBApacheKafka_p_Logfile">Logfile</a></td><td>A filepath which designates the name and location of the log file.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_Verbosity">Verbosity</a></td><td>The verbosity level that determines the amount of detail included in the log file.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_LogModules">LogModules</a></td><td>Core modules to be included in the log file.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_MaxLogFileSize">MaxLogFileSize</a></td><td>A string specifying the maximum size in bytes for a log file (for example, 10 MB).</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_MaxLogFileCount">MaxLogFileCount</a></td><td>A string specifying the maximum file count of log files.</td></tr>
</table></center></div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - Logfile" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - Logfile: A filepath which designates the name and location of the log file.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>Logfile</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_Logfile">
<p>A filepath which designates the name and location of the log file.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>Once this property is set, the driver will populate the log file as it carries out various tasks, such as when authentication is performed or queries are executed. If the specified file doesn't already exist, it will be created.</p>

<p>Connection strings and version information are also logged, though connection properties containing sensitive information are masked automatically.</p>

<p>If a relative filepath is supplied, the location of the log file will be resolved based on the path found in the <a href="#RSBApacheKafka_p_Location">Location</a> connection property.</p>

<p>For more control over what is written to the log file, you can adjust the <a href="#RSBApacheKafka_p_Verbosity">Verbosity</a> property.</p>

<p>Log contents are categorized into several modules. You can show/hide individual modules using the <a href="#RSBApacheKafka_p_LogModules">LogModules</a> property.</p>

<p>To edit the maximum size of a single logfile before a new one is created, see <a href="#RSBApacheKafka_p_MaxLogFileSize">MaxLogFileSize</a>.</p>

<p>If you would like to place a cap on the number of logfiles generated, use <a href="#RSBApacheKafka_p_MaxLogFileCount">MaxLogFileCount</a>.

<h2>Java Logging</h2>
</p>

<p>Java logging is also supported. To enable Java logging, set <u>Logfile</u> to:
<br/><pre lang="">Logfile=JAVALOG://myloggername</pre></p>

<p>As in the above sample, JAVALOG:// is a required prefix to use Java logging, and you will substitute your own Logger.</p>

<p>The supplied Logger's getLogger method is then called, using the supplied value to create the Logger instance. If a logging instance already exists, it will reference the existing instance.</p>

<p>When Java logging is enabled, the <a href="#RSBApacheKafka_p_Verbosity">Verbosity</a> will now correspond to specific logging levels.</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - Verbosity" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - Verbosity: The verbosity level that determines the amount of detail included in the log file.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>Verbosity</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_Verbosity">
<p>The verbosity level that determines the amount of detail included in the log file.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>"1"</p>

<h3>Remarks</h3>


<p>The verbosity level determines the amount of detail that the driver reports to the <a href="#RSBApacheKafka_p_Logfile">Logfile</a>. <u>Verbosity</u> levels from 1 to 5 are supported. These are detailed in the <a href="#pg_advancedlogging">Logging</a> page.</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - LogModules" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - LogModules: Core modules to be included in the log file.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>LogModules</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_LogModules">
<p>Core modules to be included in the log file.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>Only the modules specified (separated by ';') will be included in the log file. By default all modules are included.</p>

<p>See the <a href="#pg_advancedlogging">Logging</a> page for an overview.
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - MaxLogFileSize" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - MaxLogFileSize: A string specifying the maximum size in bytes for a log file (for example, 10 MB).">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>MaxLogFileSize</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_MaxLogFileSize">
<p>A string specifying the maximum size in bytes for a log file (for example, 10 MB).</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>"100MB"</p>

<h3>Remarks</h3>


<p>When the limit is hit, a new log is created in the same folder with the date and time appended to the end. The default limit is 100 MB. Values lower than 100 kB will use 100 kB as the value instead.</p>

<p>Adjust the maximum number of logfiles generated with <a href="#RSBApacheKafka_p_MaxLogFileCount">MaxLogFileCount</a>.</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - MaxLogFileCount" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - MaxLogFileCount: A string specifying the maximum file count of log files.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>MaxLogFileCount</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_MaxLogFileCount">
<p>A string specifying the maximum file count of log files.</p>







<h3>Data Type</h3>
    <p>int</p>

<h3>Default Value</h3>
<p>-1</p>

<h3>Remarks</h3>


<p>When the limit is hit, a new log is created in the same folder with the date and time appended to the end and the oldest log file will be deleted.</p>

<p>The minimum supported value is 2. A value of 0 or a negative value indicates no limit on the count.</p>

<p>Adjust the maximum size of the logfiles generated with <a href="#RSBApacheKafka_p_MaxLogFileSize">MaxLogFileSize</a>.</p>

</div>





  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - Schema" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - Schema: A path to the directory that contains the schema files defining tables, views, and stored procedures.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>Schema</h1>
       
<div class="chapter_content" id="RSBApacheKafka_c_Schema">
<p>
<p>This section provides a complete list of the Schema properties you can configure in the connection string for this provider.
<hr/>
<center></a>
<table><tr><td><b>Property</b></td><td><b>Description</b></td></tr>
<tr valign="top"><td><a href="#RSBApacheKafka_p_Location">Location</a></td><td>A path to the directory that contains the schema files defining tables, views, and stored procedures.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_BrowsableSchemas">BrowsableSchemas</a></td><td>This property restricts the schemas reported to a subset of the available schemas. For example, BrowsableSchemas=SchemaA,SchemaB,SchemaC.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_Tables">Tables</a></td><td>This property restricts the tables reported to a subset of the available tables. For example, Tables=TableA,TableB,TableC.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_Views">Views</a></td><td>Restricts the views reported to a subset of the available tables. For example, Views=ViewA,ViewB,ViewC.</td></tr>
</table></center></div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - Location" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - Location: A path to the directory that contains the schema files defining tables, views, and stored procedures.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>Location</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_Location">
<p>A path to the directory that contains the schema files defining tables, views, and stored procedures.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>"%APPDATA%\\CData\\ApacheKafka Data Provider\\Schema"</p>

<h3>Remarks</h3>


<p>The path to a directory which contains the schema files for the driver (.rsd files for tables and views, .rsb files for stored procedures). The folder location can be a relative path from the location of the executable. The <u>Location</u> property is only needed if you want to customize definitions (for example, change a column name, ignore a column, and so on) or extend the data model with new tables, views, or stored procedures.

</p>

<p>If left unspecified, the default location is "%APPDATA%\\CData\\ApacheKafka Data Provider\\Schema" with <b>%APPDATA%</b> being set to the user's configuration directory:




<p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap"><b>Platform</b> </td><td><b>%APPDATA%</b></td></tr><tr><td style="white-space:nowrap">Windows </td><td>The value of the APPDATA environment variable</td></tr><tr><td style="white-space:nowrap">Mac </td><td>~/Library/Application Support</td></tr><tr><td style="white-space:nowrap">Linux </td><td>~/.config</td></tr></table></center><p />
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - BrowsableSchemas" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - BrowsableSchemas: This property restricts the schemas reported to a subset of the available schemas. For example, BrowsableSchemas=SchemaA,SchemaB,SchemaC.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>BrowsableSchemas</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_BrowsableSchemas">
<p>This property restricts the schemas reported to a subset of the available schemas. For example, BrowsableSchemas=SchemaA,SchemaB,SchemaC.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>Listing the schemas from databases can be expensive. Providing a list of schemas in the connection string improves 
the performance.</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - Tables" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - Tables: This property restricts the tables reported to a subset of the available tables. For example, Tables=TableA,TableB,TableC.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>Tables</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_Tables">
<p>This property restricts the tables reported to a subset of the available tables. For example, Tables=TableA,TableB,TableC.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>Listing the tables from some databases can be expensive. Providing a list of tables in the connection string improves 
the performance of the driver.</p>

<p>This property can also be used as an alternative to automatically listing views if you already know which ones you want to work with and there would otherwise be too many to work with.</p>

<p>Specify the tables you want in a comma-separated list. Each table should be a valid SQL identifier with any special characters
escaped using square brackets, double-quotes or backticks. For example, Tables=TableA,[TableB/WithSlash],WithCatalog.WithSchema.`TableC With Space`. </p>

<p>Note that when connecting to a data source with multiple schemas or catalogs, you will need to provide the fully qualified name of the table in this property, as in the last example here, to avoid ambiguity between tables that exist in multiple catalogs or schemas.</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - Views" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - Views: Restricts the views reported to a subset of the available tables. For example, Views=ViewA,ViewB,ViewC.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>Views</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_Views">
<p>Restricts the views reported to a subset of the available tables. For example, Views=ViewA,ViewB,ViewC.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>Listing the views from some databases can be expensive. Providing a list of views in the connection string improves the performance of the driver. </p>

<p>This property can also be used as an alternative to automatically listing views if you already know which ones you want to work with and there would otherwise be too many to work with.
</p>

<p>Specify the views you want in a comma-separated list. Each view should be a valid SQL identifier with any special characters
escaped using square brackets, double-quotes or backticks. For example, Views=ViewA,[ViewB/WithSlash],WithCatalog.WithSchema.`ViewC With Space`. </p>

<p>Note that when connecting to a data source with multiple schemas or catalogs, you will need to provide the fully qualified name of the table in this property, as in the last example here, to avoid ambiguity between tables that exist in multiple catalogs or schemas.</p>

</div>





  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - Caching" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - Caching: Automatically caches the results of SELECT queries into a cache database specified by either CacheLocation or both of CacheConnection and CacheProvider .">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>Caching</h1>
       
<div class="chapter_content" id="RSBApacheKafka_c_Caching">
<p>
<p>This section provides a complete list of the Caching properties you can configure in the connection string for this provider.
<hr/>
<center></a>
<table><tr><td><b>Property</b></td><td><b>Description</b></td></tr>
<tr valign="top"><td><a href="#RSBApacheKafka_p_AutoCache">AutoCache</a></td><td>Automatically caches the results of SELECT queries into a cache database specified by either CacheLocation or both of CacheConnection and CacheProvider .</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_CacheDriver">CacheDriver</a></td><td>The database driver used to cache data.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_CacheConnection">CacheConnection</a></td><td>The connection string for the cache database. This property is always used in conjunction with CacheProvider . Setting both properties will override the value set for CacheLocation for caching data.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_CacheLocation">CacheLocation</a></td><td>Specifies the path to the cache when caching to a file.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_CacheTolerance">CacheTolerance</a></td><td>The tolerance for stale data in the cache specified in seconds when using AutoCache .</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_Offline">Offline</a></td><td>Use offline mode to get the data from the cache instead of the live source.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_CacheMetadata">CacheMetadata</a></td><td>This property determines whether or not to cache the table metadata to a file store.</td></tr>
</table></center></div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - AutoCache" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - AutoCache: Automatically caches the results of SELECT queries into a cache database specified by either CacheLocation or both of CacheConnection and CacheProvider .">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>AutoCache</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_AutoCache">
<p>Automatically caches the results of SELECT queries into a cache database specified by either CacheLocation or both of CacheConnection and CacheProvider .</p>







<h3>Data Type</h3>
    <p>bool</p>

<h3>Default Value</h3>
<p>false</p>

<h3>Remarks</h3>


<p>When <u>AutoCache</u> = true, the driver automatically maintains a cache of your table's data in the database of your choice. </p>

<p>

<h3>Setting the Caching Database</h3>
</p>

<p>When <u>AutoCache</u> = true, the driver caches to a simple, file-based cache. You can configure its location or cache to a different database with the following properties:
<ul><li><a href="#RSBApacheKafka_p_CacheLocation">CacheLocation</a>: Specifies the path to the file store.</li><li><a href="#RSBApacheKafka_p_CacheDriver">CacheDriver</a> and <a href="#RSBApacheKafka_p_CacheConnection">CacheConnection</a>: Specifies a driver to a database and the connection string.</li></ul></p>

<p>

<h3>See Also</h3>
</p>

<p><ul><li><a href="#RSBApacheKafka_p_CacheMetadata">CacheMetadata</a>: This property reduces the amount of metadata that crosses the network by persisting table schemas retrieved from the Apache Kafka metadata. Metadata then needs to be retrieved only once instead of every connection.</li><li><a href="#pg_cacheExplicitly">Explicitly Caching Data</a>: This section provides more examples of using <u>AutoCache</u> in <a href="#RSBApacheKafka_p_Offline">Offline</a> mode.</li><li><a href="#pg_cache">CACHE Statements</a>: You can use the CACHE statement to persist any SELECT query, as well as manage the cache; for example, refreshing schemas.</li></ul></p>

<p></p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - CacheDriver" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - CacheDriver: The database driver used to cache data.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>CacheDriver</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_CacheDriver">
<p>The database driver used to cache data.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>You can cache to any database for which you have a JDBC driver, including CData JDBC drivers.</p>

<p>The cache database is determined by the <u>CacheDriver</u> and <a href="#RSBApacheKafka_p_CacheConnection">CacheConnection</a> properties. The <u>CacheDriver</u> is the name of the JDBC driver class that you want to use to cache data. </p>

<p><b>Note:</b> you must add the <u>CacheDriver</u> JAR file to the classpath. 

</p>

<p>
<h2>Examples</h2>

The following examples show how to cache to several major databases. Refer to <a href="#RSBApacheKafka_p_CacheConnection">CacheConnection</a> for more information on the JDBC URL syntax and typical connection properties.


<h3>Derby and Java DB</h3>
</p>

<p></p>

<p>The driver simplifies Derby configuration. Java DB is the Oracle distribution of Derby. The JAR file is shipped in the JDK. You can find the JAR file, derby.jar, in the db subfolder of the JDK installation. In most caching scenarios, you need to specify only the following, after adding derby.jar to the classpath:
<br/><pre lang="">jdbc:apachekafka:CacheLocation='c:/Temp/cachedir';User=admin;Password=pass;BootStrapServers=https://localhost:9091;Topic=MyTopic;</pre>
To customize the Derby JDBC URL, use <u>CacheDriver</u> and <a href="#RSBApacheKafka_p_CacheConnection">CacheConnection</a>. For example, to cache to an in-memory database, use a JDBC URL like the following:
<br/><pre lang="">jdbc:apachekafka:CacheDriver=org.apache.derby.jdbc.EmbeddedDriver;CacheConnection='jdbc:derby:memory';User=admin;Password=pass;BootStrapServers=https://localhost:9091;Topic=MyTopic;</pre>


<h3>SQLite</h3>
</p>

<p>The following is a JDBC URL for the SQLite JDBC driver: 
<br/><pre lang="plain">jdbc:apachekafka:CacheDriver=org.sqlite.JDBC;CacheConnection='jdbc:sqlite:C:/Temp/sqlite.db';User=admin;Password=pass;BootStrapServers=https://localhost:9091;Topic=MyTopic;</pre>

  
<h3>MySQL</h3>

  </p>

<p>The following is a JDBC URL for the included CData JDBC Driver for MySQL:
  <br/><pre lang="plain">  jdbc:apachekafka:Cache Driver=cdata.jdbc.mysql.MySQLDriver;Cache Connection='jdbc:mysql:Server=localhost;Port=3306;Database=cache;User=root;Password=123456';User=admin;Password=pass;BootStrapServers=https://localhost:9091;Topic=MyTopic;
  </pre>



<h3>SQL Server</h3>
</p>

<p>The following JDBC URL uses the Microsoft JDBC Driver for SQL Server:

<br/><pre lang="plain">jdbc:apachekafka:Cache Driver=com.microsoft.sqlserver.jdbc.SQLServerDriver;Cache Connection='jdbc:sqlserver://localhost\sqlexpress:7437;user=sa;password=123456;databaseName=Cache';User=admin;Password=pass;BootStrapServers=https://localhost:9091;Topic=MyTopic;</pre>



<h3>Oracle</h3>
</p>

<p>The following is a JDBC URL for the Oracle Thin Client:
<br/><pre lang="plain">jdbc:apachekafka:Cache Driver=oracle.jdbc.OracleDriver;CacheConnection='jdbc:oracle:thin:scott/tiger@localhost:1521:orcldb';User=admin;Password=pass;BootStrapServers=https://localhost:9091;Topic=MyTopic;</pre>
NOTE: If using a version of Oracle older than 9i, the cache driver will instead be oracle.jdbc.driver.OracleDriver .



<h3>PostgreSQL</h3>
</p>

<p>The following JDBC URL uses the official PostgreSQL JDBC driver:
<br/><pre lang="plain">jdbc:apachekafka:CacheDriver=cdata.jdbc.postgresql.PostgreSQLDriver;CacheConnection='jdbc:postgresql:User=postgres;Password=admin;Database=postgres;Server=localhost;Port=5432;';User=admin;Password=pass;BootStrapServers=https://localhost:9091;Topic=MyTopic;</pre>


</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - CacheConnection" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - CacheConnection: The connection string for the cache database. This property is always used in conjunction with CacheProvider . Setting both properties will override the value set for CacheLocation for caching data.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>CacheConnection</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_CacheConnection">
<p>The connection string for the cache database. This property is always used in conjunction with CacheProvider . Setting both properties will override the value set for CacheLocation for caching data.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>The cache database is determined based on the <a href="#RSBApacheKafka_p_CacheDriver">CacheDriver</a> and <u>CacheConnection</u> properties. Both properties are required to use the cache database. Examples of common cache database settings can be found below. For more information on setting the caching database's driver, refer to <a href="#RSBApacheKafka_p_CacheDriver">CacheDriver</a>.</p>

<p>The connection string specified in the <u>CacheConnection</u> property is passed directly to the underlying <a href="#RSBApacheKafka_p_CacheDriver">CacheDriver</a>. Consult the documentation for the specific JDBC driver for more information on the available properties. Make sure to include the JDBC driver in your application's classpath.</p>

<p>

<h3>Derby and Java DB</h3>
</p>

<p>The driver simplifies caching to Derby, only requiring you to set the <a href="#RSBApacheKafka_p_CacheLocation">CacheLocation</a> property to make a basic connection.</p>

<p>Alternatively, you can configure the connection to Derby manually using <a href="#RSBApacheKafka_p_CacheDriver">CacheDriver</a> and <u>CacheConnection</u>. The following is the Derby JDBC URL syntax:
<br/><pre lang="plain">jdbc:derby:[subsubprotocol:][databaseName][;attribute=value[;attribute=value] ... ]</pre>
For example, to cache to an in-memory database, use the following:
<br/><pre lang="">jdbc:derby:memory</pre>
</p>

<p>
<h3>SQLite</h3>
</p>

<p>To cache to SQLite, you can use the SQLite JDBC driver. The following is the syntax of the JDBC URL:
<br/><pre lang="plain">jdbc:sqlite:dataSource</pre>
<ul><li><b>Data Source</b>: The path to an SQLite database file. Or, use a value of <var>:memory</var> to cache in memory.</li></ul>
</p>

<p>
<h3>MySQL</h3>
</p>

<p> 
The installation includes the CData JDBC Driver for MySQL. The following is an example JDBC URL:
<br/><pre lang="plain">jdbc:mysql:User=root;Password=root;Server=localhost;Port=3306;Database=cache</pre>
The following are typical connection properties:</p>

<p><ul><li><b>Server</b>: The IP address or domain name of the server you want to connect to.</li><li><b>Port</b>: The port that the server is running on.</li><li><b>User</b>: The user name provided for authentication to the database.</li><li><b>Password</b>: The password provided for authentication to the database.</li><li><b>Database</b>: The name of the database.</li></ul>

<h3>SQL Server</h3>
</p>

<p>The JDBC URL for the Microsoft JDBC Driver for SQL Server has the following syntax:
<br/><pre lang="plain">jdbc:sqlserver://[serverName[\instance][:port]][;database=databaseName][;property=value[;property=value] ... ]</pre>
For example:
<br/><pre lang="plain">jdbc:sqlserver://localhost\sqlexpress:1433;integratedSecurity=true</pre>
The following are typical SQL Server connection properties:
<ul><li><b>Server</b>: The name or network address of the computer running SQL Server. To connect to a named instance instead of the default instance, this property can be used to specify the host name and the instance, separated by a backslash. </li><li><b>Port</b>: The port SQL Server is running on.</li><li><b>Database</b>: The name of the SQL Server database. </li><li><b>Integrated Security</b>: Set this option to true to use the current Windows account for authentication. Set this option to false if you are setting the User and Password in the connection.

<p>To use integrated security, you will also need to add sqljdbc_auth.dll to a folder on the Windows system path. This file is located in the auth subfolder of the Microsoft JDBC Driver for SQL Server installation. The bitness of the assembly must match the bitness of your JVM.</p>
</li><li><b>User Id</b>: The user name provided for authentication with SQL Server. This property is only needed if you are not using integrated security.</li><li><b>Password</b>: The password provided for authentication with SQL Server. This property is only needed if you are not using integrated security.</li></ul>



<h3>Oracle</h3>
</p>

<p>The following is the conventional JDBC URL syntax for the Oracle JDBC Thin driver:
<br/><pre lang="plain">jdbc:oracle:thin:[userId/password]@[//]host[[:port][:sid]]</pre>

For example:

<br/><pre lang="plain">jdbc:oracle:thin:scott/tiger@myhost:1521:orcl</pre>

The following are typical connection properties:
<ul><li>
<p><b>Data Source</b>: The connect descriptor that identifies the Oracle database. This can be a TNS connect descriptor, an Oracle Net Services name that resolves to a connect descriptor, or, after version 11g, an Easy Connect naming (the host name of the Oracle server with an optional port and service name).</p>
</li><li><b>Password</b>: The password provided for authentication with the Oracle database.</li><li><b>User Id</b>: The user Id provided for authentication with the Oracle database.</li></ul>


<h3>PostgreSQL</h3>
</p>

<p>The following is the JDBC URL syntax for the official PostgreSQL JDBC driver:
<br/><pre lang="plain">jdbc:postgresql:[//[host[:port]]/]database[[?option=value][[&amp;option=value][&amp;option=value] ... ]]</pre>
For example, the following connection string connects to a database on the default host (localhost) and port (5432):
<br/><pre lang="plain">jdbc:postgresql:postgres</pre>
The following are typical connection properties:
<ul><li><b>Host</b>: The address of the server hosting the PostgreSQL database.</li><li><b>Port</b>: The port used to connect to the server hosting the PostgreSQL database.</li><li><b>Database</b>: The name of the database.</li><li><b>User name</b>: The user Id provided for authentication with the PostgreSQL database. You can specify this in the JDBC URL with the "user" parameter.</li><li><b>Password</b>: The password provided for authentication with the PostgreSQL database.</li></ul>


</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - CacheLocation" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - CacheLocation: Specifies the path to the cache when caching to a file.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>CacheLocation</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_CacheLocation">
<p>Specifies the path to the cache when caching to a file.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>"%APPDATA%\\CData\\ApacheKafka Data Provider"</p>

<h3>Remarks</h3>


<p>The <u>CacheLocation</u> is a simple, file-based cache. The driver uses Java DB, Oracle's distribution of the Derby database. To cache to Java DB, you will need to add the Java DB JAR file to the classpath. The JAR file, derby.jar, is shipped in the JDK and located in the db subfolder of the JDK installation. </p>

<p></p>

<p>If left unspecified, the default location is "%APPDATA%\\CData\\ApacheKafka Data Provider" with <b>%APPDATA%</b> being set to the user's configuration directory:

<p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap"><b>Platform</b> </td><td><b>%APPDATA%</b></td></tr><tr><td style="white-space:nowrap">Windows </td><td>The value of the APPDATA environment variable</td></tr><tr><td style="white-space:nowrap">Mac </td><td>~/Library/Application Support</td></tr><tr><td style="white-space:nowrap">Linux </td><td>~/.config</td></tr></table></center><p />


<h3>See Also</h3>
</p>

<p><ul><li><a href="#RSBApacheKafka_p_AutoCache">AutoCache</a>: Set to implicitly create and maintain a cache for later offline use. </li><li><a href="#RSBApacheKafka_p_CacheMetadata">CacheMetadata</a>: Set to persist the Apache Kafka catalog in <u>CacheLocation</u>.</li></ul>


</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - CacheTolerance" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - CacheTolerance: The tolerance for stale data in the cache specified in seconds when using AutoCache .">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>CacheTolerance</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_CacheTolerance">
<p>The tolerance for stale data in the cache specified in seconds when using AutoCache .</p>







<h3>Data Type</h3>
    <p>int</p>

<h3>Default Value</h3>
<p>600</p>

<h3>Remarks</h3>


<p>The tolerance for stale data in the cache specified in seconds. This only applies when <a href="#RSBApacheKafka_p_AutoCache">AutoCache</a> is used. The driver checks with the data source for newer records after the tolerance interval has expired. Otherwise, it returns the data directly from the cache.
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - Offline" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - Offline: Use offline mode to get the data from the cache instead of the live source.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>Offline</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_Offline">
<p>Use offline mode to get the data from the cache instead of the live source.</p>







<h3>Data Type</h3>
    <p>bool</p>

<h3>Default Value</h3>
<p>false</p>

<h3>Remarks</h3>


<p>When <u>Offline</u> = true, all queries execute against the cache as opposed to the live data source. In this mode, certain queries like INSERT, UPDATE, DELETE, and CACHE are not allowed. 
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - CacheMetadata" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - CacheMetadata: This property determines whether or not to cache the table metadata to a file store.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>CacheMetadata</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_CacheMetadata">
<p>This property determines whether or not to cache the table metadata to a file store.</p>







<h3>Data Type</h3>
    <p>bool</p>

<h3>Default Value</h3>
<p>false</p>

<h3>Remarks</h3>


<p>As you execute queries with this property set, table metadata in the Apache Kafka catalog are cached to the file store specified by <a href="#RSBApacheKafka_p_CacheLocation">CacheLocation</a> if set or the user's home directory otherwise. A table's metadata will be retrieved only once, when the table is queried for the first time.


<h3>When to Use CacheMetadata</h3>
</p>

<p>The driver automatically persists metadata in memory for up to two hours when you first discover the metadata for a table or view and therefore, <u>CacheMetadata</u> is generally not required. <u>CacheMetadata</u> becomes useful when metadata operations are expensive such as when you are working with large amounts of metadata or when you have many short-lived connections.


<h3>When Not to Use CacheMetadata</h3>
</p>

<p> 
<ul><li><b>When you are working with volatile metadata</b>: Metadata for a table is only retrieved the first time the connection to the table is made. To pick up new, changed, or deleted columns, you would need to delete and rebuild the metadata cache. Therefore, it is best to rely on the in-memory caching for cases where metadata changes often.</li><li><b>When you are caching to a database</b>: <u>CacheMetadata</u> can only be used with <a href="#RSBApacheKafka_p_CacheLocation">CacheLocation</a>. If you are caching to another database with the <a href="#RSBApacheKafka_p_CacheDriver">CacheDriver</a> and <a href="#RSBApacheKafka_p_CacheConnection">CacheConnection</a> properties, use <a href="#RSBApacheKafka_p_AutoCache">AutoCache</a> to cache implicitly. Or, use  <a href="#pg_cache">CACHE Statements</a> to cache explicitly. </li></ul>


</p>

</div>





  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - Miscellaneous" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - Miscellaneous: Specifies whether or not to return the message as a whole string.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>Miscellaneous</h1>
       
<div class="chapter_content" id="RSBApacheKafka_c_Miscellaneous">
<p>
<p>This section provides a complete list of the Miscellaneous properties you can configure in the connection string for this provider.
<hr/>
<center></a>
<table><tr><td><b>Property</b></td><td><b>Description</b></td></tr>
<tr valign="top"><td><a href="#RSBApacheKafka_p_AggregateMessages">AggregateMessages</a></td><td>Specifies whether or not to return the message as a whole string.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_BatchSize">BatchSize</a></td><td>The maximum size of each batch operation to submit.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_CompressionType">CompressionType</a></td><td>Data compression type. Batches of data will be compressed together.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_ConnectionLifeTime">ConnectionLifeTime</a></td><td>The maximum lifetime of a connection in seconds. Once the time has elapsed, the connection object is disposed.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_ConnectOnOpen">ConnectOnOpen</a></td><td>This property specifies whether to connect to the Apache Kafka when the connection is opened.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_ConsumerProperties">ConsumerProperties</a></td><td>Additional options used to configure Kafka consumers.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_CreateTablePartitions">CreateTablePartitions</a></td><td>The number of partitions assigned to a topic created with CREATE TABLE.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_CreateTableReplicationFactor">CreateTableReplicationFactor</a></td><td>The number of replicas assigned to a topic created with CREATE TABLE.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_EnableIdempotence">EnableIdempotence</a></td><td>If set to true, the Apache Kafka will ensure messages are delivered in the correct order, and without duplicates.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_FlattenArrays">FlattenArrays</a></td><td>By default, nested arrays won't show up if TypeDetectionScheme is set to SchemaRegistry. The FlattenArrays property can be used to flatten the elements of nested arrays into columns of their own. Set FlattenArrays to the number of elements you want to return from nested arrays.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_GenerateSchemaFiles">GenerateSchemaFiles</a></td><td>Indicates the user preference as to when schemas should be generated and saved.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_MaximumBatchSize">MaximumBatchSize</a></td><td>Specifies maximum batch size to gather before sending a request.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_MaxRows">MaxRows</a></td><td>Limits the number of rows returned when no aggregation or GROUP BY is used in the query. This takes precedence over LIMIT clauses.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_MessageKeyColumn">MessageKeyColumn</a></td><td>If specified, the message key sent to Apache Kafka will be read from this column.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_MessageKeyType">MessageKeyType</a></td><td>If MessageKeyColumn is specified, this property must be set to the expected type for the pertinent column.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_OffsetResetStrategy">OffsetResetStrategy</a></td><td>Specifies an offset for the consumer group.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_Other">Other</a></td><td>These hidden properties are used only in specific use cases.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_Pagesize">Pagesize</a></td><td>The maximum number of rows to fetch from Kafka at one time.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_PoolIdleTimeout">PoolIdleTimeout</a></td><td>The allowed idle time for a connection before it is closed.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_PoolMaxSize">PoolMaxSize</a></td><td>The maximum connections in the pool.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_PoolMinSize">PoolMinSize</a></td><td>The minimum number of connections in the pool.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_PoolWaitTime">PoolWaitTime</a></td><td>The max seconds to wait for an available connection.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_ProduceMeta">ProduceMeta</a></td><td>Specifies whether or not to send a meta message while producing the outgoing message.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_ProducerProperties">ProducerProperties</a></td><td>Additional options used to configure Kafka producers.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_PseudoColumns">PseudoColumns</a></td><td>This property indicates whether or not to include pseudo columns as columns to the table.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_ReadDuration">ReadDuration</a></td><td>The duration which additional messages are allowed.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_Readonly">Readonly</a></td><td>You can use this property to enforce read-only access to Apache Kafka from the provider.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_RowScanDepth">RowScanDepth</a></td><td>The maximum number of messages to scan for the columns available in the topic.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_RTK">RTK</a></td><td>The runtime key used for licensing.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_SerializationFormat">SerializationFormat</a></td><td>Specifies how to serialize/deserialize message contents.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_Timeout">Timeout</a></td><td>The value in seconds until the timeout error is thrown, canceling the operation.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_TypeDetectionScheme">TypeDetectionScheme</a></td><td>Comma-separated list of options specifying how the provider will scan the data to determine the fields and datatypes for the bucket.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_UseConfluentAvroFormat">UseConfluentAvroFormat</a></td><td>Specifies how Avro data should be formatted during an INSERT.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_UseConnectionPooling">UseConnectionPooling</a></td><td>This property enables connection pooling.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_UserDefinedViews">UserDefinedViews</a></td><td>A filepath pointing to the JSON configuration file containing your custom views.</td></tr>

<tr valign="top"><td><a href="#RSBApacheKafka_p_ValidateRegistryTopics">ValidateRegistryTopics</a></td><td>Specifies whether or not to validate schema registry topics against the Apache Kafka broker. Only has an effect when TypeDetectionScheme =SchemaRegistry.</td></tr>
</table></center></div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - AggregateMessages" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - AggregateMessages: Specifies whether or not to return the message as a whole string.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>AggregateMessages</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_AggregateMessages">
<p>Specifies whether or not to return the message as a whole string.</p>







<h3>Data Type</h3>
    <p>bool</p>

<h3>Default Value</h3>
<p>false</p>

<h3>Remarks</h3>


<p>When set to false, the result will be parsed and detected fields will appear in the resultset.
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - BatchSize" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - BatchSize: The maximum size of each batch operation to submit.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>BatchSize</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_BatchSize">
<p>The maximum size of each batch operation to submit.</p>







<h3>Data Type</h3>
    <p>int</p>

<h3>Default Value</h3>
<p>0</p>

<h3>Remarks</h3>


<p>When BatchSize is set to a value greater than 0, the batch operation will split the entire batch into separate batches of size BatchSize. The split batches will then be submitted to the server individually. This is useful when the server has limitations on the size of the request that can be submitted.
</p>

<p>Setting BatchSize to 0 will submit the entire batch as specified.
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - CompressionType" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - CompressionType: Data compression type. Batches of data will be compressed together.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>CompressionType</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_CompressionType">
<p>Data compression type. Batches of data will be compressed together.</p>





<h3>Possible Values</h3>
none, gzip, snappy, lz4

<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>"none"</p>

<h3>Remarks</h3>


<p>The following values are supported:
<p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap">NONE</td><td> Messages will not be compressed.</td></tr><tr><td style="white-space:nowrap">GZIP</td><td> Messages will be compressed using gzip.</td></tr><tr><td style="white-space:nowrap">SNAPPY</td><td> Messages will be compressed using snappy.</td></tr><tr><td style="white-space:nowrap">LZ4</td><td> Messages will be compressed using lz4.</td></tr></table></center><p />

</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - ConnectionLifeTime" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - ConnectionLifeTime: The maximum lifetime of a connection in seconds. Once the time has elapsed, the connection object is disposed.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>ConnectionLifeTime</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_ConnectionLifeTime">
<p>The maximum lifetime of a connection in seconds. Once the time has elapsed, the connection object is disposed.</p>







<h3>Data Type</h3>
    <p>int</p>

<h3>Default Value</h3>
<p>0</p>

<h3>Remarks</h3>


<p>The maximum lifetime of a connection in seconds. Once the time has elapsed, the connection object is disposed. The default is 0 which indicates there is no limit to the connection lifetime. 
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - ConnectOnOpen" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - ConnectOnOpen: This property specifies whether to connect to the Apache Kafka when the connection is opened.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>ConnectOnOpen</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_ConnectOnOpen">
<p>This property specifies whether to connect to the Apache Kafka when the connection is opened.</p>







<h3>Data Type</h3>
    <p>bool</p>

<h3>Default Value</h3>
<p>false</p>

<h3>Remarks</h3>


<p>When set to true, a connection will be made to Apache Kafka when the connection is opened. This property enables the Test Connection feature available in various database tools.
</p>

<p>This feature acts as a NOOP command as it is used to verify a connection can be made to Apache Kafka and nothing from this initial connection is maintained.
</p>

<p>Setting this property to false may provide performance improvements (depending upon the number of times a connection is opened).
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - ConsumerProperties" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - ConsumerProperties: Additional options used to configure Kafka consumers.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>ConsumerProperties</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_ConsumerProperties">
<p>Additional options used to configure Kafka consumers.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>The driver exposes several Kafka consumer configuration values directly as connection properties.
Internally, these are all mapped into properties that are passed to the Kafka client libraries.
</p>

<p>If the driver does not expose an option for the consumer configuration, it can be set here.
This option takes a connection string value and passes all its options directly to the consumer.
For example, <var>security.protocol=SASL_SSL;sasl.mechanism=SCRAM-SHA-512</var> sets the
<var>security.protocol</var> and <var>sasl.mechanism</var> consumer properties.
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - CreateTablePartitions" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - CreateTablePartitions: The number of partitions assigned to a topic created with CREATE TABLE.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>CreateTablePartitions</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_CreateTablePartitions">
<p>The number of partitions assigned to a topic created with CREATE TABLE.</p>







<h3>Data Type</h3>
    <p>int</p>

<h3>Default Value</h3>
<p>1</p>

<h3>Remarks</h3>


<p>When executing a CREATE TABLE statement, the driver creates a new empty topic.
By default, the driver creates this new topic with 1 partition.
</p>

<p>You can create topics with more partitions by changing this setting.
This can be useful if you plan on having multiple consumers process the messages on this topic.
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - CreateTableReplicationFactor" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - CreateTableReplicationFactor: The number of replicas assigned to a topic created with CREATE TABLE.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>CreateTableReplicationFactor</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_CreateTableReplicationFactor">
<p>The number of replicas assigned to a topic created with CREATE TABLE.</p>







<h3>Data Type</h3>
    <p>int</p>

<h3>Default Value</h3>
<p>3</p>

<h3>Remarks</h3>


<p>When executing a CREATE TABLE statement, the driver creates a new empty topic.
By default, the driver creates this topic with a replication factor of 3.
</p>

<p>You can create topics with a different number of replicas by changing this setting.
There are two main cases where this is useful:

<ul><li>When your cluster has fewer than 3 nodes.
This setting should never be higher than the number of nodes in your cluster.
For example, creating topics with a replication factor of 3 on a cluster with 2 nodes will fail.
</li><li>When your cluster has more than 3 nodes and you want more safety in case of failover.
Apache Kafka uses replicas to prevent data loss when a node fails, and if all the replicas fail then the topic is unavailable.</li></ul>
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - EnableIdempotence" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - EnableIdempotence: If set to true, the Apache Kafka will ensure messages are delivered in the correct order, and without duplicates.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>EnableIdempotence</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_EnableIdempotence">
<p>If set to true, the Apache Kafka will ensure messages are delivered in the correct order, and without duplicates.</p>







<h3>Data Type</h3>
    <p>bool</p>

<h3>Default Value</h3>
<p>false</p>

<h3>Remarks</h3>


<p>Gives each message a sequence number when enabled. Specifies how to serialize/deserialize the incoming or outgoing message.

</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - FlattenArrays" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - FlattenArrays: By default, nested arrays won't show up if TypeDetectionScheme is set to SchemaRegistry. The FlattenArrays property can be used to flatten the elements of nested arrays into columns of their own. Set FlattenArrays to the number of elements you want to return from nested arrays.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>FlattenArrays</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_FlattenArrays">
<p>By default, nested arrays won't show up if TypeDetectionScheme is set to SchemaRegistry. The FlattenArrays property can be used to flatten the elements of nested arrays into columns of their own. Set FlattenArrays to the number of elements you want to return from nested arrays.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>"0"</p>

<h3>Remarks</h3>


<p>Set <u>FlattenArrays</u> to the number of elements you want to return from nested arrays. The specified elements are returned as columns.</p>

<p>For example, you can return an arbitrary number of elements from an array of strings:
<br/><pre lang="">["FLOW-MATIC","LISP","COBOL"]</pre>
When <u>FlattenArrays</u> is set to 1, the preceding array is flattened into the following table:
<p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap"><b>Column Name</b></td><td><b>Column Value</b></td></tr><tr><td style="white-space:nowrap">languages.0</td><td>FLOW-MATIC</td></tr></table></center><p /></p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - GenerateSchemaFiles" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - GenerateSchemaFiles: Indicates the user preference as to when schemas should be generated and saved.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>GenerateSchemaFiles</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_GenerateSchemaFiles">
<p>Indicates the user preference as to when schemas should be generated and saved.</p>





<h3>Possible Values</h3>
Never, OnUse, OnStart, OnCreate

<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>"Never"</p>

<h3>Remarks</h3>


<p>This property outputs schemas to .rsd files in the path specified by <a href="#RSBApacheKafka_p_Location">Location</a>.
</p>

<p>Available settings are the following:
<ul><li>Never: A schema file will never be generated.</li><li>OnUse: A schema file will be generated the first time a table is referenced, provided the schema file for the table does not already exist.</li><li>OnStart: A schema file will be generated at connection time for any tables that do not currently have a schema file.</li><li>OnCreate: A schema file will be generated by when running a CREATE TABLE SQL query.</li></ul>
 Note that if you want to regenerate a file, you will first need to delete it.


<h3>Generate Schemas with SQL</h3>
</p>

<p>  When you set <u>GenerateSchemaFiles</u> to <b>OnUse</b>, the driver generates schemas as you execute SELECT queries. Schemas are generated for each table referenced in the query. </p>

<p>  When you set <u>GenerateSchemaFiles</u> to <b>OnCreate</b>, schemas are only generated when a CREATE TABLE query is executed. 


<h3>Generate Schemas on Connection</h3>
</p>

<p>Another way to use this property is to obtain schemas for every table in your database when you connect. To do so, set <u>GenerateSchemaFiles</u> to <b>OnStart</b> and connect. 


</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - MaximumBatchSize" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - MaximumBatchSize: Specifies maximum batch size to gather before sending a request.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>MaximumBatchSize</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_MaximumBatchSize">
<p>Specifies maximum batch size to gather before sending a request.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>"16384"</p>

<h3>Remarks</h3>


<p>A batch can be formed by one or more messages. The size is specified in bytes.
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - MaxRows" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - MaxRows: Limits the number of rows returned when no aggregation or GROUP BY is used in the query. This takes precedence over LIMIT clauses.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>MaxRows</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_MaxRows">
<p>Limits the number of rows returned when no aggregation or GROUP BY is used in the query. This takes precedence over LIMIT clauses.</p>







<h3>Data Type</h3>
    <p>int</p>

<h3>Default Value</h3>
<p>-1</p>

<h3>Remarks</h3>


<p>Limits the number of rows returned when no aggregation or GROUP BY is used in the query. This takes precedence over LIMIT clauses.
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - MessageKeyColumn" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - MessageKeyColumn: If specified, the message key sent to Apache Kafka will be read from this column.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>MessageKeyColumn</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_MessageKeyColumn">
<p>If specified, the message key sent to Apache Kafka will be read from this column.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>If specified, the message key sent to Apache Kafka will be read from this column.</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - MessageKeyType" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - MessageKeyType: If MessageKeyColumn is specified, this property must be set to the expected type for the pertinent column.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>MessageKeyType</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_MessageKeyType">
<p>If MessageKeyColumn is specified, this property must be set to the expected type for the pertinent column.</p>





<h3>Possible Values</h3>
Null, Binary, String, Long, Integer

<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>"Null"</p>

<h3>Remarks</h3>


<p>Be advised that, when this value is set to null, <a href="#RSBApacheKafka_p_MessageKeyColumn">MessageKeyColumn</a> will be ignored.</p>

<p>The available types are as follows:
<p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap">Null </td><td>The key column type will be treated as null.</td></tr><tr><td style="white-space:nowrap">Binary </td><td>The key column type will be treated as a binary. The value will be reported in base64.</td></tr><tr><td style="white-space:nowrap">String </td><td>The key column type will be treated as a string.</td></tr><tr><td style="white-space:nowrap">Long </td><td>The key column type will be treated as a long.</td></tr><tr><td style="white-space:nowrap">Integer </td><td>The key column type will be treated as an integer.</td></tr></table></center><p />
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - OffsetResetStrategy" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - OffsetResetStrategy: Specifies an offset for the consumer group.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>OffsetResetStrategy</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_OffsetResetStrategy">
<p>Specifies an offset for the consumer group.</p>





<h3>Possible Values</h3>
Earliest, Latest

<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>"Earliest"</p>

<h3>Remarks</h3>


<p>Select one of the following strategies:
<p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap">Latest</td><td> Will only consume messages that are produced after the consumer group is created.</td></tr><tr><td style="white-space:nowrap">Earliest</td><td> Will consume any unconsumed messages including any message produced before the lifetime of the consumer group.</td></tr></table></center><p />
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - Other" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - Other: These hidden properties are used only in specific use cases.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>Other</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_Other">
<p>These hidden properties are used only in specific use cases.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>The properties listed below are available for specific use cases. Normal driver use cases and functionality should not require these properties.</p>

<p>Specify multiple properties in a semicolon-separated list.


<h3>Caching Configuration</h3>

<p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap">CachePartial=True</td><td>Caches only a subset of columns, which you can specify in your query.
</td></tr><tr><td style="white-space:nowrap">QueryPassthrough=True</td><td>Passes the specified query to the cache database instead of using the SQL parser of the driver.</td></tr></table></center><p />


<h3>Integration and Formatting</h3>

<p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap">DefaultColumnSize</td><td>Sets the default length of string fields when the data source does not provide column length in the metadata. The default value is 2000.
</td></tr><tr><td style="white-space:nowrap">ConvertDateTimeToGMT</td><td>Determines whether to convert date-time values to GMT, instead of the local time of the machine.
</td></tr><tr><td style="white-space:nowrap">RecordToFile=filename</td><td>Records the underlying socket data transfer to the specified file.
</td></tr></table></center><p />

</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - Pagesize" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - Pagesize: The maximum number of rows to fetch from Kafka at one time.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>Pagesize</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_Pagesize">
<p>The maximum number of rows to fetch from Kafka at one time.</p>







<h3>Data Type</h3>
    <p>int</p>

<h3>Default Value</h3>
<p>1000</p>

<h3>Remarks</h3>


<p>The driver batches reads to Kafka to reduce overhead.
Instead of fetching a single row from the broker every time a query row is read, the driver will read multiple rows and save them to the resultset.
This means that only the first row read from the resultset must wait for the broker.
Later rows can be read out of this buffer directly.
</p>

<p>This option controls the maximum number of rows the driver stores on the resultset.
Setting this to a higher value will use more memory but requires waiting on the broker less often.
Lower values will give lower throughput while using less memory.
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - PoolIdleTimeout" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - PoolIdleTimeout: The allowed idle time for a connection before it is closed.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>PoolIdleTimeout</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_PoolIdleTimeout">
<p>The allowed idle time for a connection before it is closed.</p>







<h3>Data Type</h3>
    <p>int</p>

<h3>Default Value</h3>
<p>60</p>

<h3>Remarks</h3>


<p>The allowed idle time a connection can remain in the pool until the connection is closed. The default is 60 seconds.
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - PoolMaxSize" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - PoolMaxSize: The maximum connections in the pool.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>PoolMaxSize</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_PoolMaxSize">
<p>The maximum connections in the pool.</p>







<h3>Data Type</h3>
    <p>int</p>

<h3>Default Value</h3>
<p>100</p>

<h3>Remarks</h3>


<p>The maximum connections in the pool. The default is 100. To disable this property, set the property value to 0 or less.
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - PoolMinSize" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - PoolMinSize: The minimum number of connections in the pool.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>PoolMinSize</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_PoolMinSize">
<p>The minimum number of connections in the pool.</p>







<h3>Data Type</h3>
    <p>int</p>

<h3>Default Value</h3>
<p>1</p>

<h3>Remarks</h3>


<p>The minimum number of connections in the pool. The default is 1.
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - PoolWaitTime" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - PoolWaitTime: The max seconds to wait for an available connection.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>PoolWaitTime</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_PoolWaitTime">
<p>The max seconds to wait for an available connection.</p>







<h3>Data Type</h3>
    <p>int</p>

<h3>Default Value</h3>
<p>60</p>

<h3>Remarks</h3>


<p>The max seconds to wait for a connection to become available. If a new connection request is waiting for an available connection and exceeds this time, an error is thrown. By default, new requests wait forever for an available connection. 
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - ProduceMeta" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - ProduceMeta: Specifies whether or not to send a meta message while producing the outgoing message.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>ProduceMeta</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_ProduceMeta">
<p>Specifies whether or not to send a meta message while producing the outgoing message.</p>







<h3>Data Type</h3>
    <p>bool</p>

<h3>Default Value</h3>
<p>false</p>

<h3>Remarks</h3>


<p>This option is only used if <a href="#RSBApacheKafka_p_SerializationFormat">SerializationFormat</a> is set to CSV.
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - ProducerProperties" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - ProducerProperties: Additional options used to configure Kafka producers.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>ProducerProperties</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_ProducerProperties">
<p>Additional options used to configure Kafka producers.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>This option is like <a href="#RSBApacheKafka_p_ConsumerProperties">ConsumerProperties</a> but applies to producers instead.
Please refer to that property for more information.
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - PseudoColumns" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - PseudoColumns: This property indicates whether or not to include pseudo columns as columns to the table.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>PseudoColumns</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_PseudoColumns">
<p>This property indicates whether or not to include pseudo columns as columns to the table.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>This setting is particularly helpful in Entity Framework, which does not allow you to set a value for a pseudo column unless it is a table column. The value of this connection setting is of the format "Table1=Column1, Table1=Column2, Table2=Column3". You can use the "*" character to include all tables and all columns; for example, "*=*".</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - ReadDuration" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - ReadDuration: The duration which additional messages are allowed.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>ReadDuration</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_ReadDuration">
<p>The duration which additional messages are allowed.</p>







<h3>Data Type</h3>
    <p>int</p>

<h3>Default Value</h3>
<p>30</p>

<h3>Remarks</h3>


<p>A timeout for the driver to stop waiting for additional messages to come.
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - Readonly" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - Readonly: You can use this property to enforce read-only access to Apache Kafka from the provider.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>Readonly</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_Readonly">
<p>You can use this property to enforce read-only access to Apache Kafka from the provider.</p>







<h3>Data Type</h3>
    <p>bool</p>

<h3>Default Value</h3>
<p>false</p>

<h3>Remarks</h3>


<p>If this property is set to true, the driver will allow only SELECT queries. INSERT, UPDATE, DELETE, and stored procedure queries will cause an error to be thrown.
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - RowScanDepth" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - RowScanDepth: The maximum number of messages to scan for the columns available in the topic.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>RowScanDepth</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_RowScanDepth">
<p>The maximum number of messages to scan for the columns available in the topic.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>"100"</p>

<h3>Remarks</h3>


<p>Setting a high value may decrease performance. Setting a low value may prevent the data type from being 
determined properly.
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - RTK" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - RTK: The runtime key used for licensing.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>RTK</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_RTK">
<p>The runtime key used for licensing.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>
  </p>

<p>The RTK property may be used to license a build.


See the included licensing file to see how to set this property. The runtime key is only available if you purchased an OEM license.

  </p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - SerializationFormat" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - SerializationFormat: Specifies how to serialize/deserialize message contents.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>SerializationFormat</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_SerializationFormat">
<p>Specifies how to serialize/deserialize message contents.</p>





<h3>Possible Values</h3>
NONE, AUTO, JSON, CSV, XML, AVRO

<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>"AUTO"</p>

<h3>Remarks</h3>


<p>The driver uses this property differently based on the value of <a href="#RSBApacheKafka_p_TypeDetectionScheme">TypeDetectionScheme</a>:

<ul><li><b>None</b>, <b>RowScan</b>: The driver samples messages from the topic to determine their columns.
If <u>SerializationFormat</u> is <b>AUTO</b>, then the format is determined based on the message content.
Otherwise, the format is just the value of this property.
</li><li><b>SchemaRegistry</b>: The driver uses <u>SerializationFormat</u>, a fallback, if it cannot determine the format from the registry.
This is rare as it only happens when the registry does not specify a format and the schema cannot be inferred to be a JSON or Avro schema.
</li><li><b>MessageOnly</b>: <u>SerializationFormat</u> determines how the message column is surfaced.
If the format is AUTO, AVRO, or NONE, the message column is treated as binary and uses base64 encoded values.
Otherwise, the message column is treated as text.</li></ul>
</p>

<p>Available formats:
<p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap">NONE</td><td> Message will be always BASE64 encoded on both the consume and produce operations.</td></tr><tr><td style="white-space:nowrap">AUTO</td><td> Attempt to automatically figure out the current topic's serialization format.</td></tr><tr><td style="white-space:nowrap">JSON</td><td> Message will be serialized using the JSON format.</td></tr><tr><td style="white-space:nowrap">CSV</td><td> Message will be serialized using the CSV format.</td></tr><tr><td style="white-space:nowrap">XML</td><td> Message will be serialized using the XML format.</td></tr><tr><td style="white-space:nowrap">AVRO</td><td> Message will be serialized using the Avro format.</td></tr></table></center><p />

</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - Timeout" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - Timeout: The value in seconds until the timeout error is thrown, canceling the operation.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>Timeout</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_Timeout">
<p>The value in seconds until the timeout error is thrown, canceling the operation.</p>







<h3>Data Type</h3>
    <p>int</p>

<h3>Default Value</h3>
<p>60</p>

<h3>Remarks</h3>


<p>If <u>Timeout</u> = 0, operations do not time out. The operations run until they complete successfully or until they encounter an error condition.
</p>

<p>If <u>Timeout</u> expires and the operation is not yet complete, the driver throws an exception.
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - TypeDetectionScheme" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - TypeDetectionScheme: Comma-separated list of options specifying how the provider will scan the data to determine the fields and datatypes for the bucket.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>TypeDetectionScheme</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_TypeDetectionScheme">
<p>Comma-separated list of options specifying how the provider will scan the data to determine the fields and datatypes for the bucket.</p>





<h3>Possible Values</h3>
None, RowScan, SchemaRegistry, MessageOnly

<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>"None"</p>

<h3>Remarks</h3>


<p>The type dection schemes are:
<p /><center><table class='table'><tr style='display: none;'><td /><td /></tr><tr><td style="white-space:nowrap">None</td><td>Setting <u>TypeDetectionScheme</u> to <b>None</b> will return all columns as string type.</td></tr><tr><td style="white-space:nowrap">RowScan</td><td>Setting <u>TypeDetectionScheme</u> to <b>RowScan</b> will scan rows to heuristically determine the data type. The <a href="#RSBApacheKafka_p_RowScanDepth">RowScanDepth</a> determines the number of rows to be scanned. Can be used with <a href="#RSBApacheKafka_p_RowScanDepth">RowScanDepth</a> in order to change the number of rows to be scanned.</td></tr><tr><td style="white-space:nowrap">SchemaRegistry</td><td>Setting <u>TypeDetectionScheme</u> to <b>SchemaRegistry</b> will determine will make use of the Schema Registry API and use a list of predefined AVRO schemas.</td></tr><tr><td style="white-space:nowrap">MessageOnly</td><td>Setting <u>TypeDetectionScheme</u> to <b>MessageOnly</b> will push all information as a single aggregate value on a column named Message.</td></tr></table></center><p />
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - UseConfluentAvroFormat" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - UseConfluentAvroFormat: Specifies how Avro data should be formatted during an INSERT.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>UseConfluentAvroFormat</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_UseConfluentAvroFormat">
<p>Specifies how Avro data should be formatted during an INSERT.</p>







<h3>Data Type</h3>
    <p>bool</p>

<h3>Default Value</h3>
<p>false</p>

<h3>Remarks</h3>


<p>By default the driver writes out Avro data as a series of file blocks (as defined in the Avro specification).
Confluent tools and libraries cannot decode this format and it cannot be used with Confluent schema validation. 
However, it is more compact because it allows multiple rows of Avro data to be stored in a single message.  
</p>

<p>Enable this option if you use Confluent schema validation, or otherwise require compatibility with Confluent tools and libraries.
Each row inserted into an Avro topic will be a separate message and contain a reference to an schema stored in the registry.
</p>

<p>Note that this cannot be enabled if there is no <a href="#RSBApacheKafka_p_RegistryUrl">RegistryUrl</a> set or <a href="#RSBApacheKafka_p_RegistryUrl">RegistryUrl</a> points to an AWS Glue schema registry.
AWS Glue schemas do not support schema IDs which are a key part of how Confluent handles Avro data.
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - UseConnectionPooling" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - UseConnectionPooling: This property enables connection pooling.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>UseConnectionPooling</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_UseConnectionPooling">
<p>This property enables connection pooling.</p>







<h3>Data Type</h3>
    <p>bool</p>

<h3>Default Value</h3>
<p>false</p>

<h3>Remarks</h3>


<p>This property enables connection pooling. The default is false.
See <a href="#pg_connectionpoolingjdbc">Connection Pooling</a> for information on using connection pools.
</p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - UserDefinedViews" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - UserDefinedViews: A filepath pointing to the JSON configuration file containing your custom views.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>UserDefinedViews</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_UserDefinedViews">
<p>A filepath pointing to the JSON configuration file containing your custom views.</p>







<h3>Data Type</h3>
    <p>string</p>

<h3>Default Value</h3>
<p>""</p>

<h3>Remarks</h3>


<p>User Defined Views are defined in a JSON-formatted configuration file called <var>UserDefinedViews.json</var>. The driver automatically detects the views specified in this file. </p>

<p>You can also have multiple view definitions and control them using the <u>UserDefinedViews</u> connection property. When you use this property, only the specified views are seen by the driver.</p>

<p>This User Defined View configuration file is formatted as follows:
<ul><li>Each root element defines the name of a view.</li><li>Each root element contains a child element, called <b>query</b>, which contains the custom SQL query for the view.</li></ul></p>

<p>For example:
<br/><pre lang="">{
	"MyView": {
		"query": "SELECT * FROM SampleTable_1 WHERE MyColumn = 'value'"
	},
	"MyView2": {
		"query": "SELECT * FROM MyTable WHERE Id IN (1,2,3)"
	}
}</pre>

Use the <u>UserDefinedViews</u> connection property to specify the location of your JSON configuration file. For example:
<br/><pre lang="">"UserDefinedViews", "C:\\Users\\yourusername\\Desktop\\tmp\\UserDefinedViews.json"</pre></p>

</div>



  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - ValidateRegistryTopics" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - ValidateRegistryTopics: Specifies whether or not to validate schema registry topics against the Apache Kafka broker. Only has an effect when TypeDetectionScheme =SchemaRegistry.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>ValidateRegistryTopics</h1>
       
<div class="chapter_content" id="RSBApacheKafka_p_ValidateRegistryTopics">
<p>Specifies whether or not to validate schema registry topics against the Apache Kafka broker. Only has an effect when TypeDetectionScheme =SchemaRegistry.</p>







<h3>Data Type</h3>
    <p>bool</p>

<h3>Default Value</h3>
<p>true</p>

<h3>Remarks</h3>


<p>Schema registries can include metadata for topics that cannot be accessed in Kafka.
This can happen because the topic doesn't exist on the broker.
It is also possible that the principal the connection is authenticated to does not have access to the topic.
</p>

<p>By default, the driver will get a list of schemas from the registry and then filter out any that the broker does not report.
All the remaining valid topics are exposed as tables.
You can disable this behavior by setting this option to false.
This will report all schemas in the registry as tables regardless of whether they are accessible on the broker.









</p>

</div>
















  
  </div></div></div>
<span class=whtitle title="CData JDBC Driver for Apache Kafka - Third Party Copyrights" desc="CData JDBC Driver for Apache Kafka - RSBApacheKafka - Third Party Copyrights: Specifies whether or not to validate schema registry topics against the Apache Kafka broker. Only has an effect when TypeDetectionScheme =SchemaRegistry.">CData JDBC Driver for Apache Kafka</span>
<div class=whiframe>
  <! -- BEGIN CONTENT -->
  <div class="wrapper">
	  <div class="content">
      
		  <h1>Third Party Copyrights</h1>
       
<div class="chapter_content" id="copyright">

<p></p>

<p>
<h2>Apache License</h2>
</p>

<p>The Apache License
Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0</p>

<p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p>

<p></p>

<p>The driver makes use of the following libraries under the terms of the Apache 2.0 license:</p>

<p>
<h3>Apache Kafka</h3>
</p>

<p><b>Provider:</b> Apache Software Foundation</p>

<p><b>Version:</b> 3.3.1</p>

<p></p>

<p>
<h3>Jackson</h3>
</p>

<p><b>Provider:</b> FasterXML, LLC</p>

<p><b>Version:</b> 2.13.3</p>

<p></p>

<p>
<h3>jose.4.j</h3>
</p>

<p><b>Provider:</b> IETF JOSE Working Group</p>

<p><b>Version:</b> 0.7.12</p>

<p></p>

<p>
<h3>lz4-java</h3>
</p>

<p><b>Provider:</b> Adrien Grand and Rei Odaira</p>

<p><b>Version:</b> 1.8.0</p>

<p></p>

<p>
<h3>snappy-java</h3>
</p>

<p><b>Provider:</b> xerial.org</p>

<p><b>Version:</b> 1.1.8.4</p>

<p></p>

<p>
<h3>SLF4J API Module</h3>
</p>

<p><b>Provider:</b> QOS.ch Sarl</p>

<p><b>Version:</b> 1.7.36</p>

<p></p>

<p>
<h3>OSGi DTO</h3>
</p>

<p><b>Provider:</b> OSGi Alliance</p>

<p><b>Version:</b> 1.1.1</p>

<p></p>

<p>
<h3>OSGi Framework</h3>
</p>

<p><b>Provider:</b> OSGi Alliance</p>

<p><b>Version:</b> 1.10.0</p>

<p></p>

<p>
<h3>OSGi Resource</h3>
</p>

<p><b>Provider:</b> OSGi Alliance</p>

<p><b>Version:</b> 1.0.1</p>

<p>
<h2>BSD-3-Clause License</h2>
</p>

<p>Copyright (c) 2021 BSD All rights reserved.</p>

<p>Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:</p>

<p>1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.
2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.
3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.</p>

<p>THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p>

<p></p>

<p>The driver makes use of the following library under the terms of the BSD-3-Clause license:</p>

<p>
<h3>ZSTD JNI</h3>
</p>

<p><b>Provider:</b> com.github.luben</p>

<p><b>Version:</b> 1.5.2-1</p>

<p></p>

<p></p>


    </div></div></div><div class="right-table-of-contents"></div>

    
            </div>
        </div>
        <div>

				<div id='whfooter'>
					<div class=content>
            						Copyright (c) 2023 CData Software, Inc. - All rights reserved.
            
            
						<div class='build'>Build 23.0.8750</div>
					</div>&nbsp;
				</div>
        </div>
    </div>
	
	          <script type="text/javascript" src="./lib/jquery-3.5.1.min.js"></script>
      <script type="text/javascript" src="./lib/bootstrap/bootstrap.min.js"></script>
      <script type="text/javascript" src="./lib/syntaxhighlighter-3.0.83/scripts/XRegExp.js"></script>	
      <script type="text/javascript" src="./lib/syntaxhighlighter-3.0.83/scripts/shCore.js"></script>
      <script type="text/javascript" src="./lib/syntaxhighlighter-3.0.83/scripts/shAutoloader.js"></script>
      <script type="text/javascript" src="./lib/help2.js"></script>
    
		<script type="text/javascript">
			function setScrollBar() {
			  var treeHeight = $("#whtoc>ul").height();
			  var treePerentHeight = $("#whsizer").height();
			  if (treeHeight > treePerentHeight) {
				$("#whsizer").css("overflow-y", "scroll");
			  } else {
				$("#whsizer").css("overflow-y", "hidden");
			  }
			}
			
			function loadTree() {
			//   $("#whsizer").css({ "position": "fixed"});
			  $("#whtoc").addClass("tree");
			  $('#whtoc.tree li:has(ul)').addClass('parent_li').prepend("<span class='ygtvtp' title='Expand this branch'></span>");
			  $('#whtoc.tree li:not(.parent_li)').prepend("<span class='ygtvln'></span>");
			  $("#whtoc").show();
			  $('#whtoc.tree li.parent_li > span').on('click', function (e) {
				var children = $(this).parent('li.parent_li').find(' > ul > li');
				if (children.is(":visible")) {
				  children.hide();
				  $(this).attr('title', 'Expand this branch').addClass('ygtvtp').removeClass('ygtvlm');
				} else {
					children.show();
				  $(this).attr('title', 'Collapse this branch').addClass('ygtvlm').removeClass('ygtvtp');
				}
			
				var height = $("#whtoc").height();
				$("#resizerCol").css("height", 0 + "px"); //disable resizer 
				setScrollBar();
				e.stopPropagation();
			  });
			  setScrollBar();
			}
			
			loadTree();
		</script>
		
		
	</body></html>






